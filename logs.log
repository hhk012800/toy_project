2025-10-29 10:01:16,098:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-10-29 10:01:16,099:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-10-29 10:01:16,099:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-10-29 10:01:16,099:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-10-29 10:01:17,620:WARNING:
'prophet' is a soft dependency and not included in the pycaret installation. Please run: `pip install prophet` to install.
2025-10-29 10:01:44,055:INFO:PyCaret ClassificationExperiment
2025-10-29 10:01:44,055:INFO:Logging name: clf-default-name
2025-10-29 10:01:44,055:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-10-29 10:01:44,055:INFO:version 3.0.0
2025-10-29 10:01:44,055:INFO:Initializing setup()
2025-10-29 10:01:44,055:INFO:self.USI: 2777
2025-10-29 10:01:44,055:INFO:self._variable_keys: {'target_param', 'seed', 'y_train', 'fold_shuffle_param', 'X_train', 'is_multiclass', 'y_test', '_ml_usecase', 'X', 'log_plots_param', 'USI', 'exp_id', 'pipeline', 'idx', 'y', '_available_plots', 'html_param', 'fold_groups_param', 'exp_name_log', 'X_test', 'fold_generator', 'logging_param', 'gpu_n_jobs_param', 'n_jobs_param', 'memory', 'data', 'gpu_param', 'fix_imbalance'}
2025-10-29 10:01:44,055:INFO:Checking environment
2025-10-29 10:01:44,055:INFO:python_version: 3.8.15
2025-10-29 10:01:44,055:INFO:python_build: ('default', 'Nov 24 2022 14:38:14')
2025-10-29 10:01:44,055:INFO:machine: AMD64
2025-10-29 10:01:44,056:INFO:platform: Windows-10-10.0.19045-SP0
2025-10-29 10:01:44,058:INFO:Memory: svmem(total=34299187200, available=23480635392, percent=31.5, used=10818551808, free=23480635392)
2025-10-29 10:01:44,058:INFO:Physical Core: 6
2025-10-29 10:01:44,058:INFO:Logical Core: 6
2025-10-29 10:01:44,058:INFO:Checking libraries
2025-10-29 10:01:44,058:INFO:System:
2025-10-29 10:01:44,058:INFO:    python: 3.8.15 (default, Nov 24 2022, 14:38:14) [MSC v.1916 64 bit (AMD64)]
2025-10-29 10:01:44,058:INFO:executable: c:\Users\hkh97\anaconda3\envs\han\python.exe
2025-10-29 10:01:44,058:INFO:   machine: Windows-10-10.0.19045-SP0
2025-10-29 10:01:44,058:INFO:PyCaret required dependencies:
2025-10-29 10:01:44,270:INFO:                 pip: 25.0.1
2025-10-29 10:01:44,270:INFO:          setuptools: 75.1.0
2025-10-29 10:01:44,270:INFO:             pycaret: 3.0.0
2025-10-29 10:01:44,270:INFO:             IPython: 8.12.3
2025-10-29 10:01:44,270:INFO:          ipywidgets: 8.1.7
2025-10-29 10:01:44,270:INFO:                tqdm: 4.67.1
2025-10-29 10:01:44,270:INFO:               numpy: 1.24.1
2025-10-29 10:01:44,271:INFO:              pandas: 1.5.3
2025-10-29 10:01:44,271:INFO:              jinja2: 3.1.4
2025-10-29 10:01:44,271:INFO:               scipy: 1.10.1
2025-10-29 10:01:44,271:INFO:              joblib: 1.2.0
2025-10-29 10:01:44,271:INFO:             sklearn: 1.2.2
2025-10-29 10:01:44,271:INFO:                pyod: 2.0.5
2025-10-29 10:01:44,271:INFO:            imblearn: 0.12.4
2025-10-29 10:01:44,271:INFO:   category_encoders: 2.6.4
2025-10-29 10:01:44,271:INFO:            lightgbm: 4.6.0
2025-10-29 10:01:44,271:INFO:               numba: 0.58.1
2025-10-29 10:01:44,271:INFO:            requests: 2.32.4
2025-10-29 10:01:44,271:INFO:          matplotlib: 3.6.0
2025-10-29 10:01:44,271:INFO:          scikitplot: 0.3.7
2025-10-29 10:01:44,271:INFO:         yellowbrick: 1.5
2025-10-29 10:01:44,271:INFO:              plotly: 6.3.0
2025-10-29 10:01:44,271:INFO:             kaleido: 1.1.0
2025-10-29 10:01:44,271:INFO:         statsmodels: 0.14.1
2025-10-29 10:01:44,271:INFO:              sktime: 0.21.1
2025-10-29 10:01:44,271:INFO:               tbats: 1.1.3
2025-10-29 10:01:44,271:INFO:            pmdarima: 2.0.4
2025-10-29 10:01:44,271:INFO:              psutil: 7.0.0
2025-10-29 10:01:44,271:INFO:PyCaret optional dependencies:
2025-10-29 10:01:44,391:INFO:                shap: Not installed
2025-10-29 10:01:44,391:INFO:           interpret: Not installed
2025-10-29 10:01:44,391:INFO:                umap: Not installed
2025-10-29 10:01:44,391:INFO:    pandas_profiling: Not installed
2025-10-29 10:01:44,391:INFO:  explainerdashboard: Not installed
2025-10-29 10:01:44,391:INFO:             autoviz: Not installed
2025-10-29 10:01:44,391:INFO:           fairlearn: Not installed
2025-10-29 10:01:44,391:INFO:             xgboost: 2.1.4
2025-10-29 10:01:44,391:INFO:            catboost: 1.2.8
2025-10-29 10:01:44,391:INFO:              kmodes: Not installed
2025-10-29 10:01:44,391:INFO:             mlxtend: Not installed
2025-10-29 10:01:44,391:INFO:       statsforecast: Not installed
2025-10-29 10:01:44,391:INFO:        tune_sklearn: Not installed
2025-10-29 10:01:44,391:INFO:                 ray: Not installed
2025-10-29 10:01:44,391:INFO:            hyperopt: Not installed
2025-10-29 10:01:44,392:INFO:              optuna: Not installed
2025-10-29 10:01:44,392:INFO:               skopt: Not installed
2025-10-29 10:01:44,392:INFO:              mlflow: Not installed
2025-10-29 10:01:44,392:INFO:              gradio: Not installed
2025-10-29 10:01:44,392:INFO:             fastapi: Not installed
2025-10-29 10:01:44,392:INFO:             uvicorn: Not installed
2025-10-29 10:01:44,392:INFO:              m2cgen: Not installed
2025-10-29 10:01:44,392:INFO:           evidently: Not installed
2025-10-29 10:01:44,392:INFO:               fugue: Not installed
2025-10-29 10:01:44,392:INFO:           streamlit: Not installed
2025-10-29 10:01:44,392:INFO:             prophet: Not installed
2025-10-29 10:01:44,392:INFO:None
2025-10-29 10:01:44,392:INFO:Set up data.
2025-10-29 10:01:44,398:INFO:Set up train/test split.
2025-10-29 10:01:44,404:INFO:Set up index.
2025-10-29 10:01:44,404:INFO:Set up folding strategy.
2025-10-29 10:01:44,404:INFO:Assigning column types.
2025-10-29 10:01:44,407:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-10-29 10:01:44,451:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-10-29 10:01:44,456:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-10-29 10:01:44,494:INFO:Soft dependency imported: xgboost: 2.1.4
2025-10-29 10:01:44,497:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-29 10:01:44,808:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-10-29 10:01:44,809:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-10-29 10:01:44,835:INFO:Soft dependency imported: xgboost: 2.1.4
2025-10-29 10:01:44,838:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-29 10:01:44,839:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-10-29 10:01:44,882:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-10-29 10:01:44,910:INFO:Soft dependency imported: xgboost: 2.1.4
2025-10-29 10:01:44,913:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-29 10:01:44,959:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-10-29 10:01:44,987:INFO:Soft dependency imported: xgboost: 2.1.4
2025-10-29 10:01:44,989:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-29 10:01:44,990:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-10-29 10:01:45,062:INFO:Soft dependency imported: xgboost: 2.1.4
2025-10-29 10:01:45,065:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-29 10:01:45,138:INFO:Soft dependency imported: xgboost: 2.1.4
2025-10-29 10:01:45,140:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-29 10:01:45,143:INFO:Preparing preprocessing pipeline...
2025-10-29 10:01:45,145:INFO:Set up date feature engineering.
2025-10-29 10:01:45,145:INFO:Set up simple imputation.
2025-10-29 10:01:45,148:INFO:Set up encoding of categorical features.
2025-10-29 10:01:45,148:INFO:Set up feature normalization.
2025-10-29 10:01:45,230:INFO:Finished creating preprocessing pipeline.
2025-10-29 10:01:45,240:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\hkh97\AppData\Local\Temp\joblib),
         steps=[('date_feature_extractor',
                 TransformerWrapper(exclude=None, include=['Timestamp'],
                                    transformer=ExtractDateTimeFeatures(features=['day',
                                                                                  'month',
                                                                                  'year']))),
                ('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Temperature', 'Pressure',
                                             'Vibration', 'GasFlow',
                                             'Stea...
                 TransformerWrapper(exclude=None, include=['UnitID'],
                                    transformer=OneHotEncoder(cols=['UnitID'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2025-10-29 10:01:45,240:INFO:Creating final display dataframe.
2025-10-29 10:01:45,478:INFO:Setup _display_container:                     Description             Value
0                    Session id            441239
1                        Target           Failure
2                   Target type            Binary
3           Original data shape         (4368, 8)
4        Transformed data shape        (4368, 13)
5   Transformed train set shape        (3494, 13)
6    Transformed test set shape         (874, 13)
7              Numeric features                 5
8                 Date features                 1
9          Categorical features                 1
10                   Preprocess              True
11              Imputation type            simple
12           Numeric imputation              mean
13       Categorical imputation              mode
14     Maximum one-hot encoding                25
15              Encoding method              None
16                    Normalize              True
17             Normalize method            zscore
18               Fold Generator   StratifiedKFold
19                  Fold Number                10
20                     CPU Jobs                -1
21                      Use GPU             False
22               Log Experiment             False
23              Experiment Name  clf-default-name
24                          USI              2777
2025-10-29 10:01:45,554:INFO:Soft dependency imported: xgboost: 2.1.4
2025-10-29 10:01:45,557:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-29 10:01:45,634:INFO:Soft dependency imported: xgboost: 2.1.4
2025-10-29 10:01:45,636:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-29 10:01:45,637:INFO:setup() successfully completed in 1.73s...............
2025-10-29 10:01:45,638:INFO:Initializing compare_models()
2025-10-29 10:01:45,638:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A428A98DF0>, include=None, fold=None, round=4, cross_validation=True, sort=F1, n_select=10, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001A428A98DF0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'F1', 'n_select': 10, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2025-10-29 10:01:45,638:INFO:Checking exceptions
2025-10-29 10:01:45,642:INFO:Preparing display monitor
2025-10-29 10:01:45,673:INFO:Initializing Logistic Regression
2025-10-29 10:01:45,674:INFO:Total runtime is 1.660585403442383e-05 minutes
2025-10-29 10:01:45,679:INFO:SubProcess create_model() called ==================================
2025-10-29 10:01:45,679:INFO:Initializing create_model()
2025-10-29 10:01:45,679:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A428A98DF0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A43CBF3AF0>, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:01:45,680:INFO:Checking exceptions
2025-10-29 10:01:45,680:INFO:Importing libraries
2025-10-29 10:01:45,680:INFO:Copying training dataset
2025-10-29 10:01:45,686:INFO:Defining folds
2025-10-29 10:01:45,686:INFO:Declaring metric variables
2025-10-29 10:01:45,691:INFO:Importing untrained model
2025-10-29 10:01:45,698:INFO:Logistic Regression Imported successfully
2025-10-29 10:01:45,715:INFO:Starting cross validation
2025-10-29 10:01:45,716:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 10:01:45,729:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 10:01:50,071:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:01:50,073:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:01:50,075:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:01:50,077:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:01:50,077:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:01:50,079:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:01:50,080:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:01:50,080:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:01:50,082:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:01:50,084:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:01:50,098:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:01:50,143:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:01:50,407:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:01:50,421:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:01:50,426:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:01:50,431:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:01:50,433:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:01:50,702:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:01:50,704:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:01:50,706:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:01:50,709:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:01:50,710:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:01:50,842:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:01:50,843:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:01:50,846:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:01:50,848:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:01:50,849:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:01:50,851:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:01:50,852:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:01:50,853:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:01:50,855:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:01:50,856:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:01:51,007:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:01:51,010:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:01:51,012:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:01:51,016:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:01:51,017:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:01:51,179:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:01:51,181:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:01:51,184:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:01:51,186:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:01:51,187:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:35,813:INFO:PyCaret ClassificationExperiment
2025-10-29 10:03:35,813:INFO:Logging name: clf-default-name
2025-10-29 10:03:35,813:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-10-29 10:03:35,813:INFO:version 3.0.0
2025-10-29 10:03:35,813:INFO:Initializing setup()
2025-10-29 10:03:35,813:INFO:self.USI: 4935
2025-10-29 10:03:35,813:INFO:self._variable_keys: {'target_param', 'seed', 'y_train', 'fold_shuffle_param', 'X_train', 'is_multiclass', 'y_test', '_ml_usecase', 'X', 'log_plots_param', 'USI', 'exp_id', 'pipeline', 'idx', 'y', '_available_plots', 'html_param', 'fold_groups_param', 'exp_name_log', 'X_test', 'fold_generator', 'logging_param', 'gpu_n_jobs_param', 'n_jobs_param', 'memory', 'data', 'gpu_param', 'fix_imbalance'}
2025-10-29 10:03:35,813:INFO:Checking environment
2025-10-29 10:03:35,813:INFO:python_version: 3.8.15
2025-10-29 10:03:35,813:INFO:python_build: ('default', 'Nov 24 2022 14:38:14')
2025-10-29 10:03:35,814:INFO:machine: AMD64
2025-10-29 10:03:35,814:INFO:platform: Windows-10-10.0.19045-SP0
2025-10-29 10:03:35,816:INFO:Memory: svmem(total=34299187200, available=23387574272, percent=31.8, used=10911612928, free=23387574272)
2025-10-29 10:03:35,816:INFO:Physical Core: 6
2025-10-29 10:03:35,816:INFO:Logical Core: 6
2025-10-29 10:03:35,816:INFO:Checking libraries
2025-10-29 10:03:35,816:INFO:System:
2025-10-29 10:03:35,816:INFO:    python: 3.8.15 (default, Nov 24 2022, 14:38:14) [MSC v.1916 64 bit (AMD64)]
2025-10-29 10:03:35,816:INFO:executable: c:\Users\hkh97\anaconda3\envs\han\python.exe
2025-10-29 10:03:35,816:INFO:   machine: Windows-10-10.0.19045-SP0
2025-10-29 10:03:35,816:INFO:PyCaret required dependencies:
2025-10-29 10:03:35,816:INFO:                 pip: 25.0.1
2025-10-29 10:03:35,816:INFO:          setuptools: 75.1.0
2025-10-29 10:03:35,816:INFO:             pycaret: 3.0.0
2025-10-29 10:03:35,816:INFO:             IPython: 8.12.3
2025-10-29 10:03:35,816:INFO:          ipywidgets: 8.1.7
2025-10-29 10:03:35,816:INFO:                tqdm: 4.67.1
2025-10-29 10:03:35,816:INFO:               numpy: 1.24.1
2025-10-29 10:03:35,817:INFO:              pandas: 1.5.3
2025-10-29 10:03:35,817:INFO:              jinja2: 3.1.4
2025-10-29 10:03:35,817:INFO:               scipy: 1.10.1
2025-10-29 10:03:35,817:INFO:              joblib: 1.2.0
2025-10-29 10:03:35,817:INFO:             sklearn: 1.2.2
2025-10-29 10:03:35,817:INFO:                pyod: 2.0.5
2025-10-29 10:03:35,817:INFO:            imblearn: 0.12.4
2025-10-29 10:03:35,817:INFO:   category_encoders: 2.6.4
2025-10-29 10:03:35,817:INFO:            lightgbm: 4.6.0
2025-10-29 10:03:35,817:INFO:               numba: 0.58.1
2025-10-29 10:03:35,817:INFO:            requests: 2.32.4
2025-10-29 10:03:35,817:INFO:          matplotlib: 3.6.0
2025-10-29 10:03:35,817:INFO:          scikitplot: 0.3.7
2025-10-29 10:03:35,817:INFO:         yellowbrick: 1.5
2025-10-29 10:03:35,817:INFO:              plotly: 6.3.0
2025-10-29 10:03:35,817:INFO:             kaleido: 1.1.0
2025-10-29 10:03:35,817:INFO:         statsmodels: 0.14.1
2025-10-29 10:03:35,817:INFO:              sktime: 0.21.1
2025-10-29 10:03:35,817:INFO:               tbats: 1.1.3
2025-10-29 10:03:35,817:INFO:            pmdarima: 2.0.4
2025-10-29 10:03:35,818:INFO:              psutil: 7.0.0
2025-10-29 10:03:35,818:INFO:PyCaret optional dependencies:
2025-10-29 10:03:35,818:INFO:                shap: Not installed
2025-10-29 10:03:35,818:INFO:           interpret: Not installed
2025-10-29 10:03:35,818:INFO:                umap: Not installed
2025-10-29 10:03:35,818:INFO:    pandas_profiling: Not installed
2025-10-29 10:03:35,818:INFO:  explainerdashboard: Not installed
2025-10-29 10:03:35,818:INFO:             autoviz: Not installed
2025-10-29 10:03:35,818:INFO:           fairlearn: Not installed
2025-10-29 10:03:35,818:INFO:             xgboost: 2.1.4
2025-10-29 10:03:35,818:INFO:            catboost: 1.2.8
2025-10-29 10:03:35,818:INFO:              kmodes: Not installed
2025-10-29 10:03:35,818:INFO:             mlxtend: Not installed
2025-10-29 10:03:35,818:INFO:       statsforecast: Not installed
2025-10-29 10:03:35,818:INFO:        tune_sklearn: Not installed
2025-10-29 10:03:35,818:INFO:                 ray: Not installed
2025-10-29 10:03:35,818:INFO:            hyperopt: Not installed
2025-10-29 10:03:35,818:INFO:              optuna: Not installed
2025-10-29 10:03:35,818:INFO:               skopt: Not installed
2025-10-29 10:03:35,818:INFO:              mlflow: Not installed
2025-10-29 10:03:35,819:INFO:              gradio: Not installed
2025-10-29 10:03:35,819:INFO:             fastapi: Not installed
2025-10-29 10:03:35,819:INFO:             uvicorn: Not installed
2025-10-29 10:03:35,819:INFO:              m2cgen: Not installed
2025-10-29 10:03:35,819:INFO:           evidently: Not installed
2025-10-29 10:03:35,819:INFO:               fugue: Not installed
2025-10-29 10:03:35,819:INFO:           streamlit: Not installed
2025-10-29 10:03:35,819:INFO:             prophet: Not installed
2025-10-29 10:03:35,819:INFO:None
2025-10-29 10:03:35,819:INFO:Set up data.
2025-10-29 10:03:35,823:INFO:Set up train/test split.
2025-10-29 10:03:35,827:INFO:Set up index.
2025-10-29 10:03:35,827:INFO:Set up folding strategy.
2025-10-29 10:03:35,827:INFO:Assigning column types.
2025-10-29 10:03:35,830:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-10-29 10:03:35,873:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-10-29 10:03:35,874:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-10-29 10:03:35,904:INFO:Soft dependency imported: xgboost: 2.1.4
2025-10-29 10:03:35,907:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-29 10:03:35,958:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-10-29 10:03:35,959:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-10-29 10:03:35,986:INFO:Soft dependency imported: xgboost: 2.1.4
2025-10-29 10:03:35,988:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-29 10:03:35,989:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-10-29 10:03:36,034:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-10-29 10:03:36,060:INFO:Soft dependency imported: xgboost: 2.1.4
2025-10-29 10:03:36,063:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-29 10:03:36,107:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-10-29 10:03:36,135:INFO:Soft dependency imported: xgboost: 2.1.4
2025-10-29 10:03:36,137:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-29 10:03:36,138:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-10-29 10:03:36,212:INFO:Soft dependency imported: xgboost: 2.1.4
2025-10-29 10:03:36,215:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-29 10:03:36,288:INFO:Soft dependency imported: xgboost: 2.1.4
2025-10-29 10:03:36,291:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-29 10:03:36,292:INFO:Preparing preprocessing pipeline...
2025-10-29 10:03:36,293:INFO:Set up simple imputation.
2025-10-29 10:03:36,294:INFO:Set up feature normalization.
2025-10-29 10:03:36,313:INFO:Finished creating preprocessing pipeline.
2025-10-29 10:03:36,318:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\hkh97\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Temperature', 'Pressure',
                                             'Vibration', 'GasFlow',
                                             'SteamOutput'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean',
                                                              verbose=...
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent',
                                                              verbose='deprecated'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2025-10-29 10:03:36,318:INFO:Creating final display dataframe.
2025-10-29 10:03:36,403:INFO:Setup _display_container:                     Description             Value
0                    Session id            441239
1                        Target           Failure
2                   Target type            Binary
3           Original data shape         (4368, 6)
4        Transformed data shape         (4368, 6)
5   Transformed train set shape         (3494, 6)
6    Transformed test set shape          (874, 6)
7              Numeric features                 5
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                    Normalize              True
13             Normalize method            zscore
14               Fold Generator   StratifiedKFold
15                  Fold Number                10
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              4935
2025-10-29 10:03:36,477:INFO:Soft dependency imported: xgboost: 2.1.4
2025-10-29 10:03:36,480:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-29 10:03:36,558:INFO:Soft dependency imported: xgboost: 2.1.4
2025-10-29 10:03:36,560:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-29 10:03:36,562:INFO:setup() successfully completed in 0.88s...............
2025-10-29 10:03:36,562:INFO:Initializing compare_models()
2025-10-29 10:03:36,562:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A42ABF7850>, include=None, fold=None, round=4, cross_validation=True, sort=F1, n_select=10, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001A42ABF7850>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'F1', 'n_select': 10, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2025-10-29 10:03:36,562:INFO:Checking exceptions
2025-10-29 10:03:36,565:INFO:Preparing display monitor
2025-10-29 10:03:36,588:INFO:Initializing Logistic Regression
2025-10-29 10:03:36,588:INFO:Total runtime is 0.0 minutes
2025-10-29 10:03:36,593:INFO:SubProcess create_model() called ==================================
2025-10-29 10:03:36,593:INFO:Initializing create_model()
2025-10-29 10:03:36,594:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A42ABF7850>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A44E73C400>, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:03:36,594:INFO:Checking exceptions
2025-10-29 10:03:36,594:INFO:Importing libraries
2025-10-29 10:03:36,594:INFO:Copying training dataset
2025-10-29 10:03:36,600:INFO:Defining folds
2025-10-29 10:03:36,600:INFO:Declaring metric variables
2025-10-29 10:03:36,605:INFO:Importing untrained model
2025-10-29 10:03:36,609:INFO:Logistic Regression Imported successfully
2025-10-29 10:03:36,619:INFO:Starting cross validation
2025-10-29 10:03:36,621:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 10:03:36,624:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 10:03:39,963:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:39,967:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:39,969:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:39,971:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:39,974:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:39,978:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:39,981:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:39,983:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:39,986:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:39,987:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:40,018:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:40,025:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:40,047:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:40,051:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:40,054:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:40,057:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:40,059:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:40,060:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:40,061:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:40,066:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:40,068:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:40,069:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:40,300:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:40,302:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:40,302:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:40,304:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:40,304:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:40,306:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:40,307:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:40,308:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:40,309:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:40,309:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:40,332:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:40,334:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:40,336:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:40,338:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:40,339:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:40,365:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:40,367:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:40,369:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:40,372:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:40,373:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:41,156:INFO:Calculating mean and std
2025-10-29 10:03:41,157:INFO:Creating metrics dataframe
2025-10-29 10:03:41,345:INFO:Uploading results into container
2025-10-29 10:03:41,346:INFO:Uploading model into container now
2025-10-29 10:03:41,346:INFO:_master_model_container: 1
2025-10-29 10:03:41,346:INFO:_display_container: 2
2025-10-29 10:03:41,347:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=441239, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-10-29 10:03:41,347:INFO:create_model() successfully completed......................................
2025-10-29 10:03:41,762:INFO:SubProcess create_model() end ==================================
2025-10-29 10:03:41,762:INFO:Creating metrics dataframe
2025-10-29 10:03:41,772:INFO:Initializing K Neighbors Classifier
2025-10-29 10:03:41,772:INFO:Total runtime is 0.08640233675638835 minutes
2025-10-29 10:03:41,776:INFO:SubProcess create_model() called ==================================
2025-10-29 10:03:41,776:INFO:Initializing create_model()
2025-10-29 10:03:41,776:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A42ABF7850>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A44E73C400>, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:03:41,776:INFO:Checking exceptions
2025-10-29 10:03:41,776:INFO:Importing libraries
2025-10-29 10:03:41,776:INFO:Copying training dataset
2025-10-29 10:03:41,782:INFO:Defining folds
2025-10-29 10:03:41,782:INFO:Declaring metric variables
2025-10-29 10:03:41,787:INFO:Importing untrained model
2025-10-29 10:03:41,792:INFO:K Neighbors Classifier Imported successfully
2025-10-29 10:03:41,800:INFO:Starting cross validation
2025-10-29 10:03:41,801:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 10:03:41,802:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 10:03:41,970:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:41,973:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:41,979:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:41,984:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:41,986:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:41,986:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:41,993:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:41,996:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:41,997:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:41,999:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:41,999:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:42,000:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:42,020:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:42,023:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:42,025:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:42,027:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:42,028:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:42,031:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:42,037:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:42,040:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:42,042:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:42,043:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:42,360:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:42,362:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:42,365:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:42,367:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:42,368:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:42,380:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:42,382:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:42,386:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:42,387:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:42,389:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:42,389:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:42,390:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:42,392:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:42,394:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:42,396:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:42,409:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:42,412:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:42,414:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:42,417:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:42,418:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:43,308:INFO:Calculating mean and std
2025-10-29 10:03:43,310:INFO:Creating metrics dataframe
2025-10-29 10:03:43,503:INFO:Uploading results into container
2025-10-29 10:03:43,504:INFO:Uploading model into container now
2025-10-29 10:03:43,504:INFO:_master_model_container: 2
2025-10-29 10:03:43,504:INFO:_display_container: 2
2025-10-29 10:03:43,505:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-10-29 10:03:43,505:INFO:create_model() successfully completed......................................
2025-10-29 10:03:43,675:INFO:SubProcess create_model() end ==================================
2025-10-29 10:03:43,675:INFO:Creating metrics dataframe
2025-10-29 10:03:43,687:INFO:Initializing Naive Bayes
2025-10-29 10:03:43,687:INFO:Total runtime is 0.11831700007120768 minutes
2025-10-29 10:03:43,691:INFO:SubProcess create_model() called ==================================
2025-10-29 10:03:43,691:INFO:Initializing create_model()
2025-10-29 10:03:43,692:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A42ABF7850>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A44E73C400>, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:03:43,692:INFO:Checking exceptions
2025-10-29 10:03:43,692:INFO:Importing libraries
2025-10-29 10:03:43,692:INFO:Copying training dataset
2025-10-29 10:03:43,697:INFO:Defining folds
2025-10-29 10:03:43,697:INFO:Declaring metric variables
2025-10-29 10:03:43,701:INFO:Importing untrained model
2025-10-29 10:03:43,706:INFO:Naive Bayes Imported successfully
2025-10-29 10:03:43,714:INFO:Starting cross validation
2025-10-29 10:03:43,715:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 10:03:43,717:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 10:03:43,820:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:43,823:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:43,825:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:43,826:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:43,827:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:43,828:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:43,829:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:43,830:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:43,831:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:43,832:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:43,832:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:43,833:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:43,835:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:43,836:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:43,836:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:43,837:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:43,840:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:43,847:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:43,850:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:43,852:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:43,854:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:43,855:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:44,074:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:44,076:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:44,079:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:44,081:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:44,082:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:44,084:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:44,087:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:44,089:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:44,089:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:44,090:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:44,091:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:44,092:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:44,092:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:44,093:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:44,094:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:44,096:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:44,096:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:44,097:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:44,097:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:44,099:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:45,003:INFO:Calculating mean and std
2025-10-29 10:03:45,004:INFO:Creating metrics dataframe
2025-10-29 10:03:45,197:INFO:Uploading results into container
2025-10-29 10:03:45,198:INFO:Uploading model into container now
2025-10-29 10:03:45,198:INFO:_master_model_container: 3
2025-10-29 10:03:45,198:INFO:_display_container: 2
2025-10-29 10:03:45,199:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-10-29 10:03:45,199:INFO:create_model() successfully completed......................................
2025-10-29 10:03:45,375:INFO:SubProcess create_model() end ==================================
2025-10-29 10:03:45,375:INFO:Creating metrics dataframe
2025-10-29 10:03:45,387:INFO:Initializing Decision Tree Classifier
2025-10-29 10:03:45,387:INFO:Total runtime is 0.14664936860402425 minutes
2025-10-29 10:03:45,390:INFO:SubProcess create_model() called ==================================
2025-10-29 10:03:45,391:INFO:Initializing create_model()
2025-10-29 10:03:45,391:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A42ABF7850>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A44E73C400>, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:03:45,391:INFO:Checking exceptions
2025-10-29 10:03:45,391:INFO:Importing libraries
2025-10-29 10:03:45,392:INFO:Copying training dataset
2025-10-29 10:03:45,396:INFO:Defining folds
2025-10-29 10:03:45,396:INFO:Declaring metric variables
2025-10-29 10:03:45,402:INFO:Importing untrained model
2025-10-29 10:03:45,407:INFO:Decision Tree Classifier Imported successfully
2025-10-29 10:03:45,416:INFO:Starting cross validation
2025-10-29 10:03:45,417:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 10:03:45,421:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 10:03:45,567:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:45,569:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:45,570:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:45,573:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:45,574:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:45,575:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:45,575:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:45,577:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:45,578:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:45,580:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:45,581:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:45,581:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:45,582:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:45,631:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:45,633:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:45,636:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:45,639:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:45,641:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:45,871:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:45,874:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:45,878:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:45,880:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:45,882:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:45,883:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:45,887:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:45,890:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:45,895:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:45,898:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:45,900:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:45,903:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:45,908:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:45,911:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:45,912:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:45,913:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:45,914:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:45,917:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:45,920:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:45,922:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:46,822:INFO:Calculating mean and std
2025-10-29 10:03:46,823:INFO:Creating metrics dataframe
2025-10-29 10:03:47,007:INFO:Uploading results into container
2025-10-29 10:03:47,008:INFO:Uploading model into container now
2025-10-29 10:03:47,008:INFO:_master_model_container: 4
2025-10-29 10:03:47,008:INFO:_display_container: 2
2025-10-29 10:03:47,009:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=441239, splitter='best')
2025-10-29 10:03:47,009:INFO:create_model() successfully completed......................................
2025-10-29 10:03:47,173:INFO:SubProcess create_model() end ==================================
2025-10-29 10:03:47,173:INFO:Creating metrics dataframe
2025-10-29 10:03:47,184:INFO:Initializing SVM - Linear Kernel
2025-10-29 10:03:47,184:INFO:Total runtime is 0.17660262187321982 minutes
2025-10-29 10:03:47,188:INFO:SubProcess create_model() called ==================================
2025-10-29 10:03:47,188:INFO:Initializing create_model()
2025-10-29 10:03:47,188:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A42ABF7850>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A44E73C400>, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:03:47,188:INFO:Checking exceptions
2025-10-29 10:03:47,188:INFO:Importing libraries
2025-10-29 10:03:47,189:INFO:Copying training dataset
2025-10-29 10:03:47,193:INFO:Defining folds
2025-10-29 10:03:47,193:INFO:Declaring metric variables
2025-10-29 10:03:47,197:INFO:Importing untrained model
2025-10-29 10:03:47,202:INFO:SVM - Linear Kernel Imported successfully
2025-10-29 10:03:47,210:INFO:Starting cross validation
2025-10-29 10:03:47,211:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 10:03:47,214:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 10:03:47,297:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 10:03:47,299:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:47,301:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:47,303:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 10:03:47,303:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:47,304:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:47,305:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:47,308:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 10:03:47,309:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 10:03:47,314:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 10:03:47,318:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:47,320:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:47,322:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:47,323:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:47,324:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:47,324:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:47,325:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:47,333:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 10:03:47,336:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:47,338:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:47,340:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:47,341:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:47,524:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 10:03:47,527:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:47,529:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:47,531:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:47,532:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:47,540:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 10:03:47,541:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:47,543:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 10:03:47,544:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:47,545:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:47,547:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:47,547:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:47,547:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 10:03:47,548:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:47,549:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

"F-score is", len(true_sum))

2025-10-29 10:03:47,550:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:47,551:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:47,553:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:47,554:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:48,482:INFO:Calculating mean and std
2025-10-29 10:03:48,483:INFO:Creating metrics dataframe
2025-10-29 10:03:48,667:INFO:Uploading results into container
2025-10-29 10:03:48,668:INFO:Uploading model into container now
2025-10-29 10:03:48,669:INFO:_master_model_container: 5
2025-10-29 10:03:48,669:INFO:_display_container: 2
2025-10-29 10:03:48,669:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=441239, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2025-10-29 10:03:48,669:INFO:create_model() successfully completed......................................
2025-10-29 10:03:48,836:INFO:SubProcess create_model() end ==================================
2025-10-29 10:03:48,836:INFO:Creating metrics dataframe
2025-10-29 10:03:48,847:INFO:Initializing Ridge Classifier
2025-10-29 10:03:48,847:INFO:Total runtime is 0.20431187550226848 minutes
2025-10-29 10:03:48,852:INFO:SubProcess create_model() called ==================================
2025-10-29 10:03:48,852:INFO:Initializing create_model()
2025-10-29 10:03:48,852:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A42ABF7850>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A44E73C400>, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:03:48,852:INFO:Checking exceptions
2025-10-29 10:03:48,853:INFO:Importing libraries
2025-10-29 10:03:48,853:INFO:Copying training dataset
2025-10-29 10:03:48,858:INFO:Defining folds
2025-10-29 10:03:48,858:INFO:Declaring metric variables
2025-10-29 10:03:48,862:INFO:Importing untrained model
2025-10-29 10:03:48,867:INFO:Ridge Classifier Imported successfully
2025-10-29 10:03:48,875:INFO:Starting cross validation
2025-10-29 10:03:48,876:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 10:03:48,878:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 10:03:48,965:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2025-10-29 10:03:48,969:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2025-10-29 10:03:48,969:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2025-10-29 10:03:48,970:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:48,971:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:48,971:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:48,972:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2025-10-29 10:03:48,972:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2025-10-29 10:03:48,973:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:48,975:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:48,975:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:48,977:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:48,977:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:48,977:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:48,978:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:48,978:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:48,979:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:48,979:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:48,980:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:48,998:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2025-10-29 10:03:49,001:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:49,003:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:49,005:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:49,006:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:49,201:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2025-10-29 10:03:49,202:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2025-10-29 10:03:49,203:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:49,204:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:49,205:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:49,207:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:49,208:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:49,209:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:49,209:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:49,210:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:49,220:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2025-10-29 10:03:49,222:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:49,224:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:49,227:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:49,228:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:49,229:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2025-10-29 10:03:49,231:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:49,234:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:49,235:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:49,236:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:50,159:INFO:Calculating mean and std
2025-10-29 10:03:50,160:INFO:Creating metrics dataframe
2025-10-29 10:03:50,339:INFO:Uploading results into container
2025-10-29 10:03:50,339:INFO:Uploading model into container now
2025-10-29 10:03:50,340:INFO:_master_model_container: 6
2025-10-29 10:03:50,340:INFO:_display_container: 2
2025-10-29 10:03:50,340:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=441239,
                solver='auto', tol=0.0001)
2025-10-29 10:03:50,340:INFO:create_model() successfully completed......................................
2025-10-29 10:03:50,510:INFO:SubProcess create_model() end ==================================
2025-10-29 10:03:50,510:INFO:Creating metrics dataframe
2025-10-29 10:03:50,524:INFO:Initializing Random Forest Classifier
2025-10-29 10:03:50,524:INFO:Total runtime is 0.23227047125498454 minutes
2025-10-29 10:03:50,528:INFO:SubProcess create_model() called ==================================
2025-10-29 10:03:50,528:INFO:Initializing create_model()
2025-10-29 10:03:50,528:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A42ABF7850>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A44E73C400>, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:03:50,528:INFO:Checking exceptions
2025-10-29 10:03:50,528:INFO:Importing libraries
2025-10-29 10:03:50,528:INFO:Copying training dataset
2025-10-29 10:03:50,534:INFO:Defining folds
2025-10-29 10:03:50,534:INFO:Declaring metric variables
2025-10-29 10:03:50,539:INFO:Importing untrained model
2025-10-29 10:03:50,543:INFO:Random Forest Classifier Imported successfully
2025-10-29 10:03:50,552:INFO:Starting cross validation
2025-10-29 10:03:50,553:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 10:03:50,555:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 10:03:51,228:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:51,230:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:51,233:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:51,234:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:51,235:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:51,236:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:51,236:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:51,239:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:51,241:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:51,242:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:51,249:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:51,250:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:51,285:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:51,287:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:51,292:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:51,295:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:51,296:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:51,452:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:51,455:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:51,457:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:51,459:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:51,460:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:52,026:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:52,026:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:52,028:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:52,028:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:52,030:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:52,031:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:52,032:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:52,033:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:52,033:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:52,034:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:52,046:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:52,048:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:52,050:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:52,052:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:52,053:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:52,117:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:52,120:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:52,122:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:52,124:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:52,125:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:52,609:INFO:Calculating mean and std
2025-10-29 10:03:52,610:INFO:Creating metrics dataframe
2025-10-29 10:03:52,802:INFO:Uploading results into container
2025-10-29 10:03:52,803:INFO:Uploading model into container now
2025-10-29 10:03:52,803:INFO:_master_model_container: 7
2025-10-29 10:03:52,803:INFO:_display_container: 2
2025-10-29 10:03:52,804:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=441239, verbose=0, warm_start=False)
2025-10-29 10:03:52,804:INFO:create_model() successfully completed......................................
2025-10-29 10:03:52,971:INFO:SubProcess create_model() end ==================================
2025-10-29 10:03:52,971:INFO:Creating metrics dataframe
2025-10-29 10:03:52,984:INFO:Initializing Quadratic Discriminant Analysis
2025-10-29 10:03:52,985:INFO:Total runtime is 0.2732775052388509 minutes
2025-10-29 10:03:52,989:INFO:SubProcess create_model() called ==================================
2025-10-29 10:03:52,989:INFO:Initializing create_model()
2025-10-29 10:03:52,989:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A42ABF7850>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A44E73C400>, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:03:52,989:INFO:Checking exceptions
2025-10-29 10:03:52,989:INFO:Importing libraries
2025-10-29 10:03:52,990:INFO:Copying training dataset
2025-10-29 10:03:52,994:INFO:Defining folds
2025-10-29 10:03:52,995:INFO:Declaring metric variables
2025-10-29 10:03:52,998:INFO:Importing untrained model
2025-10-29 10:03:53,002:INFO:Quadratic Discriminant Analysis Imported successfully
2025-10-29 10:03:53,011:INFO:Starting cross validation
2025-10-29 10:03:53,012:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 10:03:53,014:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 10:03:53,082:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-10-29 10:03:53,082:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-10-29 10:03:53,099:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-10-29 10:03:53,098:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-10-29 10:03:53,128:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:53,131:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:53,131:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:53,133:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:53,134:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:53,135:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:53,136:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:53,137:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:53,138:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:53,139:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:53,149:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:53,153:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:53,156:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:53,158:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:53,159:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:53,164:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:53,167:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:53,170:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:53,172:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:53,173:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:53,288:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-10-29 10:03:53,330:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:53,332:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:53,334:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:53,337:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:53,338:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:53,339:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-10-29 10:03:53,340:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-10-29 10:03:53,375:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-10-29 10:03:53,378:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:53,380:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:53,381:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:53,383:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:53,383:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:53,385:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:53,386:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:53,387:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:53,387:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:53,388:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:53,413:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:53,415:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:53,417:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:53,419:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:53,420:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:54,310:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_validation.py:378: FitFailedWarning: 
2 fits failed out of a total of 10.
The score on these train-test partitions for these parameters will be set to 0.
If these failures are not expected, you can try to debug them by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
2 fits failed with the following error:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\discriminant_analysis.py", line 917, in fit
    raise ValueError(
ValueError: y has only 1 sample in class 1, covariance is ill defined.

  warnings.warn(some_fits_failed_message, FitFailedWarning)

2025-10-29 10:03:54,310:INFO:Calculating mean and std
2025-10-29 10:03:54,311:INFO:Creating metrics dataframe
2025-10-29 10:03:54,504:INFO:Uploading results into container
2025-10-29 10:03:54,505:INFO:Uploading model into container now
2025-10-29 10:03:54,505:INFO:_master_model_container: 8
2025-10-29 10:03:54,506:INFO:_display_container: 2
2025-10-29 10:03:54,506:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2025-10-29 10:03:54,506:INFO:create_model() successfully completed......................................
2025-10-29 10:03:54,673:INFO:SubProcess create_model() end ==================================
2025-10-29 10:03:54,674:INFO:Creating metrics dataframe
2025-10-29 10:03:54,688:INFO:Initializing Ada Boost Classifier
2025-10-29 10:03:54,688:INFO:Total runtime is 0.3016740838686625 minutes
2025-10-29 10:03:54,691:INFO:SubProcess create_model() called ==================================
2025-10-29 10:03:54,692:INFO:Initializing create_model()
2025-10-29 10:03:54,692:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A42ABF7850>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A44E73C400>, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:03:54,692:INFO:Checking exceptions
2025-10-29 10:03:54,692:INFO:Importing libraries
2025-10-29 10:03:54,692:INFO:Copying training dataset
2025-10-29 10:03:54,697:INFO:Defining folds
2025-10-29 10:03:54,697:INFO:Declaring metric variables
2025-10-29 10:03:54,701:INFO:Importing untrained model
2025-10-29 10:03:54,706:INFO:Ada Boost Classifier Imported successfully
2025-10-29 10:03:54,714:INFO:Starting cross validation
2025-10-29 10:03:54,715:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 10:03:54,717:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 10:03:54,822:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:54,824:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:54,826:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:54,827:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:54,828:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:54,829:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:54,830:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:54,834:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:54,838:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:54,840:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:54,842:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:54,845:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:54,846:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:54,859:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:54,861:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:54,864:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:54,866:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:54,867:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:55,087:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:55,089:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:55,091:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:55,093:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:55,094:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:55,097:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:55,098:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:55,099:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:55,100:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:55,102:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:55,102:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:55,103:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:55,104:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:55,104:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:55,105:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:55,105:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:55,105:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:55,107:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:55,110:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:55,111:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:56,041:INFO:Calculating mean and std
2025-10-29 10:03:56,042:INFO:Creating metrics dataframe
2025-10-29 10:03:56,229:INFO:Uploading results into container
2025-10-29 10:03:56,229:INFO:Uploading model into container now
2025-10-29 10:03:56,230:INFO:_master_model_container: 9
2025-10-29 10:03:56,230:INFO:_display_container: 2
2025-10-29 10:03:56,230:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=441239)
2025-10-29 10:03:56,230:INFO:create_model() successfully completed......................................
2025-10-29 10:03:56,399:INFO:SubProcess create_model() end ==================================
2025-10-29 10:03:56,399:INFO:Creating metrics dataframe
2025-10-29 10:03:56,412:INFO:Initializing Gradient Boosting Classifier
2025-10-29 10:03:56,412:INFO:Total runtime is 0.330397625764211 minutes
2025-10-29 10:03:56,417:INFO:SubProcess create_model() called ==================================
2025-10-29 10:03:56,417:INFO:Initializing create_model()
2025-10-29 10:03:56,417:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A42ABF7850>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A44E73C400>, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:03:56,417:INFO:Checking exceptions
2025-10-29 10:03:56,417:INFO:Importing libraries
2025-10-29 10:03:56,417:INFO:Copying training dataset
2025-10-29 10:03:56,423:INFO:Defining folds
2025-10-29 10:03:56,423:INFO:Declaring metric variables
2025-10-29 10:03:56,427:INFO:Importing untrained model
2025-10-29 10:03:56,432:INFO:Gradient Boosting Classifier Imported successfully
2025-10-29 10:03:56,441:INFO:Starting cross validation
2025-10-29 10:03:56,442:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 10:03:56,444:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 10:03:56,980:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:56,993:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:56,996:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:56,998:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:56,999:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:57,000:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:57,001:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:57,001:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:57,022:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:57,025:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:57,025:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:57,027:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:57,027:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:57,029:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:57,029:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:57,031:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:57,032:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:57,033:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:57,687:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:57,689:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:57,693:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:57,695:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:57,696:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:57,705:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:57,708:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:57,710:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:57,712:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:57,713:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:57,726:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:57,728:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:57,730:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:57,731:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:57,733:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:57,733:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:57,734:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:57,735:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:57,737:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:57,739:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:58,313:INFO:Calculating mean and std
2025-10-29 10:03:58,314:INFO:Creating metrics dataframe
2025-10-29 10:03:58,508:INFO:Uploading results into container
2025-10-29 10:03:58,509:INFO:Uploading model into container now
2025-10-29 10:03:58,509:INFO:_master_model_container: 10
2025-10-29 10:03:58,509:INFO:_display_container: 2
2025-10-29 10:03:58,510:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=441239, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-10-29 10:03:58,510:INFO:create_model() successfully completed......................................
2025-10-29 10:03:58,681:INFO:SubProcess create_model() end ==================================
2025-10-29 10:03:58,681:INFO:Creating metrics dataframe
2025-10-29 10:03:58,694:INFO:Initializing Linear Discriminant Analysis
2025-10-29 10:03:58,694:INFO:Total runtime is 0.36842927932739256 minutes
2025-10-29 10:03:58,699:INFO:SubProcess create_model() called ==================================
2025-10-29 10:03:58,699:INFO:Initializing create_model()
2025-10-29 10:03:58,699:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A42ABF7850>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A44E73C400>, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:03:58,699:INFO:Checking exceptions
2025-10-29 10:03:58,699:INFO:Importing libraries
2025-10-29 10:03:58,700:INFO:Copying training dataset
2025-10-29 10:03:58,704:INFO:Defining folds
2025-10-29 10:03:58,704:INFO:Declaring metric variables
2025-10-29 10:03:58,708:INFO:Importing untrained model
2025-10-29 10:03:58,713:INFO:Linear Discriminant Analysis Imported successfully
2025-10-29 10:03:58,722:INFO:Starting cross validation
2025-10-29 10:03:58,723:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 10:03:58,724:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 10:03:58,836:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:58,837:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:58,838:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:58,839:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:58,839:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:58,840:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:58,841:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:58,841:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:58,842:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:58,844:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:58,844:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:58,845:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:58,845:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:58,845:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:58,846:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:58,854:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:58,856:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:58,858:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:58,861:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:58,861:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:59,187:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:59,188:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:59,189:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:59,191:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:59,192:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:59,194:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:59,194:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:59,195:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:59,196:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:59,198:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:59,228:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:59,230:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:59,231:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:03:59,232:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:59,234:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:59,234:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:59,235:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:03:59,236:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:03:59,238:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:03:59,240:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:00,220:INFO:Calculating mean and std
2025-10-29 10:04:00,221:INFO:Creating metrics dataframe
2025-10-29 10:04:00,423:INFO:Uploading results into container
2025-10-29 10:04:00,424:INFO:Uploading model into container now
2025-10-29 10:04:00,424:INFO:_master_model_container: 11
2025-10-29 10:04:00,424:INFO:_display_container: 2
2025-10-29 10:04:00,424:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2025-10-29 10:04:00,424:INFO:create_model() successfully completed......................................
2025-10-29 10:04:00,594:INFO:SubProcess create_model() end ==================================
2025-10-29 10:04:00,594:INFO:Creating metrics dataframe
2025-10-29 10:04:00,608:INFO:Initializing Extra Trees Classifier
2025-10-29 10:04:00,608:INFO:Total runtime is 0.40032735268274944 minutes
2025-10-29 10:04:00,613:INFO:SubProcess create_model() called ==================================
2025-10-29 10:04:00,613:INFO:Initializing create_model()
2025-10-29 10:04:00,613:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A42ABF7850>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A44E73C400>, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:04:00,613:INFO:Checking exceptions
2025-10-29 10:04:00,613:INFO:Importing libraries
2025-10-29 10:04:00,613:INFO:Copying training dataset
2025-10-29 10:04:00,618:INFO:Defining folds
2025-10-29 10:04:00,618:INFO:Declaring metric variables
2025-10-29 10:04:00,623:INFO:Importing untrained model
2025-10-29 10:04:00,628:INFO:Extra Trees Classifier Imported successfully
2025-10-29 10:04:00,636:INFO:Starting cross validation
2025-10-29 10:04:00,637:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 10:04:00,638:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 10:04:01,365:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:04:01,367:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:01,369:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:01,372:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:01,373:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:01,398:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:04:01,401:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:01,404:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:01,404:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:01,411:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:01,413:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:01,464:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:01,470:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:04:01,473:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:01,475:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:01,478:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:01,479:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:01,596:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:04:01,599:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:01,602:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:01,605:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:01,606:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:02,253:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:04:02,255:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:02,257:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:02,260:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:02,261:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:02,264:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:04:02,266:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:02,269:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:02,271:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:02,272:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:02,322:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:04:02,324:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:02,332:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:02,334:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:02,337:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:02,376:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:04:02,378:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:02,380:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:02,383:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:02,384:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:02,926:INFO:Calculating mean and std
2025-10-29 10:04:02,928:INFO:Creating metrics dataframe
2025-10-29 10:04:03,129:INFO:Uploading results into container
2025-10-29 10:04:03,130:INFO:Uploading model into container now
2025-10-29 10:04:03,130:INFO:_master_model_container: 12
2025-10-29 10:04:03,130:INFO:_display_container: 2
2025-10-29 10:04:03,131:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=441239, verbose=0, warm_start=False)
2025-10-29 10:04:03,131:INFO:create_model() successfully completed......................................
2025-10-29 10:04:03,308:INFO:SubProcess create_model() end ==================================
2025-10-29 10:04:03,309:INFO:Creating metrics dataframe
2025-10-29 10:04:03,323:INFO:Initializing Extreme Gradient Boosting
2025-10-29 10:04:03,323:INFO:Total runtime is 0.44558968146642053 minutes
2025-10-29 10:04:03,327:INFO:SubProcess create_model() called ==================================
2025-10-29 10:04:03,327:INFO:Initializing create_model()
2025-10-29 10:04:03,327:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A42ABF7850>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A44E73C400>, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:04:03,327:INFO:Checking exceptions
2025-10-29 10:04:03,327:INFO:Importing libraries
2025-10-29 10:04:03,327:INFO:Copying training dataset
2025-10-29 10:04:03,332:INFO:Defining folds
2025-10-29 10:04:03,332:INFO:Declaring metric variables
2025-10-29 10:04:03,336:INFO:Importing untrained model
2025-10-29 10:04:03,342:INFO:Extreme Gradient Boosting Imported successfully
2025-10-29 10:04:03,349:INFO:Starting cross validation
2025-10-29 10:04:03,351:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 10:04:03,353:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 10:04:04,160:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:04:04,163:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:04,163:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:04:04,163:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:04,166:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:04,166:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:04,168:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:04,168:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:04,169:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:04,172:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:04,173:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:04,179:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:04:04,183:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:04,183:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:04,186:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:04,189:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:04,190:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:04,221:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:04:04,223:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:04,226:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:04,229:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:04,230:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:04,522:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:04:04,522:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:04:04,525:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:04,525:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:04,527:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:04,527:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:04,529:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:04,529:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:04,530:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:04,530:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:04,537:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:04:04,539:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:04,541:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:04:04,541:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:04,543:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:04,543:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:04,544:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:04,545:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:04,548:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:04,548:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:05,530:INFO:Calculating mean and std
2025-10-29 10:04:05,531:INFO:Creating metrics dataframe
2025-10-29 10:04:05,740:INFO:Uploading results into container
2025-10-29 10:04:05,741:INFO:Uploading model into container now
2025-10-29 10:04:05,741:INFO:_master_model_container: 13
2025-10-29 10:04:05,741:INFO:_display_container: 2
2025-10-29 10:04:05,742:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device=None, early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2025-10-29 10:04:05,742:INFO:create_model() successfully completed......................................
2025-10-29 10:04:05,911:INFO:SubProcess create_model() end ==================================
2025-10-29 10:04:05,911:INFO:Creating metrics dataframe
2025-10-29 10:04:05,926:INFO:Initializing Light Gradient Boosting Machine
2025-10-29 10:04:05,926:INFO:Total runtime is 0.4889598488807679 minutes
2025-10-29 10:04:05,930:INFO:SubProcess create_model() called ==================================
2025-10-29 10:04:05,930:INFO:Initializing create_model()
2025-10-29 10:04:05,930:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A42ABF7850>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A44E73C400>, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:04:05,930:INFO:Checking exceptions
2025-10-29 10:04:05,931:INFO:Importing libraries
2025-10-29 10:04:05,931:INFO:Copying training dataset
2025-10-29 10:04:05,936:INFO:Defining folds
2025-10-29 10:04:05,936:INFO:Declaring metric variables
2025-10-29 10:04:05,940:INFO:Importing untrained model
2025-10-29 10:04:05,944:INFO:Light Gradient Boosting Machine Imported successfully
2025-10-29 10:04:05,952:INFO:Starting cross validation
2025-10-29 10:04:05,953:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 10:04:05,955:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 10:04:06,151:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:06,330:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:04:06,333:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:06,336:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:06,338:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:06,340:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:06,534:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:04:06,536:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:06,539:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:06,542:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:06,543:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:06,637:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:04:06,639:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:06,641:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:06,644:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:06,645:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:06,722:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:04:06,724:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:06,727:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:06,729:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:06,730:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:06,922:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:04:06,924:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:06,926:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:06,930:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:06,931:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:06,967:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:04:06,968:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:06,971:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:06,974:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:06,975:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:07,015:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:04:07,017:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:07,193:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:04:07,195:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:07,198:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:07,201:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:07,202:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:07,553:INFO:Calculating mean and std
2025-10-29 10:04:07,554:INFO:Creating metrics dataframe
2025-10-29 10:04:07,768:INFO:Uploading results into container
2025-10-29 10:04:07,768:INFO:Uploading model into container now
2025-10-29 10:04:07,769:INFO:_master_model_container: 14
2025-10-29 10:04:07,769:INFO:_display_container: 2
2025-10-29 10:04:07,770:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=441239, reg_alpha=0.0, reg_lambda=0.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2025-10-29 10:04:07,770:INFO:create_model() successfully completed......................................
2025-10-29 10:04:07,940:INFO:SubProcess create_model() end ==================================
2025-10-29 10:04:07,941:INFO:Creating metrics dataframe
2025-10-29 10:04:07,958:INFO:Initializing CatBoost Classifier
2025-10-29 10:04:07,959:INFO:Total runtime is 0.5228359659512838 minutes
2025-10-29 10:04:07,963:INFO:SubProcess create_model() called ==================================
2025-10-29 10:04:07,963:INFO:Initializing create_model()
2025-10-29 10:04:07,964:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A42ABF7850>, estimator=catboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A44E73C400>, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:04:07,964:INFO:Checking exceptions
2025-10-29 10:04:07,964:INFO:Importing libraries
2025-10-29 10:04:07,964:INFO:Copying training dataset
2025-10-29 10:04:07,971:INFO:Defining folds
2025-10-29 10:04:07,971:INFO:Declaring metric variables
2025-10-29 10:04:07,989:INFO:Importing untrained model
2025-10-29 10:04:08,007:INFO:CatBoost Classifier Imported successfully
2025-10-29 10:04:08,016:INFO:Starting cross validation
2025-10-29 10:04:08,018:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 10:04:08,021:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 10:04:14,717:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:04:14,719:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:14,722:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:14,724:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:14,725:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:14,833:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:04:14,836:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:14,843:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:14,849:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:14,851:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:14,888:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:04:14,891:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:14,893:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:14,895:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:14,896:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:14,904:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:04:14,906:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:14,909:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:14,911:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:14,912:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:14,973:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:04:14,975:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:14,977:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:14,979:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:14,980:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:15,369:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_validation.py:378: FitFailedWarning: 
5 fits failed out of a total of 10.
The score on these train-test partitions for these parameters will be set to 0.
If these failures are not expected, you can try to debug them by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
5 fits failed with the following error:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\catboost\core.py", line 5245, in fit
    self._fit(X, y, cat_features, text_features, embedding_features, None, graph, sample_weight, None, None, None, None, baseline, use_best_model,
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\catboost\core.py", line 2410, in _fit
    self._train(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\catboost\core.py", line 1790, in _train
    self._object._train(train_pool, test_pool, params, allow_clear_pool, init_model._object if init_model else None)
  File "_catboost.pyx", line 5023, in _catboost._CatBoost._train
  File "_catboost.pyx", line 5072, in _catboost._CatBoost._train
_catboost.CatBoostError: catboost/libs/train_lib/dir_helper.cpp:20: Can't create train working dir: catboost_info

  warnings.warn(some_fits_failed_message, FitFailedWarning)

2025-10-29 10:04:15,369:INFO:Calculating mean and std
2025-10-29 10:04:15,371:INFO:Creating metrics dataframe
2025-10-29 10:04:15,585:INFO:Uploading results into container
2025-10-29 10:04:15,586:INFO:Uploading model into container now
2025-10-29 10:04:15,586:INFO:_master_model_container: 15
2025-10-29 10:04:15,586:INFO:_display_container: 2
2025-10-29 10:04:15,586:INFO:<catboost.core.CatBoostClassifier object at 0x000001A4265B6F40>
2025-10-29 10:04:15,586:INFO:create_model() successfully completed......................................
2025-10-29 10:04:15,809:WARNING:create_model() for <catboost.core.CatBoostClassifier object at 0x000001A4265B6F40> raised an exception or returned all 0.0, trying without fit_kwargs:
2025-10-29 10:04:15,810:WARNING:Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 796, in compare_models
    assert (
AssertionError

2025-10-29 10:04:15,810:INFO:Initializing create_model()
2025-10-29 10:04:15,810:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A42ABF7850>, estimator=catboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A44E73C400>, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:04:15,810:INFO:Checking exceptions
2025-10-29 10:04:15,810:INFO:Importing libraries
2025-10-29 10:04:15,810:INFO:Copying training dataset
2025-10-29 10:04:15,816:INFO:Defining folds
2025-10-29 10:04:15,816:INFO:Declaring metric variables
2025-10-29 10:04:15,822:INFO:Importing untrained model
2025-10-29 10:04:15,827:INFO:CatBoost Classifier Imported successfully
2025-10-29 10:04:15,837:INFO:Starting cross validation
2025-10-29 10:04:15,840:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 10:04:15,843:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 10:04:15,977:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:04:15,979:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:15,981:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:15,983:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:15,985:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:16,262:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:04:16,265:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:16,267:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:16,269:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:16,271:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:16,582:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:04:16,584:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:16,587:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:16,589:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:16,591:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:16,872:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:04:16,874:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:16,876:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:16,879:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:16,881:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:17,149:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:04:17,152:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:17,155:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:17,157:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:17,159:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:21,324:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:21,457:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:04:21,460:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:21,462:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:21,465:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:21,466:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:21,496:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:04:21,497:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:21,500:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:21,502:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:21,503:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:21,735:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:04:21,738:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:21,741:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:21,746:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:21,747:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:21,848:INFO:Calculating mean and std
2025-10-29 10:04:21,850:INFO:Creating metrics dataframe
2025-10-29 10:04:22,053:INFO:Uploading results into container
2025-10-29 10:04:22,054:INFO:Uploading model into container now
2025-10-29 10:04:22,055:INFO:_master_model_container: 16
2025-10-29 10:04:22,055:INFO:_display_container: 2
2025-10-29 10:04:22,055:INFO:<catboost.core.CatBoostClassifier object at 0x000001A425EA1400>
2025-10-29 10:04:22,055:INFO:create_model() successfully completed......................................
2025-10-29 10:04:22,221:INFO:SubProcess create_model() end ==================================
2025-10-29 10:04:22,221:INFO:Creating metrics dataframe
2025-10-29 10:04:22,236:INFO:Initializing Dummy Classifier
2025-10-29 10:04:22,236:INFO:Total runtime is 0.7607914527257283 minutes
2025-10-29 10:04:22,245:INFO:SubProcess create_model() called ==================================
2025-10-29 10:04:22,246:INFO:Initializing create_model()
2025-10-29 10:04:22,246:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A42ABF7850>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A44E73C400>, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:04:22,246:INFO:Checking exceptions
2025-10-29 10:04:22,246:INFO:Importing libraries
2025-10-29 10:04:22,246:INFO:Copying training dataset
2025-10-29 10:04:22,252:INFO:Defining folds
2025-10-29 10:04:22,252:INFO:Declaring metric variables
2025-10-29 10:04:22,256:INFO:Importing untrained model
2025-10-29 10:04:22,260:INFO:Dummy Classifier Imported successfully
2025-10-29 10:04:22,267:INFO:Starting cross validation
2025-10-29 10:04:22,268:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 10:04:22,270:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 10:04:22,358:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:04:22,359:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:04:22,360:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:22,362:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:22,363:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:22,364:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:22,365:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:22,366:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:22,366:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:22,371:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:22,372:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:22,378:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:04:22,380:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:22,382:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:22,383:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:22,384:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:04:22,385:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:22,386:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:22,386:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:22,389:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:22,391:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:22,392:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:22,613:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:04:22,615:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:22,617:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:22,619:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:22,620:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:22,641:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:04:22,642:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:22,644:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:22,647:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:22,648:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:22,648:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:04:22,650:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:22,653:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:22,653:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:04:22,656:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:22,656:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:22,657:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:22,658:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:22,661:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:22,662:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:23,764:INFO:Calculating mean and std
2025-10-29 10:04:23,765:INFO:Creating metrics dataframe
2025-10-29 10:04:23,981:INFO:Uploading results into container
2025-10-29 10:04:23,981:INFO:Uploading model into container now
2025-10-29 10:04:23,982:INFO:_master_model_container: 17
2025-10-29 10:04:23,982:INFO:_display_container: 2
2025-10-29 10:04:23,982:INFO:DummyClassifier(constant=None, random_state=441239, strategy='prior')
2025-10-29 10:04:23,982:INFO:create_model() successfully completed......................................
2025-10-29 10:04:24,146:INFO:SubProcess create_model() end ==================================
2025-10-29 10:04:24,146:INFO:Creating metrics dataframe
2025-10-29 10:04:24,172:INFO:Initializing create_model()
2025-10-29 10:04:24,172:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A42ABF7850>, estimator=SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=441239, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:04:24,172:INFO:Checking exceptions
2025-10-29 10:04:24,174:INFO:Importing libraries
2025-10-29 10:04:24,174:INFO:Copying training dataset
2025-10-29 10:04:24,178:INFO:Defining folds
2025-10-29 10:04:24,178:INFO:Declaring metric variables
2025-10-29 10:04:24,178:INFO:Importing untrained model
2025-10-29 10:04:24,178:INFO:Declaring custom model
2025-10-29 10:04:24,179:INFO:SVM - Linear Kernel Imported successfully
2025-10-29 10:04:24,180:INFO:Cross validation set to False
2025-10-29 10:04:24,180:INFO:Fitting Model
2025-10-29 10:04:24,351:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=441239, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2025-10-29 10:04:24,352:INFO:create_model() successfully completed......................................
2025-10-29 10:04:24,525:INFO:Initializing create_model()
2025-10-29 10:04:24,525:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A42ABF7850>, estimator=LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:04:24,525:INFO:Checking exceptions
2025-10-29 10:04:24,527:INFO:Importing libraries
2025-10-29 10:04:24,527:INFO:Copying training dataset
2025-10-29 10:04:24,531:INFO:Defining folds
2025-10-29 10:04:24,531:INFO:Declaring metric variables
2025-10-29 10:04:24,532:INFO:Importing untrained model
2025-10-29 10:04:24,532:INFO:Declaring custom model
2025-10-29 10:04:24,532:INFO:Linear Discriminant Analysis Imported successfully
2025-10-29 10:04:24,533:INFO:Cross validation set to False
2025-10-29 10:04:24,533:INFO:Fitting Model
2025-10-29 10:04:24,715:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2025-10-29 10:04:24,715:INFO:create_model() successfully completed......................................
2025-10-29 10:04:24,890:INFO:Initializing create_model()
2025-10-29 10:04:24,890:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A42ABF7850>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=441239, splitter='best'), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:04:24,890:INFO:Checking exceptions
2025-10-29 10:04:24,892:INFO:Importing libraries
2025-10-29 10:04:24,892:INFO:Copying training dataset
2025-10-29 10:04:24,895:INFO:Defining folds
2025-10-29 10:04:24,895:INFO:Declaring metric variables
2025-10-29 10:04:24,896:INFO:Importing untrained model
2025-10-29 10:04:24,896:INFO:Declaring custom model
2025-10-29 10:04:24,896:INFO:Decision Tree Classifier Imported successfully
2025-10-29 10:04:24,897:INFO:Cross validation set to False
2025-10-29 10:04:24,897:INFO:Fitting Model
2025-10-29 10:04:25,073:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=441239, splitter='best')
2025-10-29 10:04:25,073:INFO:create_model() successfully completed......................................
2025-10-29 10:04:25,248:INFO:Initializing create_model()
2025-10-29 10:04:25,248:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A42ABF7850>, estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=441239), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:04:25,248:INFO:Checking exceptions
2025-10-29 10:04:25,250:INFO:Importing libraries
2025-10-29 10:04:25,250:INFO:Copying training dataset
2025-10-29 10:04:25,254:INFO:Defining folds
2025-10-29 10:04:25,254:INFO:Declaring metric variables
2025-10-29 10:04:25,254:INFO:Importing untrained model
2025-10-29 10:04:25,254:INFO:Declaring custom model
2025-10-29 10:04:25,254:INFO:str Imported successfully
2025-10-29 10:04:25,255:INFO:Cross validation set to False
2025-10-29 10:04:25,255:INFO:Fitting Model
2025-10-29 10:04:25,428:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=441239)
2025-10-29 10:04:25,428:INFO:create_model() successfully completed......................................
2025-10-29 10:04:25,600:INFO:Initializing create_model()
2025-10-29 10:04:25,600:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A42ABF7850>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=441239, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:04:25,600:INFO:Checking exceptions
2025-10-29 10:04:25,602:INFO:Importing libraries
2025-10-29 10:04:25,602:INFO:Copying training dataset
2025-10-29 10:04:25,606:INFO:Defining folds
2025-10-29 10:04:25,606:INFO:Declaring metric variables
2025-10-29 10:04:25,606:INFO:Importing untrained model
2025-10-29 10:04:25,606:INFO:Declaring custom model
2025-10-29 10:04:25,607:INFO:Gradient Boosting Classifier Imported successfully
2025-10-29 10:04:25,607:INFO:Cross validation set to False
2025-10-29 10:04:25,607:INFO:Fitting Model
2025-10-29 10:04:26,094:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=441239, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-10-29 10:04:26,094:INFO:create_model() successfully completed......................................
2025-10-29 10:04:26,274:INFO:Initializing create_model()
2025-10-29 10:04:26,274:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A42ABF7850>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=441239, reg_alpha=0.0, reg_lambda=0.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:04:26,274:INFO:Checking exceptions
2025-10-29 10:04:26,276:INFO:Importing libraries
2025-10-29 10:04:26,276:INFO:Copying training dataset
2025-10-29 10:04:26,281:INFO:Defining folds
2025-10-29 10:04:26,281:INFO:Declaring metric variables
2025-10-29 10:04:26,281:INFO:Importing untrained model
2025-10-29 10:04:26,281:INFO:Declaring custom model
2025-10-29 10:04:26,283:INFO:Light Gradient Boosting Machine Imported successfully
2025-10-29 10:04:26,283:INFO:Cross validation set to False
2025-10-29 10:04:26,283:INFO:Fitting Model
2025-10-29 10:04:26,315:INFO:[LightGBM] [Info] Number of positive: 2, number of negative: 3492
2025-10-29 10:04:26,315:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000097 seconds.
2025-10-29 10:04:26,315:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-10-29 10:04:26,315:INFO:[LightGBM] [Info] Total Bins 1275
2025-10-29 10:04:26,315:INFO:[LightGBM] [Info] Number of data points in the train set: 3494, number of used features: 5
2025-10-29 10:04:26,315:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000572 -> initscore=-7.465083
2025-10-29 10:04:26,316:INFO:[LightGBM] [Info] Start training from score -7.465083
2025-10-29 10:04:26,316:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 10:04:26,316:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 10:04:26,316:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 10:04:26,316:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 10:04:26,316:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 10:04:26,316:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 10:04:26,317:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 10:04:26,317:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 10:04:26,317:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 10:04:26,317:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 10:04:26,317:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 10:04:26,318:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 10:04:26,318:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 10:04:26,318:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 10:04:26,318:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 10:04:26,318:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 10:04:26,319:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 10:04:26,319:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 10:04:26,320:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 10:04:26,320:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 10:04:26,321:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 10:04:26,321:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 10:04:26,322:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 10:04:26,322:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 10:04:26,322:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 10:04:26,323:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 10:04:26,323:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 10:04:26,324:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 10:04:26,325:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 10:04:26,325:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 10:04:26,326:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 10:04:26,327:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 10:04:26,346:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 10:04:26,350:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 10:04:26,351:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 10:04:26,351:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 10:04:26,352:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 10:04:26,505:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=441239, reg_alpha=0.0, reg_lambda=0.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2025-10-29 10:04:26,505:INFO:create_model() successfully completed......................................
2025-10-29 10:04:26,681:INFO:Initializing create_model()
2025-10-29 10:04:26,681:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A42ABF7850>, estimator=<catboost.core.CatBoostClassifier object at 0x000001A425EA1400>, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:04:26,681:INFO:Checking exceptions
2025-10-29 10:04:26,683:INFO:Importing libraries
2025-10-29 10:04:26,683:INFO:Copying training dataset
2025-10-29 10:04:26,687:INFO:Defining folds
2025-10-29 10:04:26,687:INFO:Declaring metric variables
2025-10-29 10:04:26,687:INFO:Importing untrained model
2025-10-29 10:04:26,687:INFO:Declaring custom model
2025-10-29 10:04:26,688:INFO:CatBoost Classifier Imported successfully
2025-10-29 10:04:26,689:INFO:Cross validation set to False
2025-10-29 10:04:26,689:INFO:Fitting Model
2025-10-29 10:04:28,945:INFO:<catboost.core.CatBoostClassifier object at 0x000001A42A80EE80>
2025-10-29 10:04:28,945:INFO:create_model() successfully completed......................................
2025-10-29 10:04:29,113:INFO:Initializing create_model()
2025-10-29 10:04:29,113:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A42ABF7850>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=441239, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:04:29,113:INFO:Checking exceptions
2025-10-29 10:04:29,115:INFO:Importing libraries
2025-10-29 10:04:29,115:INFO:Copying training dataset
2025-10-29 10:04:29,118:INFO:Defining folds
2025-10-29 10:04:29,119:INFO:Declaring metric variables
2025-10-29 10:04:29,119:INFO:Importing untrained model
2025-10-29 10:04:29,119:INFO:Declaring custom model
2025-10-29 10:04:29,120:INFO:Logistic Regression Imported successfully
2025-10-29 10:04:29,120:INFO:Cross validation set to False
2025-10-29 10:04:29,120:INFO:Fitting Model
2025-10-29 10:04:29,300:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=441239, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-10-29 10:04:29,300:INFO:create_model() successfully completed......................................
2025-10-29 10:04:29,468:INFO:Initializing create_model()
2025-10-29 10:04:29,468:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A42ABF7850>, estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform'), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:04:29,468:INFO:Checking exceptions
2025-10-29 10:04:29,469:INFO:Importing libraries
2025-10-29 10:04:29,469:INFO:Copying training dataset
2025-10-29 10:04:29,473:INFO:Defining folds
2025-10-29 10:04:29,473:INFO:Declaring metric variables
2025-10-29 10:04:29,473:INFO:Importing untrained model
2025-10-29 10:04:29,474:INFO:Declaring custom model
2025-10-29 10:04:29,474:INFO:K Neighbors Classifier Imported successfully
2025-10-29 10:04:29,475:INFO:Cross validation set to False
2025-10-29 10:04:29,475:INFO:Fitting Model
2025-10-29 10:04:29,645:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-10-29 10:04:29,645:INFO:create_model() successfully completed......................................
2025-10-29 10:04:29,815:INFO:Initializing create_model()
2025-10-29 10:04:29,815:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A42ABF7850>, estimator=GaussianNB(priors=None, var_smoothing=1e-09), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:04:29,815:INFO:Checking exceptions
2025-10-29 10:04:29,816:INFO:Importing libraries
2025-10-29 10:04:29,816:INFO:Copying training dataset
2025-10-29 10:04:29,820:INFO:Defining folds
2025-10-29 10:04:29,820:INFO:Declaring metric variables
2025-10-29 10:04:29,820:INFO:Importing untrained model
2025-10-29 10:04:29,820:INFO:Declaring custom model
2025-10-29 10:04:29,821:INFO:Naive Bayes Imported successfully
2025-10-29 10:04:29,821:INFO:Cross validation set to False
2025-10-29 10:04:29,821:INFO:Fitting Model
2025-10-29 10:04:29,988:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-10-29 10:04:29,988:INFO:create_model() successfully completed......................................
2025-10-29 10:04:30,173:INFO:_master_model_container: 17
2025-10-29 10:04:30,174:INFO:_display_container: 2
2025-10-29 10:04:30,176:INFO:[SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=441239, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False), LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001), DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=441239, splitter='best'), AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=441239), GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=441239, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=441239, reg_alpha=0.0, reg_lambda=0.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), <catboost.core.CatBoostClassifier object at 0x000001A42A80EE80>, LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=441239, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform'), GaussianNB(priors=None, var_smoothing=1e-09)]
2025-10-29 10:04:30,176:INFO:compare_models() successfully completed......................................
2025-10-29 10:04:30,177:INFO:Initializing tune_model()
2025-10-29 10:04:30,177:INFO:tune_model(estimator=SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=441239, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A42ABF7850>)
2025-10-29 10:04:30,177:INFO:Checking exceptions
2025-10-29 10:04:30,192:INFO:Copying training dataset
2025-10-29 10:04:30,196:INFO:Checking base model
2025-10-29 10:04:30,196:INFO:Base model : SVM - Linear Kernel
2025-10-29 10:04:30,200:INFO:Declaring metric variables
2025-10-29 10:04:30,204:INFO:Defining Hyperparameters
2025-10-29 10:04:30,370:INFO:Tuning with n_jobs=-1
2025-10-29 10:04:30,370:INFO:Initializing RandomizedSearchCV
2025-10-29 10:04:30,375:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 10:04:46,138:INFO:best_params: {'actual_estimator__penalty': 'elasticnet', 'actual_estimator__learning_rate': 'constant', 'actual_estimator__l1_ratio': 0.8000000001, 'actual_estimator__fit_intercept': True, 'actual_estimator__eta0': 0.001, 'actual_estimator__alpha': 0.05}
2025-10-29 10:04:46,139:INFO:Hyperparameter search completed
2025-10-29 10:04:46,139:INFO:SubProcess create_model() called ==================================
2025-10-29 10:04:46,140:INFO:Initializing create_model()
2025-10-29 10:04:46,140:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A42ABF7850>, estimator=SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=441239, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A43CACFEE0>, model_only=True, return_train_score=False, kwargs={'penalty': 'elasticnet', 'learning_rate': 'constant', 'l1_ratio': 0.8000000001, 'fit_intercept': True, 'eta0': 0.001, 'alpha': 0.05})
2025-10-29 10:04:46,140:INFO:Checking exceptions
2025-10-29 10:04:46,141:INFO:Importing libraries
2025-10-29 10:04:46,141:INFO:Copying training dataset
2025-10-29 10:04:46,146:INFO:Defining folds
2025-10-29 10:04:46,146:INFO:Declaring metric variables
2025-10-29 10:04:46,150:INFO:Importing untrained model
2025-10-29 10:04:46,150:INFO:Declaring custom model
2025-10-29 10:04:46,154:INFO:SVM - Linear Kernel Imported successfully
2025-10-29 10:04:46,163:INFO:Starting cross validation
2025-10-29 10:04:46,164:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 10:04:46,166:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 10:04:46,251:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 10:04:46,252:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 10:04:46,254:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:46,254:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:46,255:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 10:04:46,256:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:46,257:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:46,258:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 10:04:46,258:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:46,259:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:46,260:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:46,260:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:46,261:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:46,262:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:46,267:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 10:04:46,269:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:46,271:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 10:04:46,271:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:46,272:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:46,274:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:46,275:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:46,275:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:46,277:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:46,278:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:46,545:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 10:04:46,548:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:46,551:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:46,554:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:46,555:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:46,591:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 10:04:46,593:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:46,595:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:46,595:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 10:04:46,597:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:46,597:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:46,598:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:46,600:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:46,602:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:46,603:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:46,608:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 10:04:46,610:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:46,612:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:46,615:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:46,616:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:47,667:INFO:Calculating mean and std
2025-10-29 10:04:47,668:INFO:Creating metrics dataframe
2025-10-29 10:04:47,674:INFO:Finalizing model
2025-10-29 10:04:47,913:INFO:Uploading results into container
2025-10-29 10:04:47,914:INFO:Uploading model into container now
2025-10-29 10:04:47,915:INFO:_master_model_container: 18
2025-10-29 10:04:47,915:INFO:_display_container: 3
2025-10-29 10:04:47,915:INFO:SGDClassifier(alpha=0.05, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.8000000001, learning_rate='constant', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1,
              penalty='elasticnet', power_t=0.5, random_state=441239,
              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,
              warm_start=False)
2025-10-29 10:04:47,915:INFO:create_model() successfully completed......................................
2025-10-29 10:04:48,084:INFO:SubProcess create_model() end ==================================
2025-10-29 10:04:48,084:INFO:choose_better activated
2025-10-29 10:04:48,088:INFO:SubProcess create_model() called ==================================
2025-10-29 10:04:48,089:INFO:Initializing create_model()
2025-10-29 10:04:48,089:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A42ABF7850>, estimator=SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=441239, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:04:48,089:INFO:Checking exceptions
2025-10-29 10:04:48,091:INFO:Importing libraries
2025-10-29 10:04:48,091:INFO:Copying training dataset
2025-10-29 10:04:48,095:INFO:Defining folds
2025-10-29 10:04:48,095:INFO:Declaring metric variables
2025-10-29 10:04:48,095:INFO:Importing untrained model
2025-10-29 10:04:48,095:INFO:Declaring custom model
2025-10-29 10:04:48,096:INFO:SVM - Linear Kernel Imported successfully
2025-10-29 10:04:48,096:INFO:Starting cross validation
2025-10-29 10:04:48,097:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 10:04:48,099:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 10:04:48,196:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 10:04:48,198:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 10:04:48,199:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:48,200:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 10:04:48,201:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:48,201:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:48,203:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:48,203:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:48,204:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 10:04:48,204:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 10:04:48,204:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:48,206:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:48,207:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:48,209:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:48,212:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:48,212:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:48,213:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:48,221:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 10:04:48,223:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:48,226:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:48,228:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:48,229:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:48,466:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 10:04:48,469:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:48,469:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 10:04:48,471:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:48,471:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:48,473:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:48,473:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:48,475:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:48,476:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:48,477:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:48,482:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 10:04:48,484:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:48,486:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:48,489:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:48,490:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:48,498:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 10:04:48,500:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:48,502:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:04:48,505:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:04:48,506:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:04:49,611:INFO:Calculating mean and std
2025-10-29 10:04:49,612:INFO:Creating metrics dataframe
2025-10-29 10:04:49,614:INFO:Finalizing model
2025-10-29 10:04:49,859:INFO:Uploading results into container
2025-10-29 10:04:49,860:INFO:Uploading model into container now
2025-10-29 10:04:49,860:INFO:_master_model_container: 19
2025-10-29 10:04:49,860:INFO:_display_container: 4
2025-10-29 10:04:49,861:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=441239, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2025-10-29 10:04:49,861:INFO:create_model() successfully completed......................................
2025-10-29 10:04:50,026:INFO:SubProcess create_model() end ==================================
2025-10-29 10:04:50,027:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=441239, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False) result for Accuracy is 1.0
2025-10-29 10:04:50,028:INFO:SGDClassifier(alpha=0.05, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.8000000001, learning_rate='constant', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1,
              penalty='elasticnet', power_t=0.5, random_state=441239,
              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,
              warm_start=False) result for Accuracy is 0.9994
2025-10-29 10:04:50,028:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=441239, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False) is best model
2025-10-29 10:04:50,028:INFO:choose_better completed
2025-10-29 10:04:50,029:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2025-10-29 10:04:50,041:INFO:_master_model_container: 19
2025-10-29 10:04:50,041:INFO:_display_container: 3
2025-10-29 10:04:50,042:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=441239, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2025-10-29 10:04:50,042:INFO:tune_model() successfully completed......................................
2025-10-29 10:04:50,370:INFO:Initializing evaluate_model()
2025-10-29 10:04:50,370:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A42ABF7850>, estimator=SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=441239, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None, use_train_data=False)
2025-10-29 10:04:50,381:INFO:Initializing plot_model()
2025-10-29 10:04:50,381:INFO:plot_model(plot=pipeline, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=441239, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A42ABF7850>, system=True)
2025-10-29 10:04:50,381:INFO:Checking exceptions
2025-10-29 10:04:50,383:INFO:Preloading libraries
2025-10-29 10:04:50,383:INFO:Copying training dataset
2025-10-29 10:04:50,383:INFO:Plot type: pipeline
2025-10-29 10:04:50,536:INFO:Visual Rendered Successfully
2025-10-29 10:04:50,708:INFO:plot_model() successfully completed......................................
2025-10-29 10:09:52,614:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:09:52,618:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:09:52,620:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:34,630:INFO:PyCaret ClassificationExperiment
2025-10-29 10:13:34,630:INFO:Logging name: clf-default-name
2025-10-29 10:13:34,630:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-10-29 10:13:34,630:INFO:version 3.0.0
2025-10-29 10:13:34,630:INFO:Initializing setup()
2025-10-29 10:13:34,630:INFO:self.USI: 5abc
2025-10-29 10:13:34,630:INFO:self._variable_keys: {'target_param', 'seed', 'y_train', 'fold_shuffle_param', 'X_train', 'is_multiclass', 'y_test', '_ml_usecase', 'X', 'log_plots_param', 'USI', 'exp_id', 'pipeline', 'idx', 'y', '_available_plots', 'html_param', 'fold_groups_param', 'exp_name_log', 'X_test', 'fold_generator', 'logging_param', 'gpu_n_jobs_param', 'n_jobs_param', 'memory', 'data', 'gpu_param', 'fix_imbalance'}
2025-10-29 10:13:34,631:INFO:Checking environment
2025-10-29 10:13:34,631:INFO:python_version: 3.8.15
2025-10-29 10:13:34,631:INFO:python_build: ('default', 'Nov 24 2022 14:38:14')
2025-10-29 10:13:34,631:INFO:machine: AMD64
2025-10-29 10:13:34,631:INFO:platform: Windows-10-10.0.19045-SP0
2025-10-29 10:13:34,633:INFO:Memory: svmem(total=34299187200, available=23346647040, percent=31.9, used=10952540160, free=23346647040)
2025-10-29 10:13:34,633:INFO:Physical Core: 6
2025-10-29 10:13:34,633:INFO:Logical Core: 6
2025-10-29 10:13:34,633:INFO:Checking libraries
2025-10-29 10:13:34,633:INFO:System:
2025-10-29 10:13:34,633:INFO:    python: 3.8.15 (default, Nov 24 2022, 14:38:14) [MSC v.1916 64 bit (AMD64)]
2025-10-29 10:13:34,633:INFO:executable: c:\Users\hkh97\anaconda3\envs\han\python.exe
2025-10-29 10:13:34,633:INFO:   machine: Windows-10-10.0.19045-SP0
2025-10-29 10:13:34,633:INFO:PyCaret required dependencies:
2025-10-29 10:13:34,633:INFO:                 pip: 25.0.1
2025-10-29 10:13:34,633:INFO:          setuptools: 75.1.0
2025-10-29 10:13:34,633:INFO:             pycaret: 3.0.0
2025-10-29 10:13:34,633:INFO:             IPython: 8.12.3
2025-10-29 10:13:34,633:INFO:          ipywidgets: 8.1.7
2025-10-29 10:13:34,634:INFO:                tqdm: 4.67.1
2025-10-29 10:13:34,634:INFO:               numpy: 1.24.1
2025-10-29 10:13:34,634:INFO:              pandas: 1.5.3
2025-10-29 10:13:34,634:INFO:              jinja2: 3.1.4
2025-10-29 10:13:34,634:INFO:               scipy: 1.10.1
2025-10-29 10:13:34,634:INFO:              joblib: 1.2.0
2025-10-29 10:13:34,634:INFO:             sklearn: 1.2.2
2025-10-29 10:13:34,634:INFO:                pyod: 2.0.5
2025-10-29 10:13:34,634:INFO:            imblearn: 0.12.4
2025-10-29 10:13:34,634:INFO:   category_encoders: 2.6.4
2025-10-29 10:13:34,634:INFO:            lightgbm: 4.6.0
2025-10-29 10:13:34,634:INFO:               numba: 0.58.1
2025-10-29 10:13:34,634:INFO:            requests: 2.32.4
2025-10-29 10:13:34,634:INFO:          matplotlib: 3.6.0
2025-10-29 10:13:34,634:INFO:          scikitplot: 0.3.7
2025-10-29 10:13:34,634:INFO:         yellowbrick: 1.5
2025-10-29 10:13:34,634:INFO:              plotly: 6.3.0
2025-10-29 10:13:34,634:INFO:             kaleido: 1.1.0
2025-10-29 10:13:34,634:INFO:         statsmodels: 0.14.1
2025-10-29 10:13:34,634:INFO:              sktime: 0.21.1
2025-10-29 10:13:34,634:INFO:               tbats: 1.1.3
2025-10-29 10:13:34,635:INFO:            pmdarima: 2.0.4
2025-10-29 10:13:34,635:INFO:              psutil: 7.0.0
2025-10-29 10:13:34,635:INFO:PyCaret optional dependencies:
2025-10-29 10:13:34,635:INFO:                shap: Not installed
2025-10-29 10:13:34,635:INFO:           interpret: Not installed
2025-10-29 10:13:34,635:INFO:                umap: Not installed
2025-10-29 10:13:34,635:INFO:    pandas_profiling: Not installed
2025-10-29 10:13:34,635:INFO:  explainerdashboard: Not installed
2025-10-29 10:13:34,635:INFO:             autoviz: Not installed
2025-10-29 10:13:34,635:INFO:           fairlearn: Not installed
2025-10-29 10:13:34,635:INFO:             xgboost: 2.1.4
2025-10-29 10:13:34,635:INFO:            catboost: 1.2.8
2025-10-29 10:13:34,635:INFO:              kmodes: Not installed
2025-10-29 10:13:34,635:INFO:             mlxtend: Not installed
2025-10-29 10:13:34,635:INFO:       statsforecast: Not installed
2025-10-29 10:13:34,635:INFO:        tune_sklearn: Not installed
2025-10-29 10:13:34,635:INFO:                 ray: Not installed
2025-10-29 10:13:34,635:INFO:            hyperopt: Not installed
2025-10-29 10:13:34,635:INFO:              optuna: Not installed
2025-10-29 10:13:34,635:INFO:               skopt: Not installed
2025-10-29 10:13:34,635:INFO:              mlflow: Not installed
2025-10-29 10:13:34,636:INFO:              gradio: Not installed
2025-10-29 10:13:34,636:INFO:             fastapi: Not installed
2025-10-29 10:13:34,636:INFO:             uvicorn: Not installed
2025-10-29 10:13:34,636:INFO:              m2cgen: Not installed
2025-10-29 10:13:34,636:INFO:           evidently: Not installed
2025-10-29 10:13:34,636:INFO:               fugue: Not installed
2025-10-29 10:13:34,636:INFO:           streamlit: Not installed
2025-10-29 10:13:34,636:INFO:             prophet: Not installed
2025-10-29 10:13:34,636:INFO:None
2025-10-29 10:13:34,636:INFO:Set up data.
2025-10-29 10:13:34,640:INFO:Set up train/test split.
2025-10-29 10:13:34,644:INFO:Set up index.
2025-10-29 10:13:34,644:INFO:Set up folding strategy.
2025-10-29 10:13:34,644:INFO:Assigning column types.
2025-10-29 10:13:34,647:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-10-29 10:13:34,692:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-10-29 10:13:34,693:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-10-29 10:13:34,721:INFO:Soft dependency imported: xgboost: 2.1.4
2025-10-29 10:13:34,724:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-29 10:13:34,769:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-10-29 10:13:34,770:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-10-29 10:13:34,798:INFO:Soft dependency imported: xgboost: 2.1.4
2025-10-29 10:13:34,801:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-29 10:13:34,802:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-10-29 10:13:34,846:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-10-29 10:13:34,874:INFO:Soft dependency imported: xgboost: 2.1.4
2025-10-29 10:13:34,876:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-29 10:13:34,924:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-10-29 10:13:34,952:INFO:Soft dependency imported: xgboost: 2.1.4
2025-10-29 10:13:34,955:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-29 10:13:34,955:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-10-29 10:13:35,029:INFO:Soft dependency imported: xgboost: 2.1.4
2025-10-29 10:13:35,032:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-29 10:13:35,109:INFO:Soft dependency imported: xgboost: 2.1.4
2025-10-29 10:13:35,112:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-29 10:13:35,113:INFO:Preparing preprocessing pipeline...
2025-10-29 10:13:35,114:INFO:Set up simple imputation.
2025-10-29 10:13:35,114:INFO:Set up feature normalization.
2025-10-29 10:13:35,136:INFO:Finished creating preprocessing pipeline.
2025-10-29 10:13:35,141:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\hkh97\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Temperature', 'Pressure',
                                             'Vibration', 'GasFlow',
                                             'SteamOutput'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean',
                                                              verbose=...
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent',
                                                              verbose='deprecated'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2025-10-29 10:13:35,141:INFO:Creating final display dataframe.
2025-10-29 10:13:35,229:INFO:Setup _display_container:                     Description             Value
0                    Session id            441239
1                        Target           Failure
2                   Target type            Binary
3           Original data shape         (4368, 6)
4        Transformed data shape         (4368, 6)
5   Transformed train set shape         (3494, 6)
6    Transformed test set shape          (874, 6)
7              Numeric features                 5
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                    Normalize              True
13             Normalize method            zscore
14               Fold Generator   StratifiedKFold
15                  Fold Number                10
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              5abc
2025-10-29 10:13:35,306:INFO:Soft dependency imported: xgboost: 2.1.4
2025-10-29 10:13:35,308:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-29 10:13:35,383:INFO:Soft dependency imported: xgboost: 2.1.4
2025-10-29 10:13:35,386:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-29 10:13:35,387:INFO:setup() successfully completed in 0.9s...............
2025-10-29 10:13:35,387:INFO:Initializing compare_models()
2025-10-29 10:13:35,387:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A44E6CA790>, include=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=10, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001A44E6CA790>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 10, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2025-10-29 10:13:35,387:INFO:Checking exceptions
2025-10-29 10:13:35,390:INFO:Preparing display monitor
2025-10-29 10:13:35,414:INFO:Initializing Logistic Regression
2025-10-29 10:13:35,414:INFO:Total runtime is 0.0 minutes
2025-10-29 10:13:35,418:INFO:SubProcess create_model() called ==================================
2025-10-29 10:13:35,418:INFO:Initializing create_model()
2025-10-29 10:13:35,418:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A44E6CA790>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A45351CE50>, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:13:35,418:INFO:Checking exceptions
2025-10-29 10:13:35,418:INFO:Importing libraries
2025-10-29 10:13:35,418:INFO:Copying training dataset
2025-10-29 10:13:35,425:INFO:Defining folds
2025-10-29 10:13:35,425:INFO:Declaring metric variables
2025-10-29 10:13:35,429:INFO:Importing untrained model
2025-10-29 10:13:35,434:INFO:Logistic Regression Imported successfully
2025-10-29 10:13:35,443:INFO:Starting cross validation
2025-10-29 10:13:35,444:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 10:13:35,446:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 10:13:38,724:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:38,744:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:38,751:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:38,758:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:38,759:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:38,760:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:38,761:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:38,762:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:38,763:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:38,764:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:38,765:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:38,881:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:38,889:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:38,891:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:38,894:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:38,895:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:38,958:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:38,963:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:38,965:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:38,968:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:38,970:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:39,111:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:39,113:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:39,116:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:39,118:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:39,119:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:39,120:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:39,123:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:39,125:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:39,126:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:39,127:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:39,128:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:39,129:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:39,131:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:39,133:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:39,135:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:39,143:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:39,145:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:39,147:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:39,150:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:39,151:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:40,112:INFO:Calculating mean and std
2025-10-29 10:13:40,114:INFO:Creating metrics dataframe
2025-10-29 10:13:40,322:INFO:Uploading results into container
2025-10-29 10:13:40,323:INFO:Uploading model into container now
2025-10-29 10:13:40,324:INFO:_master_model_container: 1
2025-10-29 10:13:40,324:INFO:_display_container: 2
2025-10-29 10:13:40,324:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=441239, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-10-29 10:13:40,324:INFO:create_model() successfully completed......................................
2025-10-29 10:13:40,510:INFO:SubProcess create_model() end ==================================
2025-10-29 10:13:40,511:INFO:Creating metrics dataframe
2025-10-29 10:13:40,521:INFO:Initializing K Neighbors Classifier
2025-10-29 10:13:40,521:INFO:Total runtime is 0.08512287537256877 minutes
2025-10-29 10:13:40,525:INFO:SubProcess create_model() called ==================================
2025-10-29 10:13:40,525:INFO:Initializing create_model()
2025-10-29 10:13:40,525:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A44E6CA790>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A45351CE50>, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:13:40,525:INFO:Checking exceptions
2025-10-29 10:13:40,525:INFO:Importing libraries
2025-10-29 10:13:40,525:INFO:Copying training dataset
2025-10-29 10:13:40,530:INFO:Defining folds
2025-10-29 10:13:40,530:INFO:Declaring metric variables
2025-10-29 10:13:40,535:INFO:Importing untrained model
2025-10-29 10:13:40,540:INFO:K Neighbors Classifier Imported successfully
2025-10-29 10:13:40,549:INFO:Starting cross validation
2025-10-29 10:13:40,550:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 10:13:40,552:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 10:13:40,707:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:40,708:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:40,710:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:40,713:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:40,713:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:40,715:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:40,716:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:40,717:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:40,718:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:40,721:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:40,727:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:40,733:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:40,736:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:40,738:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:40,740:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:40,743:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:40,744:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:40,769:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:40,771:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:40,773:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:40,780:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:40,782:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:41,011:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:41,013:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:41,015:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:41,018:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:41,019:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:41,042:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:41,043:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:41,046:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:41,049:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:41,050:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:41,055:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:41,057:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:41,060:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:41,062:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:41,063:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:41,069:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:41,071:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:41,074:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:41,076:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:41,077:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:42,138:INFO:Calculating mean and std
2025-10-29 10:13:42,139:INFO:Creating metrics dataframe
2025-10-29 10:13:42,362:INFO:Uploading results into container
2025-10-29 10:13:42,362:INFO:Uploading model into container now
2025-10-29 10:13:42,363:INFO:_master_model_container: 2
2025-10-29 10:13:42,363:INFO:_display_container: 2
2025-10-29 10:13:42,363:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-10-29 10:13:42,363:INFO:create_model() successfully completed......................................
2025-10-29 10:13:42,538:INFO:SubProcess create_model() end ==================================
2025-10-29 10:13:42,538:INFO:Creating metrics dataframe
2025-10-29 10:13:42,549:INFO:Initializing Naive Bayes
2025-10-29 10:13:42,549:INFO:Total runtime is 0.11891541481018067 minutes
2025-10-29 10:13:42,553:INFO:SubProcess create_model() called ==================================
2025-10-29 10:13:42,554:INFO:Initializing create_model()
2025-10-29 10:13:42,554:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A44E6CA790>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A45351CE50>, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:13:42,554:INFO:Checking exceptions
2025-10-29 10:13:42,554:INFO:Importing libraries
2025-10-29 10:13:42,554:INFO:Copying training dataset
2025-10-29 10:13:42,558:INFO:Defining folds
2025-10-29 10:13:42,558:INFO:Declaring metric variables
2025-10-29 10:13:42,562:INFO:Importing untrained model
2025-10-29 10:13:42,567:INFO:Naive Bayes Imported successfully
2025-10-29 10:13:42,575:INFO:Starting cross validation
2025-10-29 10:13:42,576:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 10:13:42,578:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 10:13:42,676:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:42,678:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:42,681:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:42,683:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:42,684:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:42,684:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:42,687:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:42,689:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:42,692:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:42,693:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:42,693:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:42,695:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:42,695:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:42,696:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:42,699:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:42,701:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:42,701:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:42,703:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:42,705:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:42,707:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:42,708:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:42,709:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:42,974:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:42,976:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:42,978:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:42,980:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:42,981:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:42,981:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:42,984:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:42,984:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:42,986:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:42,987:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:42,988:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:42,989:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:42,990:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:42,990:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:42,991:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:42,994:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:42,995:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:42,998:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:43,000:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:43,001:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:44,109:INFO:Calculating mean and std
2025-10-29 10:13:44,110:INFO:Creating metrics dataframe
2025-10-29 10:13:44,332:INFO:Uploading results into container
2025-10-29 10:13:44,332:INFO:Uploading model into container now
2025-10-29 10:13:44,332:INFO:_master_model_container: 3
2025-10-29 10:13:44,333:INFO:_display_container: 2
2025-10-29 10:13:44,333:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-10-29 10:13:44,333:INFO:create_model() successfully completed......................................
2025-10-29 10:13:44,509:INFO:SubProcess create_model() end ==================================
2025-10-29 10:13:44,509:INFO:Creating metrics dataframe
2025-10-29 10:13:44,521:INFO:Initializing Decision Tree Classifier
2025-10-29 10:13:44,521:INFO:Total runtime is 0.15178827047348023 minutes
2025-10-29 10:13:44,526:INFO:SubProcess create_model() called ==================================
2025-10-29 10:13:44,526:INFO:Initializing create_model()
2025-10-29 10:13:44,526:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A44E6CA790>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A45351CE50>, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:13:44,526:INFO:Checking exceptions
2025-10-29 10:13:44,526:INFO:Importing libraries
2025-10-29 10:13:44,526:INFO:Copying training dataset
2025-10-29 10:13:44,532:INFO:Defining folds
2025-10-29 10:13:44,532:INFO:Declaring metric variables
2025-10-29 10:13:44,536:INFO:Importing untrained model
2025-10-29 10:13:44,541:INFO:Decision Tree Classifier Imported successfully
2025-10-29 10:13:44,549:INFO:Starting cross validation
2025-10-29 10:13:44,550:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 10:13:44,553:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 10:13:44,651:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:44,653:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:44,654:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:44,656:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:44,656:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:44,659:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:44,660:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:44,669:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:44,672:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:44,674:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:44,676:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:44,679:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:44,680:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:44,685:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:44,687:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:44,689:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:44,692:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:44,693:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:44,926:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:44,927:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:44,929:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:44,931:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:44,932:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:44,944:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:44,946:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:44,946:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:44,948:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

lt))

2025-10-29 10:13:44,950:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:44,950:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:44,951:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:44,952:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:44,953:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:44,959:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:44,961:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:44,963:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:44,965:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:44,966:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:45,996:INFO:Calculating mean and std
2025-10-29 10:13:45,998:INFO:Creating metrics dataframe
2025-10-29 10:13:46,214:INFO:Uploading results into container
2025-10-29 10:13:46,215:INFO:Uploading model into container now
2025-10-29 10:13:46,215:INFO:_master_model_container: 4
2025-10-29 10:13:46,215:INFO:_display_container: 2
2025-10-29 10:13:46,216:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=441239, splitter='best')
2025-10-29 10:13:46,216:INFO:create_model() successfully completed......................................
2025-10-29 10:13:46,386:INFO:SubProcess create_model() end ==================================
2025-10-29 10:13:46,386:INFO:Creating metrics dataframe
2025-10-29 10:13:46,398:INFO:Initializing SVM - Linear Kernel
2025-10-29 10:13:46,398:INFO:Total runtime is 0.18307222525278727 minutes
2025-10-29 10:13:46,401:INFO:SubProcess create_model() called ==================================
2025-10-29 10:13:46,402:INFO:Initializing create_model()
2025-10-29 10:13:46,402:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A44E6CA790>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A45351CE50>, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:13:46,402:INFO:Checking exceptions
2025-10-29 10:13:46,402:INFO:Importing libraries
2025-10-29 10:13:46,402:INFO:Copying training dataset
2025-10-29 10:13:46,407:INFO:Defining folds
2025-10-29 10:13:46,407:INFO:Declaring metric variables
2025-10-29 10:13:46,411:INFO:Importing untrained model
2025-10-29 10:13:46,417:INFO:SVM - Linear Kernel Imported successfully
2025-10-29 10:13:46,424:INFO:Starting cross validation
2025-10-29 10:13:46,425:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 10:13:46,427:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 10:13:46,503:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 10:13:46,506:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:46,508:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:46,508:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 10:13:46,510:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:46,510:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:46,511:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:46,514:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:46,515:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 10:13:46,516:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:46,517:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:46,523:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 10:13:46,523:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 10:13:46,526:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:46,528:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:46,530:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:46,531:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:46,565:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 10:13:46,568:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:46,570:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:46,572:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:46,572:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:46,770:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 10:13:46,772:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:46,774:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:46,776:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 10:13:46,777:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:46,778:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:46,779:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:46,780:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:46,782:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 10:13:46,783:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:46,784:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:46,784:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:46,786:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:46,788:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:46,789:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 10:13:46,789:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:46,791:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:46,793:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:46,796:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:46,797:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:47,842:INFO:Calculating mean and std
2025-10-29 10:13:47,843:INFO:Creating metrics dataframe
2025-10-29 10:13:48,064:INFO:Uploading results into container
2025-10-29 10:13:48,065:INFO:Uploading model into container now
2025-10-29 10:13:48,065:INFO:_master_model_container: 5
2025-10-29 10:13:48,065:INFO:_display_container: 2
2025-10-29 10:13:48,066:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=441239, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2025-10-29 10:13:48,066:INFO:create_model() successfully completed......................................
2025-10-29 10:13:48,233:INFO:SubProcess create_model() end ==================================
2025-10-29 10:13:48,233:INFO:Creating metrics dataframe
2025-10-29 10:13:48,246:INFO:Initializing Ridge Classifier
2025-10-29 10:13:48,246:INFO:Total runtime is 0.21387321949005128 minutes
2025-10-29 10:13:48,249:INFO:SubProcess create_model() called ==================================
2025-10-29 10:13:48,250:INFO:Initializing create_model()
2025-10-29 10:13:48,250:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A44E6CA790>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A45351CE50>, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:13:48,250:INFO:Checking exceptions
2025-10-29 10:13:48,250:INFO:Importing libraries
2025-10-29 10:13:48,250:INFO:Copying training dataset
2025-10-29 10:13:48,255:INFO:Defining folds
2025-10-29 10:13:48,255:INFO:Declaring metric variables
2025-10-29 10:13:48,259:INFO:Importing untrained model
2025-10-29 10:13:48,264:INFO:Ridge Classifier Imported successfully
2025-10-29 10:13:48,272:INFO:Starting cross validation
2025-10-29 10:13:48,273:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 10:13:48,275:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 10:13:48,348:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2025-10-29 10:13:48,350:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:48,352:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:48,354:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:48,355:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:48,358:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2025-10-29 10:13:48,362:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:48,362:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2025-10-29 10:13:48,366:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:48,369:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2025-10-29 10:13:48,370:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2025-10-29 10:13:48,371:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:48,373:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:48,374:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:48,375:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:48,377:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:48,377:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:48,378:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:48,378:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:48,380:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2025-10-29 10:13:48,383:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:48,385:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:48,387:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:48,388:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:48,608:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2025-10-29 10:13:48,609:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2025-10-29 10:13:48,610:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:48,611:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:48,612:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:48,612:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:48,614:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:48,615:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:48,615:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:48,616:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:48,632:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2025-10-29 10:13:48,633:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2025-10-29 10:13:48,634:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:48,635:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:48,636:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:48,638:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:48,638:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:48,639:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:48,639:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:48,640:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:49,722:INFO:Calculating mean and std
2025-10-29 10:13:49,723:INFO:Creating metrics dataframe
2025-10-29 10:13:49,932:INFO:Uploading results into container
2025-10-29 10:13:49,933:INFO:Uploading model into container now
2025-10-29 10:13:49,934:INFO:_master_model_container: 6
2025-10-29 10:13:49,934:INFO:_display_container: 2
2025-10-29 10:13:49,934:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=441239,
                solver='auto', tol=0.0001)
2025-10-29 10:13:49,934:INFO:create_model() successfully completed......................................
2025-10-29 10:13:50,102:INFO:SubProcess create_model() end ==================================
2025-10-29 10:13:50,103:INFO:Creating metrics dataframe
2025-10-29 10:13:50,115:INFO:Initializing Random Forest Classifier
2025-10-29 10:13:50,116:INFO:Total runtime is 0.2450399478276571 minutes
2025-10-29 10:13:50,119:INFO:SubProcess create_model() called ==================================
2025-10-29 10:13:50,120:INFO:Initializing create_model()
2025-10-29 10:13:50,120:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A44E6CA790>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A45351CE50>, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:13:50,120:INFO:Checking exceptions
2025-10-29 10:13:50,120:INFO:Importing libraries
2025-10-29 10:13:50,120:INFO:Copying training dataset
2025-10-29 10:13:50,126:INFO:Defining folds
2025-10-29 10:13:50,126:INFO:Declaring metric variables
2025-10-29 10:13:50,130:INFO:Importing untrained model
2025-10-29 10:13:50,134:INFO:Random Forest Classifier Imported successfully
2025-10-29 10:13:50,143:INFO:Starting cross validation
2025-10-29 10:13:50,144:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 10:13:50,146:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 10:13:50,464:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:50,466:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:50,468:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:50,469:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:50,471:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:50,471:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:50,471:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:50,473:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:50,473:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:50,475:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:50,477:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:50,511:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:50,513:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:50,515:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:50,515:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:50,517:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:50,518:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:50,524:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:50,526:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:50,528:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:50,530:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:50,531:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:50,996:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:50,998:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:51,000:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:51,002:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:51,003:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:51,003:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:51,004:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:51,006:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:51,008:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:51,009:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:51,012:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:51,015:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:51,017:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:51,019:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:51,020:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:51,051:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:51,054:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:51,057:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:51,059:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:51,062:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:51,946:INFO:Calculating mean and std
2025-10-29 10:13:51,947:INFO:Creating metrics dataframe
2025-10-29 10:13:52,160:INFO:Uploading results into container
2025-10-29 10:13:52,161:INFO:Uploading model into container now
2025-10-29 10:13:52,161:INFO:_master_model_container: 7
2025-10-29 10:13:52,161:INFO:_display_container: 2
2025-10-29 10:13:52,162:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=441239, verbose=0, warm_start=False)
2025-10-29 10:13:52,162:INFO:create_model() successfully completed......................................
2025-10-29 10:13:52,333:INFO:SubProcess create_model() end ==================================
2025-10-29 10:13:52,333:INFO:Creating metrics dataframe
2025-10-29 10:13:52,347:INFO:Initializing Quadratic Discriminant Analysis
2025-10-29 10:13:52,347:INFO:Total runtime is 0.28222461938858034 minutes
2025-10-29 10:13:52,352:INFO:SubProcess create_model() called ==================================
2025-10-29 10:13:52,352:INFO:Initializing create_model()
2025-10-29 10:13:52,352:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A44E6CA790>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A45351CE50>, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:13:52,352:INFO:Checking exceptions
2025-10-29 10:13:52,353:INFO:Importing libraries
2025-10-29 10:13:52,353:INFO:Copying training dataset
2025-10-29 10:13:52,358:INFO:Defining folds
2025-10-29 10:13:52,359:INFO:Declaring metric variables
2025-10-29 10:13:52,363:INFO:Importing untrained model
2025-10-29 10:13:52,368:INFO:Quadratic Discriminant Analysis Imported successfully
2025-10-29 10:13:52,377:INFO:Starting cross validation
2025-10-29 10:13:52,379:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 10:13:52,381:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 10:13:52,443:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-10-29 10:13:52,450:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-10-29 10:13:52,465:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-10-29 10:13:52,469:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-10-29 10:13:52,500:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:52,502:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:52,504:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:52,506:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:52,508:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:52,508:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:52,508:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:52,509:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:52,510:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:52,513:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:52,514:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:52,515:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:52,516:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:52,520:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:52,521:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:52,550:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:52,552:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:52,557:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:52,559:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:52,562:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:52,770:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-10-29 10:13:52,771:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-10-29 10:13:52,784:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-10-29 10:13:52,787:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-10-29 10:13:52,812:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:52,814:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:52,814:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:52,817:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:52,818:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:52,819:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:52,820:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:52,821:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:52,821:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:52,822:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:52,826:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:52,828:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:52,830:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:52,833:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:52,833:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:52,835:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:52,836:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:52,836:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:52,840:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:52,842:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:53,913:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_validation.py:378: FitFailedWarning: 
2 fits failed out of a total of 10.
The score on these train-test partitions for these parameters will be set to 0.
If these failures are not expected, you can try to debug them by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
2 fits failed with the following error:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\discriminant_analysis.py", line 917, in fit
    raise ValueError(
ValueError: y has only 1 sample in class 1, covariance is ill defined.

  warnings.warn(some_fits_failed_message, FitFailedWarning)

2025-10-29 10:13:53,914:INFO:Calculating mean and std
2025-10-29 10:13:53,915:INFO:Creating metrics dataframe
2025-10-29 10:13:54,132:INFO:Uploading results into container
2025-10-29 10:13:54,133:INFO:Uploading model into container now
2025-10-29 10:13:54,133:INFO:_master_model_container: 8
2025-10-29 10:13:54,133:INFO:_display_container: 2
2025-10-29 10:13:54,134:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2025-10-29 10:13:54,134:INFO:create_model() successfully completed......................................
2025-10-29 10:13:54,315:INFO:SubProcess create_model() end ==================================
2025-10-29 10:13:54,315:INFO:Creating metrics dataframe
2025-10-29 10:13:54,329:INFO:Initializing Ada Boost Classifier
2025-10-29 10:13:54,329:INFO:Total runtime is 0.31525296767552696 minutes
2025-10-29 10:13:54,333:INFO:SubProcess create_model() called ==================================
2025-10-29 10:13:54,334:INFO:Initializing create_model()
2025-10-29 10:13:54,334:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A44E6CA790>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A45351CE50>, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:13:54,334:INFO:Checking exceptions
2025-10-29 10:13:54,334:INFO:Importing libraries
2025-10-29 10:13:54,335:INFO:Copying training dataset
2025-10-29 10:13:54,340:INFO:Defining folds
2025-10-29 10:13:54,340:INFO:Declaring metric variables
2025-10-29 10:13:54,344:INFO:Importing untrained model
2025-10-29 10:13:54,350:INFO:Ada Boost Classifier Imported successfully
2025-10-29 10:13:54,359:INFO:Starting cross validation
2025-10-29 10:13:54,360:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 10:13:54,363:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 10:13:54,479:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:54,481:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:54,482:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:54,483:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:54,486:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:54,488:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:54,489:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:54,490:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:54,497:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:54,501:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:54,517:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:54,518:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:54,520:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:54,520:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:54,523:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:54,524:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:54,525:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:54,527:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:54,789:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:54,792:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:54,793:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:54,794:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:54,794:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:54,795:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:54,796:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

"F-score is", len(true_sum))

2025-10-29 10:13:54,797:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:54,799:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:54,800:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:54,800:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:54,801:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:54,801:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:54,802:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:54,802:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:54,805:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:54,808:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:54,809:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:55,879:INFO:Calculating mean and std
2025-10-29 10:13:55,880:INFO:Creating metrics dataframe
2025-10-29 10:13:56,094:INFO:Uploading results into container
2025-10-29 10:13:56,095:INFO:Uploading model into container now
2025-10-29 10:13:56,095:INFO:_master_model_container: 9
2025-10-29 10:13:56,095:INFO:_display_container: 2
2025-10-29 10:13:56,096:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=441239)
2025-10-29 10:13:56,096:INFO:create_model() successfully completed......................................
2025-10-29 10:13:56,265:INFO:SubProcess create_model() end ==================================
2025-10-29 10:13:56,265:INFO:Creating metrics dataframe
2025-10-29 10:13:56,279:INFO:Initializing Gradient Boosting Classifier
2025-10-29 10:13:56,279:INFO:Total runtime is 0.34774942000706993 minutes
2025-10-29 10:13:56,282:INFO:SubProcess create_model() called ==================================
2025-10-29 10:13:56,283:INFO:Initializing create_model()
2025-10-29 10:13:56,283:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A44E6CA790>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A45351CE50>, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:13:56,283:INFO:Checking exceptions
2025-10-29 10:13:56,283:INFO:Importing libraries
2025-10-29 10:13:56,283:INFO:Copying training dataset
2025-10-29 10:13:56,288:INFO:Defining folds
2025-10-29 10:13:56,288:INFO:Declaring metric variables
2025-10-29 10:13:56,292:INFO:Importing untrained model
2025-10-29 10:13:56,297:INFO:Gradient Boosting Classifier Imported successfully
2025-10-29 10:13:56,305:INFO:Starting cross validation
2025-10-29 10:13:56,307:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 10:13:56,309:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 10:13:56,530:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:56,531:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:56,532:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:56,533:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:56,534:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:56,537:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:56,538:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:56,554:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:56,554:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:56,556:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:56,559:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:56,561:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:56,562:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:56,570:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:56,572:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:56,574:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:56,576:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:56,585:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:56,940:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:56,942:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:56,944:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:56,946:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:56,947:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:56,949:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:56,951:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:56,953:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:56,955:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:56,956:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:56,959:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:56,961:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:56,963:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:56,965:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:56,966:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:56,977:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:56,979:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:56,982:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:56,984:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:56,985:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:57,930:INFO:Calculating mean and std
2025-10-29 10:13:57,931:INFO:Creating metrics dataframe
2025-10-29 10:13:58,149:INFO:Uploading results into container
2025-10-29 10:13:58,150:INFO:Uploading model into container now
2025-10-29 10:13:58,150:INFO:_master_model_container: 10
2025-10-29 10:13:58,150:INFO:_display_container: 2
2025-10-29 10:13:58,151:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=441239, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-10-29 10:13:58,151:INFO:create_model() successfully completed......................................
2025-10-29 10:13:58,322:INFO:SubProcess create_model() end ==================================
2025-10-29 10:13:58,322:INFO:Creating metrics dataframe
2025-10-29 10:13:58,336:INFO:Initializing Linear Discriminant Analysis
2025-10-29 10:13:58,336:INFO:Total runtime is 0.3820411006609599 minutes
2025-10-29 10:13:58,340:INFO:SubProcess create_model() called ==================================
2025-10-29 10:13:58,340:INFO:Initializing create_model()
2025-10-29 10:13:58,340:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A44E6CA790>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A45351CE50>, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:13:58,341:INFO:Checking exceptions
2025-10-29 10:13:58,341:INFO:Importing libraries
2025-10-29 10:13:58,341:INFO:Copying training dataset
2025-10-29 10:13:58,346:INFO:Defining folds
2025-10-29 10:13:58,346:INFO:Declaring metric variables
2025-10-29 10:13:58,350:INFO:Importing untrained model
2025-10-29 10:13:58,354:INFO:Linear Discriminant Analysis Imported successfully
2025-10-29 10:13:58,362:INFO:Starting cross validation
2025-10-29 10:13:58,364:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 10:13:58,366:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 10:13:58,469:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:58,471:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:58,472:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:58,474:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:58,475:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:58,476:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:58,477:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:58,477:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:58,480:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:58,481:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:58,486:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:58,489:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:58,492:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:58,494:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:58,495:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:58,500:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:58,502:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:58,504:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:58,506:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:58,507:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:58,791:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:58,794:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:58,795:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:58,795:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:58,796:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:58,797:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:58,799:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:58,800:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:58,800:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:58,801:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:58,802:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:58,805:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:58,808:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:58,809:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:58,810:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:13:58,815:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:58,821:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:13:58,826:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:13:58,828:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:13:59,952:INFO:Calculating mean and std
2025-10-29 10:13:59,954:INFO:Creating metrics dataframe
2025-10-29 10:14:00,178:INFO:Uploading results into container
2025-10-29 10:14:00,180:INFO:Uploading model into container now
2025-10-29 10:14:00,180:INFO:_master_model_container: 11
2025-10-29 10:14:00,180:INFO:_display_container: 2
2025-10-29 10:14:00,181:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2025-10-29 10:14:00,181:INFO:create_model() successfully completed......................................
2025-10-29 10:14:00,360:INFO:SubProcess create_model() end ==================================
2025-10-29 10:14:00,361:INFO:Creating metrics dataframe
2025-10-29 10:14:00,375:INFO:Initializing Extra Trees Classifier
2025-10-29 10:14:00,375:INFO:Total runtime is 0.4160169323285421 minutes
2025-10-29 10:14:00,378:INFO:SubProcess create_model() called ==================================
2025-10-29 10:14:00,378:INFO:Initializing create_model()
2025-10-29 10:14:00,379:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A44E6CA790>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A45351CE50>, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:14:00,379:INFO:Checking exceptions
2025-10-29 10:14:00,379:INFO:Importing libraries
2025-10-29 10:14:00,379:INFO:Copying training dataset
2025-10-29 10:14:00,384:INFO:Defining folds
2025-10-29 10:14:00,384:INFO:Declaring metric variables
2025-10-29 10:14:00,388:INFO:Importing untrained model
2025-10-29 10:14:00,392:INFO:Extra Trees Classifier Imported successfully
2025-10-29 10:14:00,400:INFO:Starting cross validation
2025-10-29 10:14:00,401:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 10:14:00,403:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 10:14:00,778:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:00,781:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:00,786:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:00,789:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:14:00,792:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:14:00,797:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:00,800:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:00,802:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:00,805:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:14:00,805:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:00,806:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:14:00,837:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:00,839:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:00,841:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:00,842:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:00,844:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:14:00,845:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:14:00,951:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:00,953:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:00,956:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:00,959:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:14:00,960:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:14:01,327:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:01,329:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:01,331:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:01,332:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:01,334:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:14:01,334:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:14:01,335:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:01,337:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:01,340:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:14:01,341:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:14:01,354:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:01,357:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:01,359:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:01,362:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:14:01,362:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:14:01,364:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:01,366:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:01,368:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:01,370:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:14:01,371:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:14:02,228:INFO:Calculating mean and std
2025-10-29 10:14:02,230:INFO:Creating metrics dataframe
2025-10-29 10:14:02,442:INFO:Uploading results into container
2025-10-29 10:14:02,443:INFO:Uploading model into container now
2025-10-29 10:14:02,444:INFO:_master_model_container: 12
2025-10-29 10:14:02,444:INFO:_display_container: 2
2025-10-29 10:14:02,445:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=441239, verbose=0, warm_start=False)
2025-10-29 10:14:02,445:INFO:create_model() successfully completed......................................
2025-10-29 10:14:02,618:INFO:SubProcess create_model() end ==================================
2025-10-29 10:14:02,618:INFO:Creating metrics dataframe
2025-10-29 10:14:02,633:INFO:Initializing Extreme Gradient Boosting
2025-10-29 10:14:02,633:INFO:Total runtime is 0.45365116993586224 minutes
2025-10-29 10:14:02,636:INFO:SubProcess create_model() called ==================================
2025-10-29 10:14:02,637:INFO:Initializing create_model()
2025-10-29 10:14:02,637:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A44E6CA790>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A45351CE50>, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:14:02,637:INFO:Checking exceptions
2025-10-29 10:14:02,637:INFO:Importing libraries
2025-10-29 10:14:02,637:INFO:Copying training dataset
2025-10-29 10:14:02,642:INFO:Defining folds
2025-10-29 10:14:02,643:INFO:Declaring metric variables
2025-10-29 10:14:02,647:INFO:Importing untrained model
2025-10-29 10:14:02,653:INFO:Extreme Gradient Boosting Imported successfully
2025-10-29 10:14:02,660:INFO:Starting cross validation
2025-10-29 10:14:02,661:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 10:14:02,670:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 10:14:03,626:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:03,745:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:03,883:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:03,886:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:03,888:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:03,891:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:14:03,893:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:14:03,925:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:03,928:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:03,930:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:03,932:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:14:03,933:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:14:03,937:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:03,940:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:03,941:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:03,944:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:14:03,945:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:14:04,020:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:04,022:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:04,024:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:04,026:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:14:04,027:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:14:04,035:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:04,037:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:04,039:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:04,041:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:14:04,042:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:14:04,158:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:04,161:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:04,164:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:04,167:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:14:04,168:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:14:04,250:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:04,253:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:04,256:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:04,259:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:14:04,260:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:14:04,288:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:04,290:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:04,293:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:04,295:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:14:04,296:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:14:05,107:INFO:Calculating mean and std
2025-10-29 10:14:05,108:INFO:Creating metrics dataframe
2025-10-29 10:14:05,338:INFO:Uploading results into container
2025-10-29 10:14:05,339:INFO:Uploading model into container now
2025-10-29 10:14:05,339:INFO:_master_model_container: 13
2025-10-29 10:14:05,339:INFO:_display_container: 2
2025-10-29 10:14:05,340:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device=None, early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2025-10-29 10:14:05,341:INFO:create_model() successfully completed......................................
2025-10-29 10:14:05,529:INFO:SubProcess create_model() end ==================================
2025-10-29 10:14:05,529:INFO:Creating metrics dataframe
2025-10-29 10:14:05,544:INFO:Initializing Light Gradient Boosting Machine
2025-10-29 10:14:05,544:INFO:Total runtime is 0.5021717786788941 minutes
2025-10-29 10:14:05,549:INFO:SubProcess create_model() called ==================================
2025-10-29 10:14:05,549:INFO:Initializing create_model()
2025-10-29 10:14:05,550:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A44E6CA790>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A45351CE50>, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:14:05,550:INFO:Checking exceptions
2025-10-29 10:14:05,550:INFO:Importing libraries
2025-10-29 10:14:05,550:INFO:Copying training dataset
2025-10-29 10:14:05,554:INFO:Defining folds
2025-10-29 10:14:05,555:INFO:Declaring metric variables
2025-10-29 10:14:05,559:INFO:Importing untrained model
2025-10-29 10:14:05,565:INFO:Light Gradient Boosting Machine Imported successfully
2025-10-29 10:14:05,573:INFO:Starting cross validation
2025-10-29 10:14:05,574:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 10:14:05,576:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 10:14:05,784:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:05,788:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:05,789:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:05,790:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:05,791:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:05,793:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:14:05,794:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:14:05,795:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:05,797:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:14:05,798:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:14:05,875:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:05,877:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:05,879:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:05,882:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:14:05,883:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:14:05,901:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:05,904:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:05,906:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:05,908:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:14:05,910:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:14:06,275:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:06,278:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:06,286:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:06,288:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:06,289:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:06,290:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:06,291:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:06,293:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:14:06,293:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:06,294:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:14:06,295:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:14:06,296:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:14:06,302:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:06,304:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:06,307:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:06,314:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:14:06,314:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:14:06,724:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:07,199:INFO:Calculating mean and std
2025-10-29 10:14:07,201:INFO:Creating metrics dataframe
2025-10-29 10:14:07,416:INFO:Uploading results into container
2025-10-29 10:14:07,417:INFO:Uploading model into container now
2025-10-29 10:14:07,417:INFO:_master_model_container: 14
2025-10-29 10:14:07,417:INFO:_display_container: 2
2025-10-29 10:14:07,418:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=441239, reg_alpha=0.0, reg_lambda=0.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2025-10-29 10:14:07,418:INFO:create_model() successfully completed......................................
2025-10-29 10:14:07,594:INFO:SubProcess create_model() end ==================================
2025-10-29 10:14:07,595:INFO:Creating metrics dataframe
2025-10-29 10:14:07,611:INFO:Initializing CatBoost Classifier
2025-10-29 10:14:07,611:INFO:Total runtime is 0.5366130232810974 minutes
2025-10-29 10:14:07,616:INFO:SubProcess create_model() called ==================================
2025-10-29 10:14:07,617:INFO:Initializing create_model()
2025-10-29 10:14:07,617:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A44E6CA790>, estimator=catboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A45351CE50>, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:14:07,617:INFO:Checking exceptions
2025-10-29 10:14:07,617:INFO:Importing libraries
2025-10-29 10:14:07,617:INFO:Copying training dataset
2025-10-29 10:14:07,623:INFO:Defining folds
2025-10-29 10:14:07,623:INFO:Declaring metric variables
2025-10-29 10:14:07,629:INFO:Importing untrained model
2025-10-29 10:14:07,634:INFO:CatBoost Classifier Imported successfully
2025-10-29 10:14:07,642:INFO:Starting cross validation
2025-10-29 10:14:07,643:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 10:14:07,644:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 10:14:08,565:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:08,567:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:08,569:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:08,572:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:14:08,573:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:14:08,589:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:08,595:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:08,599:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:08,602:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:14:08,603:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:14:08,647:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:08,649:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:08,651:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:08,654:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:14:08,655:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:14:08,682:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:08,814:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:08,818:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:08,824:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:08,828:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:14:08,829:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:14:09,074:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:09,077:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:09,080:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:09,083:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:14:09,088:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:14:09,109:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:09,111:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:09,114:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:09,116:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:14:09,117:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:14:09,123:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:09,126:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:09,127:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:09,129:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:09,129:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:09,131:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:09,131:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:14:09,133:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:14:09,134:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:14:09,135:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:14:10,078:INFO:Calculating mean and std
2025-10-29 10:14:10,079:INFO:Creating metrics dataframe
2025-10-29 10:14:10,301:INFO:Uploading results into container
2025-10-29 10:14:10,301:INFO:Uploading model into container now
2025-10-29 10:14:10,302:INFO:_master_model_container: 15
2025-10-29 10:14:10,302:INFO:_display_container: 2
2025-10-29 10:14:10,302:INFO:<catboost.core.CatBoostClassifier object at 0x000001A43CA2E700>
2025-10-29 10:14:10,302:INFO:create_model() successfully completed......................................
2025-10-29 10:14:10,478:INFO:SubProcess create_model() end ==================================
2025-10-29 10:14:10,478:INFO:Creating metrics dataframe
2025-10-29 10:14:10,493:INFO:Initializing Dummy Classifier
2025-10-29 10:14:10,493:INFO:Total runtime is 0.584649384021759 minutes
2025-10-29 10:14:10,498:INFO:SubProcess create_model() called ==================================
2025-10-29 10:14:10,498:INFO:Initializing create_model()
2025-10-29 10:14:10,498:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A44E6CA790>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A45351CE50>, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:14:10,498:INFO:Checking exceptions
2025-10-29 10:14:10,498:INFO:Importing libraries
2025-10-29 10:14:10,498:INFO:Copying training dataset
2025-10-29 10:14:10,504:INFO:Defining folds
2025-10-29 10:14:10,504:INFO:Declaring metric variables
2025-10-29 10:14:10,508:INFO:Importing untrained model
2025-10-29 10:14:10,512:INFO:Dummy Classifier Imported successfully
2025-10-29 10:14:10,520:INFO:Starting cross validation
2025-10-29 10:14:10,520:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 10:14:10,522:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 10:14:10,619:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:10,622:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:10,624:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:10,626:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:10,628:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:10,629:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:10,631:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:10,634:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:10,634:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:14:10,636:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:14:10,636:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:10,638:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:10,641:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:14:10,643:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:14:10,645:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:14:10,651:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:14:10,652:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:10,666:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:10,668:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:10,670:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:10,672:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:14:10,673:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:14:10,920:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:10,920:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:10,922:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:10,922:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:10,924:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:10,924:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:10,925:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:10,926:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:10,927:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:14:10,927:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:14:10,928:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:14:10,928:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:14:10,929:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:10,931:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:14:10,932:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:14:10,933:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:10,935:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:10,937:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:10,940:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:14:10,941:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:14:12,113:INFO:Calculating mean and std
2025-10-29 10:14:12,115:INFO:Creating metrics dataframe
2025-10-29 10:14:12,331:INFO:Uploading results into container
2025-10-29 10:14:12,331:INFO:Uploading model into container now
2025-10-29 10:14:12,332:INFO:_master_model_container: 16
2025-10-29 10:14:12,332:INFO:_display_container: 2
2025-10-29 10:14:12,332:INFO:DummyClassifier(constant=None, random_state=441239, strategy='prior')
2025-10-29 10:14:12,332:INFO:create_model() successfully completed......................................
2025-10-29 10:14:12,574:INFO:SubProcess create_model() end ==================================
2025-10-29 10:14:12,574:INFO:Creating metrics dataframe
2025-10-29 10:14:12,607:INFO:Initializing create_model()
2025-10-29 10:14:12,607:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A44E6CA790>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=441239, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:14:12,607:INFO:Checking exceptions
2025-10-29 10:14:12,609:INFO:Importing libraries
2025-10-29 10:14:12,609:INFO:Copying training dataset
2025-10-29 10:14:12,615:INFO:Defining folds
2025-10-29 10:14:12,615:INFO:Declaring metric variables
2025-10-29 10:14:12,615:INFO:Importing untrained model
2025-10-29 10:14:12,615:INFO:Declaring custom model
2025-10-29 10:14:12,616:INFO:Logistic Regression Imported successfully
2025-10-29 10:14:12,617:INFO:Cross validation set to False
2025-10-29 10:14:12,617:INFO:Fitting Model
2025-10-29 10:14:12,800:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=441239, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-10-29 10:14:12,800:INFO:create_model() successfully completed......................................
2025-10-29 10:14:12,977:INFO:Initializing create_model()
2025-10-29 10:14:12,977:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A44E6CA790>, estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform'), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:14:12,977:INFO:Checking exceptions
2025-10-29 10:14:12,979:INFO:Importing libraries
2025-10-29 10:14:12,979:INFO:Copying training dataset
2025-10-29 10:14:12,983:INFO:Defining folds
2025-10-29 10:14:12,983:INFO:Declaring metric variables
2025-10-29 10:14:12,984:INFO:Importing untrained model
2025-10-29 10:14:12,984:INFO:Declaring custom model
2025-10-29 10:14:12,984:INFO:K Neighbors Classifier Imported successfully
2025-10-29 10:14:12,985:INFO:Cross validation set to False
2025-10-29 10:14:12,985:INFO:Fitting Model
2025-10-29 10:14:13,156:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-10-29 10:14:13,156:INFO:create_model() successfully completed......................................
2025-10-29 10:14:13,344:INFO:Initializing create_model()
2025-10-29 10:14:13,344:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A44E6CA790>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=441239, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:14:13,344:INFO:Checking exceptions
2025-10-29 10:14:13,347:INFO:Importing libraries
2025-10-29 10:14:13,347:INFO:Copying training dataset
2025-10-29 10:14:13,351:INFO:Defining folds
2025-10-29 10:14:13,351:INFO:Declaring metric variables
2025-10-29 10:14:13,351:INFO:Importing untrained model
2025-10-29 10:14:13,351:INFO:Declaring custom model
2025-10-29 10:14:13,352:INFO:Random Forest Classifier Imported successfully
2025-10-29 10:14:13,353:INFO:Cross validation set to False
2025-10-29 10:14:13,353:INFO:Fitting Model
2025-10-29 10:14:13,852:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=441239, verbose=0, warm_start=False)
2025-10-29 10:14:13,852:INFO:create_model() successfully completed......................................
2025-10-29 10:14:14,079:INFO:Initializing create_model()
2025-10-29 10:14:14,079:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A44E6CA790>, estimator=LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:14:14,079:INFO:Checking exceptions
2025-10-29 10:14:14,081:INFO:Importing libraries
2025-10-29 10:14:14,082:INFO:Copying training dataset
2025-10-29 10:14:14,087:INFO:Defining folds
2025-10-29 10:14:14,087:INFO:Declaring metric variables
2025-10-29 10:14:14,087:INFO:Importing untrained model
2025-10-29 10:14:14,088:INFO:Declaring custom model
2025-10-29 10:14:14,088:INFO:Linear Discriminant Analysis Imported successfully
2025-10-29 10:14:14,089:INFO:Cross validation set to False
2025-10-29 10:14:14,089:INFO:Fitting Model
2025-10-29 10:14:14,282:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2025-10-29 10:14:14,282:INFO:create_model() successfully completed......................................
2025-10-29 10:14:14,456:INFO:Initializing create_model()
2025-10-29 10:14:14,456:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A44E6CA790>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=441239, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:14:14,456:INFO:Checking exceptions
2025-10-29 10:14:14,458:INFO:Importing libraries
2025-10-29 10:14:14,459:INFO:Copying training dataset
2025-10-29 10:14:14,463:INFO:Defining folds
2025-10-29 10:14:14,463:INFO:Declaring metric variables
2025-10-29 10:14:14,463:INFO:Importing untrained model
2025-10-29 10:14:14,463:INFO:Declaring custom model
2025-10-29 10:14:14,464:INFO:Extra Trees Classifier Imported successfully
2025-10-29 10:14:14,464:INFO:Cross validation set to False
2025-10-29 10:14:14,465:INFO:Fitting Model
2025-10-29 10:14:14,855:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=441239, verbose=0, warm_start=False)
2025-10-29 10:14:14,855:INFO:create_model() successfully completed......................................
2025-10-29 10:14:15,034:INFO:Initializing create_model()
2025-10-29 10:14:15,034:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A44E6CA790>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device=None, early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:14:15,034:INFO:Checking exceptions
2025-10-29 10:14:15,036:INFO:Importing libraries
2025-10-29 10:14:15,036:INFO:Copying training dataset
2025-10-29 10:14:15,040:INFO:Defining folds
2025-10-29 10:14:15,040:INFO:Declaring metric variables
2025-10-29 10:14:15,040:INFO:Importing untrained model
2025-10-29 10:14:15,040:INFO:Declaring custom model
2025-10-29 10:14:15,041:INFO:Extreme Gradient Boosting Imported successfully
2025-10-29 10:14:15,042:INFO:Cross validation set to False
2025-10-29 10:14:15,042:INFO:Fitting Model
2025-10-29 10:14:15,234:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device=None, early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2025-10-29 10:14:15,234:INFO:create_model() successfully completed......................................
2025-10-29 10:14:15,413:INFO:Initializing create_model()
2025-10-29 10:14:15,413:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A44E6CA790>, estimator=<catboost.core.CatBoostClassifier object at 0x000001A43CA2E700>, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:14:15,413:INFO:Checking exceptions
2025-10-29 10:14:15,415:INFO:Importing libraries
2025-10-29 10:14:15,415:INFO:Copying training dataset
2025-10-29 10:14:15,420:INFO:Defining folds
2025-10-29 10:14:15,420:INFO:Declaring metric variables
2025-10-29 10:14:15,420:INFO:Importing untrained model
2025-10-29 10:14:15,420:INFO:Declaring custom model
2025-10-29 10:14:15,421:INFO:CatBoost Classifier Imported successfully
2025-10-29 10:14:15,421:INFO:Cross validation set to False
2025-10-29 10:14:15,421:INFO:Fitting Model
2025-10-29 10:14:15,605:INFO:<catboost.core.CatBoostClassifier object at 0x000001A42AC7C850>
2025-10-29 10:14:15,605:INFO:create_model() successfully completed......................................
2025-10-29 10:14:15,781:INFO:Initializing create_model()
2025-10-29 10:14:15,782:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A44E6CA790>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=441239, splitter='best'), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:14:15,782:INFO:Checking exceptions
2025-10-29 10:14:15,784:INFO:Importing libraries
2025-10-29 10:14:15,784:INFO:Copying training dataset
2025-10-29 10:14:15,787:INFO:Defining folds
2025-10-29 10:14:15,788:INFO:Declaring metric variables
2025-10-29 10:14:15,788:INFO:Importing untrained model
2025-10-29 10:14:15,788:INFO:Declaring custom model
2025-10-29 10:14:15,788:INFO:Decision Tree Classifier Imported successfully
2025-10-29 10:14:15,789:INFO:Cross validation set to False
2025-10-29 10:14:15,789:INFO:Fitting Model
2025-10-29 10:14:15,964:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=441239, splitter='best')
2025-10-29 10:14:15,964:INFO:create_model() successfully completed......................................
2025-10-29 10:14:16,136:INFO:Initializing create_model()
2025-10-29 10:14:16,137:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A44E6CA790>, estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=441239), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:14:16,137:INFO:Checking exceptions
2025-10-29 10:14:16,139:INFO:Importing libraries
2025-10-29 10:14:16,139:INFO:Copying training dataset
2025-10-29 10:14:16,142:INFO:Defining folds
2025-10-29 10:14:16,143:INFO:Declaring metric variables
2025-10-29 10:14:16,143:INFO:Importing untrained model
2025-10-29 10:14:16,143:INFO:Declaring custom model
2025-10-29 10:14:16,143:INFO:str Imported successfully
2025-10-29 10:14:16,144:INFO:Cross validation set to False
2025-10-29 10:14:16,144:INFO:Fitting Model
2025-10-29 10:14:16,313:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=441239)
2025-10-29 10:14:16,313:INFO:create_model() successfully completed......................................
2025-10-29 10:14:16,485:INFO:Initializing create_model()
2025-10-29 10:14:16,485:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A44E6CA790>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=441239, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:14:16,485:INFO:Checking exceptions
2025-10-29 10:14:16,487:INFO:Importing libraries
2025-10-29 10:14:16,487:INFO:Copying training dataset
2025-10-29 10:14:16,491:INFO:Defining folds
2025-10-29 10:14:16,491:INFO:Declaring metric variables
2025-10-29 10:14:16,491:INFO:Importing untrained model
2025-10-29 10:14:16,492:INFO:Declaring custom model
2025-10-29 10:14:16,492:INFO:Gradient Boosting Classifier Imported successfully
2025-10-29 10:14:16,493:INFO:Cross validation set to False
2025-10-29 10:14:16,493:INFO:Fitting Model
2025-10-29 10:14:16,677:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=441239, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-10-29 10:14:16,677:INFO:create_model() successfully completed......................................
2025-10-29 10:14:16,872:INFO:_master_model_container: 16
2025-10-29 10:14:16,872:INFO:_display_container: 2
2025-10-29 10:14:16,875:INFO:[LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=441239, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform'), RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=441239, verbose=0, warm_start=False), LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001), ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=441239, verbose=0, warm_start=False), XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device=None, early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...), <catboost.core.CatBoostClassifier object at 0x000001A42AC7C850>, DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=441239, splitter='best'), AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=441239), GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=441239, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)]
2025-10-29 10:14:16,875:INFO:compare_models() successfully completed......................................
2025-10-29 10:14:16,876:INFO:Initializing tune_model()
2025-10-29 10:14:16,876:INFO:tune_model(estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=441239, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A44E6CA790>)
2025-10-29 10:14:16,876:INFO:Checking exceptions
2025-10-29 10:14:16,895:INFO:Copying training dataset
2025-10-29 10:14:16,899:INFO:Checking base model
2025-10-29 10:14:16,899:INFO:Base model : Logistic Regression
2025-10-29 10:14:16,902:INFO:Declaring metric variables
2025-10-29 10:14:16,906:INFO:Defining Hyperparameters
2025-10-29 10:14:17,122:INFO:Tuning with n_jobs=-1
2025-10-29 10:14:17,122:INFO:Initializing RandomizedSearchCV
2025-10-29 10:14:17,126:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 10:14:33,387:INFO:best_params: {'actual_estimator__class_weight': 'balanced', 'actual_estimator__C': 9.859}
2025-10-29 10:14:33,388:INFO:Hyperparameter search completed
2025-10-29 10:14:33,388:INFO:SubProcess create_model() called ==================================
2025-10-29 10:14:33,388:INFO:Initializing create_model()
2025-10-29 10:14:33,388:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A44E6CA790>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=441239, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A4265B6FD0>, model_only=True, return_train_score=False, kwargs={'class_weight': 'balanced', 'C': 9.859})
2025-10-29 10:14:33,389:INFO:Checking exceptions
2025-10-29 10:14:33,389:INFO:Importing libraries
2025-10-29 10:14:33,389:INFO:Copying training dataset
2025-10-29 10:14:33,392:INFO:Defining folds
2025-10-29 10:14:33,392:INFO:Declaring metric variables
2025-10-29 10:14:33,396:INFO:Importing untrained model
2025-10-29 10:14:33,397:INFO:Declaring custom model
2025-10-29 10:14:33,401:INFO:Logistic Regression Imported successfully
2025-10-29 10:14:33,408:INFO:Starting cross validation
2025-10-29 10:14:33,409:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 10:14:33,411:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 10:14:33,511:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:33,514:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:33,516:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:33,518:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:33,518:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:14:33,519:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:14:33,520:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:33,522:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:33,525:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:14:33,526:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:14:33,532:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:33,534:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:33,536:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:33,538:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:14:33,539:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:14:33,547:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:33,549:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:33,551:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:33,553:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:14:33,555:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:14:33,790:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:33,792:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:33,794:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:33,796:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:14:33,797:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:14:33,800:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:33,800:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:33,802:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:33,802:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:33,804:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:33,805:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:33,806:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:14:33,807:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:14:33,807:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:14:33,808:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:14:33,814:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:33,816:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:33,818:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:33,820:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:14:33,821:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:14:34,871:INFO:Calculating mean and std
2025-10-29 10:14:34,873:INFO:Creating metrics dataframe
2025-10-29 10:14:34,879:INFO:Finalizing model
2025-10-29 10:14:35,137:INFO:Uploading results into container
2025-10-29 10:14:35,137:INFO:Uploading model into container now
2025-10-29 10:14:35,138:INFO:_master_model_container: 17
2025-10-29 10:14:35,138:INFO:_display_container: 3
2025-10-29 10:14:35,138:INFO:LogisticRegression(C=9.859, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=1000, multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=441239, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-10-29 10:14:35,138:INFO:create_model() successfully completed......................................
2025-10-29 10:14:35,321:INFO:SubProcess create_model() end ==================================
2025-10-29 10:14:35,321:INFO:choose_better activated
2025-10-29 10:14:35,325:INFO:SubProcess create_model() called ==================================
2025-10-29 10:14:35,326:INFO:Initializing create_model()
2025-10-29 10:14:35,326:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A44E6CA790>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=441239, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2025-10-29 10:14:35,326:INFO:Checking exceptions
2025-10-29 10:14:35,328:INFO:Importing libraries
2025-10-29 10:14:35,328:INFO:Copying training dataset
2025-10-29 10:14:35,332:INFO:Defining folds
2025-10-29 10:14:35,332:INFO:Declaring metric variables
2025-10-29 10:14:35,332:INFO:Importing untrained model
2025-10-29 10:14:35,332:INFO:Declaring custom model
2025-10-29 10:14:35,333:INFO:Logistic Regression Imported successfully
2025-10-29 10:14:35,333:INFO:Starting cross validation
2025-10-29 10:14:35,334:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 10:14:35,335:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 10:14:35,444:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:35,446:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:35,448:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:35,449:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:35,450:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:35,451:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:14:35,452:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:14:35,452:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:35,453:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:35,454:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:14:35,456:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:14:35,458:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:35,460:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:35,462:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:35,464:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:14:35,464:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:35,465:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:14:35,474:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:35,477:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:35,479:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:35,481:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:14:35,482:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:14:35,716:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:35,719:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:35,721:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:35,723:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:14:35,724:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:14:35,730:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:35,733:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:35,735:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:35,737:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:14:35,738:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:14:35,745:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:35,747:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:35,749:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:35,751:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:14:35,752:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:14:35,755:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 10:14:35,757:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:35,759:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:14:35,761:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 10:14:35,762:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 10:14:36,808:INFO:Calculating mean and std
2025-10-29 10:14:36,808:INFO:Creating metrics dataframe
2025-10-29 10:14:36,810:INFO:Finalizing model
2025-10-29 10:14:37,055:INFO:Uploading results into container
2025-10-29 10:14:37,055:INFO:Uploading model into container now
2025-10-29 10:14:37,056:INFO:_master_model_container: 18
2025-10-29 10:14:37,056:INFO:_display_container: 4
2025-10-29 10:14:37,056:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=441239, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-10-29 10:14:37,056:INFO:create_model() successfully completed......................................
2025-10-29 10:14:37,223:INFO:SubProcess create_model() end ==================================
2025-10-29 10:14:37,224:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=441239, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False) result for Accuracy is 0.9994
2025-10-29 10:14:37,224:INFO:LogisticRegression(C=9.859, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=1000, multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=441239, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False) result for Accuracy is 1.0
2025-10-29 10:14:37,225:INFO:LogisticRegression(C=9.859, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=1000, multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=441239, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False) is best model
2025-10-29 10:14:37,225:INFO:choose_better completed
2025-10-29 10:14:37,235:INFO:_master_model_container: 18
2025-10-29 10:14:37,235:INFO:_display_container: 3
2025-10-29 10:14:37,236:INFO:LogisticRegression(C=9.859, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=1000, multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=441239, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-10-29 10:14:37,236:INFO:tune_model() successfully completed......................................
2025-10-29 10:14:37,548:INFO:Initializing evaluate_model()
2025-10-29 10:14:37,548:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A44E6CA790>, estimator=LogisticRegression(C=9.859, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=1000, multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=441239, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None, use_train_data=False)
2025-10-29 10:14:37,556:INFO:Initializing plot_model()
2025-10-29 10:14:37,556:INFO:plot_model(plot=pipeline, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=LogisticRegression(C=9.859, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=1000, multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=441239, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A44E6CA790>, system=True)
2025-10-29 10:14:37,557:INFO:Checking exceptions
2025-10-29 10:14:37,559:INFO:Preloading libraries
2025-10-29 10:14:37,559:INFO:Copying training dataset
2025-10-29 10:14:37,559:INFO:Plot type: pipeline
2025-10-29 10:14:37,657:INFO:Visual Rendered Successfully
2025-10-29 10:14:37,828:INFO:plot_model() successfully completed......................................
2025-10-29 10:15:42,426:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:15:42,428:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:15:42,430:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:16:37,721:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:16:37,725:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:16:37,727:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 10:18:08,871:INFO:PyCaret ClassificationExperiment
2025-10-29 10:18:08,871:INFO:Logging name: clf-default-name
2025-10-29 10:18:08,871:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-10-29 10:18:08,871:INFO:version 3.0.0
2025-10-29 10:18:08,871:INFO:Initializing setup()
2025-10-29 10:18:08,871:INFO:self.USI: 1785
2025-10-29 10:18:08,871:INFO:self._variable_keys: {'target_param', 'seed', 'y_train', 'fold_shuffle_param', 'X_train', 'is_multiclass', 'y_test', '_ml_usecase', 'X', 'log_plots_param', 'USI', 'exp_id', 'pipeline', 'idx', 'y', '_available_plots', 'html_param', 'fold_groups_param', 'exp_name_log', 'X_test', 'fold_generator', 'logging_param', 'gpu_n_jobs_param', 'n_jobs_param', 'memory', 'data', 'gpu_param', 'fix_imbalance'}
2025-10-29 10:18:08,871:INFO:Checking environment
2025-10-29 10:18:08,871:INFO:python_version: 3.8.15
2025-10-29 10:18:08,871:INFO:python_build: ('default', 'Nov 24 2022 14:38:14')
2025-10-29 10:18:08,872:INFO:machine: AMD64
2025-10-29 10:18:08,872:INFO:platform: Windows-10-10.0.19045-SP0
2025-10-29 10:18:08,874:INFO:Memory: svmem(total=34299187200, available=21750325248, percent=36.6, used=12548861952, free=21750325248)
2025-10-29 10:18:08,874:INFO:Physical Core: 6
2025-10-29 10:18:08,874:INFO:Logical Core: 6
2025-10-29 10:18:08,874:INFO:Checking libraries
2025-10-29 10:18:08,874:INFO:System:
2025-10-29 10:18:08,874:INFO:    python: 3.8.15 (default, Nov 24 2022, 14:38:14) [MSC v.1916 64 bit (AMD64)]
2025-10-29 10:18:08,874:INFO:executable: c:\Users\hkh97\anaconda3\envs\han\python.exe
2025-10-29 10:18:08,874:INFO:   machine: Windows-10-10.0.19045-SP0
2025-10-29 10:18:08,874:INFO:PyCaret required dependencies:
2025-10-29 10:18:08,874:INFO:                 pip: 25.0.1
2025-10-29 10:18:08,874:INFO:          setuptools: 75.1.0
2025-10-29 10:18:08,874:INFO:             pycaret: 3.0.0
2025-10-29 10:18:08,875:INFO:             IPython: 8.12.3
2025-10-29 10:18:08,875:INFO:          ipywidgets: 8.1.7
2025-10-29 10:18:08,875:INFO:                tqdm: 4.67.1
2025-10-29 10:18:08,875:INFO:               numpy: 1.24.1
2025-10-29 10:18:08,875:INFO:              pandas: 1.5.3
2025-10-29 10:18:08,875:INFO:              jinja2: 3.1.4
2025-10-29 10:18:08,875:INFO:               scipy: 1.10.1
2025-10-29 10:18:08,875:INFO:              joblib: 1.2.0
2025-10-29 10:18:08,875:INFO:             sklearn: 1.2.2
2025-10-29 10:18:08,875:INFO:                pyod: 2.0.5
2025-10-29 10:18:08,875:INFO:            imblearn: 0.12.4
2025-10-29 10:18:08,875:INFO:   category_encoders: 2.6.4
2025-10-29 10:18:08,875:INFO:            lightgbm: 4.6.0
2025-10-29 10:18:08,875:INFO:               numba: 0.58.1
2025-10-29 10:18:08,875:INFO:            requests: 2.32.4
2025-10-29 10:18:08,875:INFO:          matplotlib: 3.6.0
2025-10-29 10:18:08,875:INFO:          scikitplot: 0.3.7
2025-10-29 10:18:08,875:INFO:         yellowbrick: 1.5
2025-10-29 10:18:08,875:INFO:              plotly: 6.3.0
2025-10-29 10:18:08,875:INFO:             kaleido: 1.1.0
2025-10-29 10:18:08,875:INFO:         statsmodels: 0.14.1
2025-10-29 10:18:08,875:INFO:              sktime: 0.21.1
2025-10-29 10:18:08,876:INFO:               tbats: 1.1.3
2025-10-29 10:18:08,876:INFO:            pmdarima: 2.0.4
2025-10-29 10:18:08,876:INFO:              psutil: 7.0.0
2025-10-29 10:18:08,876:INFO:PyCaret optional dependencies:
2025-10-29 10:18:08,876:INFO:                shap: Not installed
2025-10-29 10:18:08,876:INFO:           interpret: Not installed
2025-10-29 10:18:08,876:INFO:                umap: Not installed
2025-10-29 10:18:08,876:INFO:    pandas_profiling: Not installed
2025-10-29 10:18:08,876:INFO:  explainerdashboard: Not installed
2025-10-29 10:18:08,876:INFO:             autoviz: Not installed
2025-10-29 10:18:08,876:INFO:           fairlearn: Not installed
2025-10-29 10:18:08,876:INFO:             xgboost: 2.1.4
2025-10-29 10:18:08,876:INFO:            catboost: 1.2.8
2025-10-29 10:18:08,876:INFO:              kmodes: Not installed
2025-10-29 10:18:08,876:INFO:             mlxtend: Not installed
2025-10-29 10:18:08,876:INFO:       statsforecast: Not installed
2025-10-29 10:18:08,876:INFO:        tune_sklearn: Not installed
2025-10-29 10:18:08,876:INFO:                 ray: Not installed
2025-10-29 10:18:08,876:INFO:            hyperopt: Not installed
2025-10-29 10:18:08,876:INFO:              optuna: Not installed
2025-10-29 10:18:08,877:INFO:               skopt: Not installed
2025-10-29 10:18:08,877:INFO:              mlflow: Not installed
2025-10-29 10:18:08,877:INFO:              gradio: Not installed
2025-10-29 10:18:08,877:INFO:             fastapi: Not installed
2025-10-29 10:18:08,877:INFO:             uvicorn: Not installed
2025-10-29 10:18:08,877:INFO:              m2cgen: Not installed
2025-10-29 10:18:08,877:INFO:           evidently: Not installed
2025-10-29 10:18:08,877:INFO:               fugue: Not installed
2025-10-29 10:18:08,877:INFO:           streamlit: Not installed
2025-10-29 10:18:08,877:INFO:             prophet: Not installed
2025-10-29 10:18:08,877:INFO:None
2025-10-29 10:18:08,877:INFO:Set up data.
2025-10-29 10:18:08,882:INFO:Set up train/test split.
2025-10-29 10:18:08,885:INFO:Set up index.
2025-10-29 10:18:08,885:INFO:Set up folding strategy.
2025-10-29 10:18:08,886:INFO:Assigning column types.
2025-10-29 10:18:08,888:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-10-29 10:18:08,932:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-10-29 10:18:08,933:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-10-29 10:18:08,961:INFO:Soft dependency imported: xgboost: 2.1.4
2025-10-29 10:18:08,963:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-29 10:18:09,007:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-10-29 10:18:09,009:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-10-29 10:18:09,036:INFO:Soft dependency imported: xgboost: 2.1.4
2025-10-29 10:18:09,038:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-29 10:18:09,039:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-10-29 10:18:09,083:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-10-29 10:18:09,110:INFO:Soft dependency imported: xgboost: 2.1.4
2025-10-29 10:18:09,113:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-29 10:18:09,158:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-10-29 10:18:09,185:INFO:Soft dependency imported: xgboost: 2.1.4
2025-10-29 10:18:09,187:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-29 10:18:09,188:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-10-29 10:18:09,260:INFO:Soft dependency imported: xgboost: 2.1.4
2025-10-29 10:18:09,262:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-29 10:18:09,335:INFO:Soft dependency imported: xgboost: 2.1.4
2025-10-29 10:18:09,338:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-29 10:18:09,339:INFO:Preparing preprocessing pipeline...
2025-10-29 10:18:09,340:INFO:Set up simple imputation.
2025-10-29 10:18:09,340:INFO:Set up feature normalization.
2025-10-29 10:18:09,361:INFO:Finished creating preprocessing pipeline.
2025-10-29 10:18:09,366:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\hkh97\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Temperature', 'Pressure',
                                             'Vibration', 'GasFlow',
                                             'SteamOutput'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean',
                                                              verbose=...
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent',
                                                              verbose='deprecated'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2025-10-29 10:18:09,367:INFO:Creating final display dataframe.
2025-10-29 10:18:09,446:INFO:Setup _display_container:                     Description             Value
0                    Session id            441239
1                        Target           Failure
2                   Target type            Binary
3           Original data shape         (4368, 6)
4        Transformed data shape         (4368, 6)
5   Transformed train set shape         (3494, 6)
6    Transformed test set shape          (874, 6)
7              Numeric features                 5
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                    Normalize              True
13             Normalize method            zscore
14               Fold Generator   StratifiedKFold
15                  Fold Number                10
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              1785
2025-10-29 10:18:09,521:INFO:Soft dependency imported: xgboost: 2.1.4
2025-10-29 10:18:09,524:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-29 10:18:09,602:INFO:Soft dependency imported: xgboost: 2.1.4
2025-10-29 10:18:09,605:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-29 10:18:09,606:INFO:setup() successfully completed in 0.88s...............
2025-10-29 10:18:09,606:INFO:Initializing compare_models()
2025-10-29 10:18:09,606:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A42ACDF580>, include=None, fold=None, round=4, cross_validation=True, sort=precision , n_select=10, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001A42ACDF580>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'precision ', 'n_select': 10, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2025-10-29 10:18:09,606:INFO:Checking exceptions
2025-10-29 12:31:23,266:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-10-29 12:31:23,266:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-10-29 12:31:23,266:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-10-29 12:31:23,266:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-10-29 12:31:23,993:WARNING:
'prophet' is a soft dependency and not included in the pycaret installation. Please run: `pip install prophet` to install.
2025-10-29 12:31:29,083:INFO:PyCaret ClassificationExperiment
2025-10-29 12:31:29,083:INFO:Logging name: clf-default-name
2025-10-29 12:31:29,083:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-10-29 12:31:29,083:INFO:version 3.0.0
2025-10-29 12:31:29,083:INFO:Initializing setup()
2025-10-29 12:31:29,083:INFO:self.USI: cf3e
2025-10-29 12:31:29,084:INFO:self._variable_keys: {'pipeline', 'y_test', 'logging_param', 'html_param', 'exp_name_log', 'memory', 'seed', 'n_jobs_param', 'gpu_param', 'y_train', 'X', 'exp_id', 'fold_groups_param', 'X_test', 'fold_generator', 'gpu_n_jobs_param', 'log_plots_param', 'idx', 'X_train', 'target_param', 'y', 'fold_shuffle_param', '_available_plots', 'fix_imbalance', '_ml_usecase', 'is_multiclass', 'USI', 'data'}
2025-10-29 12:31:29,084:INFO:Checking environment
2025-10-29 12:31:29,084:INFO:python_version: 3.8.15
2025-10-29 12:31:29,084:INFO:python_build: ('default', 'Nov 24 2022 14:38:14')
2025-10-29 12:31:29,084:INFO:machine: AMD64
2025-10-29 12:31:29,084:INFO:platform: Windows-10-10.0.19045-SP0
2025-10-29 12:31:29,087:INFO:Memory: svmem(total=34299187200, available=18437189632, percent=46.2, used=15861997568, free=18437189632)
2025-10-29 12:31:29,087:INFO:Physical Core: 6
2025-10-29 12:31:29,087:INFO:Logical Core: 6
2025-10-29 12:31:29,087:INFO:Checking libraries
2025-10-29 12:31:29,087:INFO:System:
2025-10-29 12:31:29,087:INFO:    python: 3.8.15 (default, Nov 24 2022, 14:38:14) [MSC v.1916 64 bit (AMD64)]
2025-10-29 12:31:29,087:INFO:executable: c:\Users\hkh97\anaconda3\envs\han\python.exe
2025-10-29 12:31:29,087:INFO:   machine: Windows-10-10.0.19045-SP0
2025-10-29 12:31:29,087:INFO:PyCaret required dependencies:
2025-10-29 12:31:29,171:INFO:                 pip: 25.0.1
2025-10-29 12:31:29,172:INFO:          setuptools: 75.1.0
2025-10-29 12:31:29,172:INFO:             pycaret: 3.0.0
2025-10-29 12:31:29,172:INFO:             IPython: 8.12.3
2025-10-29 12:31:29,172:INFO:          ipywidgets: 8.1.7
2025-10-29 12:31:29,172:INFO:                tqdm: 4.67.1
2025-10-29 12:31:29,172:INFO:               numpy: 1.24.1
2025-10-29 12:31:29,172:INFO:              pandas: 1.5.3
2025-10-29 12:31:29,172:INFO:              jinja2: 3.1.4
2025-10-29 12:31:29,172:INFO:               scipy: 1.10.1
2025-10-29 12:31:29,172:INFO:              joblib: 1.2.0
2025-10-29 12:31:29,172:INFO:             sklearn: 1.2.2
2025-10-29 12:31:29,172:INFO:                pyod: 2.0.5
2025-10-29 12:31:29,172:INFO:            imblearn: 0.12.4
2025-10-29 12:31:29,172:INFO:   category_encoders: 2.6.4
2025-10-29 12:31:29,172:INFO:            lightgbm: 4.6.0
2025-10-29 12:31:29,172:INFO:               numba: 0.58.1
2025-10-29 12:31:29,172:INFO:            requests: 2.32.4
2025-10-29 12:31:29,172:INFO:          matplotlib: 3.6.0
2025-10-29 12:31:29,172:INFO:          scikitplot: 0.3.7
2025-10-29 12:31:29,173:INFO:         yellowbrick: 1.5
2025-10-29 12:31:29,173:INFO:              plotly: 6.3.0
2025-10-29 12:31:29,173:INFO:             kaleido: 1.1.0
2025-10-29 12:31:29,173:INFO:         statsmodels: 0.14.1
2025-10-29 12:31:29,173:INFO:              sktime: 0.21.1
2025-10-29 12:31:29,173:INFO:               tbats: 1.1.3
2025-10-29 12:31:29,173:INFO:            pmdarima: 2.0.4
2025-10-29 12:31:29,173:INFO:              psutil: 7.0.0
2025-10-29 12:31:29,173:INFO:PyCaret optional dependencies:
2025-10-29 12:31:29,213:INFO:                shap: Not installed
2025-10-29 12:31:29,213:INFO:           interpret: Not installed
2025-10-29 12:31:29,213:INFO:                umap: Not installed
2025-10-29 12:31:29,213:INFO:    pandas_profiling: Not installed
2025-10-29 12:31:29,213:INFO:  explainerdashboard: Not installed
2025-10-29 12:31:29,214:INFO:             autoviz: Not installed
2025-10-29 12:31:29,214:INFO:           fairlearn: Not installed
2025-10-29 12:31:29,214:INFO:             xgboost: 2.1.4
2025-10-29 12:31:29,214:INFO:            catboost: 1.2.8
2025-10-29 12:31:29,214:INFO:              kmodes: Not installed
2025-10-29 12:31:29,214:INFO:             mlxtend: Not installed
2025-10-29 12:31:29,214:INFO:       statsforecast: Not installed
2025-10-29 12:31:29,214:INFO:        tune_sklearn: Not installed
2025-10-29 12:31:29,214:INFO:                 ray: Not installed
2025-10-29 12:31:29,214:INFO:            hyperopt: Not installed
2025-10-29 12:31:29,214:INFO:              optuna: Not installed
2025-10-29 12:31:29,214:INFO:               skopt: Not installed
2025-10-29 12:31:29,214:INFO:              mlflow: Not installed
2025-10-29 12:31:29,214:INFO:              gradio: Not installed
2025-10-29 12:31:29,214:INFO:             fastapi: Not installed
2025-10-29 12:31:29,214:INFO:             uvicorn: Not installed
2025-10-29 12:31:29,214:INFO:              m2cgen: Not installed
2025-10-29 12:31:29,214:INFO:           evidently: Not installed
2025-10-29 12:31:29,215:INFO:               fugue: Not installed
2025-10-29 12:31:29,215:INFO:           streamlit: Not installed
2025-10-29 12:31:29,215:INFO:             prophet: Not installed
2025-10-29 12:31:29,215:INFO:None
2025-10-29 12:31:29,215:INFO:Set up data.
2025-10-29 12:31:29,219:INFO:Set up train/test split.
2025-10-29 12:31:29,224:INFO:Set up index.
2025-10-29 12:31:29,224:INFO:Set up folding strategy.
2025-10-29 12:31:29,224:INFO:Assigning column types.
2025-10-29 12:31:29,227:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-10-29 12:31:29,273:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-10-29 12:31:29,275:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-10-29 12:31:29,308:INFO:Soft dependency imported: xgboost: 2.1.4
2025-10-29 12:31:29,311:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-29 12:31:29,384:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-10-29 12:31:29,385:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-10-29 12:31:29,413:INFO:Soft dependency imported: xgboost: 2.1.4
2025-10-29 12:31:29,415:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-29 12:31:29,416:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-10-29 12:31:29,463:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-10-29 12:31:29,493:INFO:Soft dependency imported: xgboost: 2.1.4
2025-10-29 12:31:29,496:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-29 12:31:29,547:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-10-29 12:31:29,577:INFO:Soft dependency imported: xgboost: 2.1.4
2025-10-29 12:31:29,582:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-29 12:31:29,583:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-10-29 12:31:29,671:INFO:Soft dependency imported: xgboost: 2.1.4
2025-10-29 12:31:29,674:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-29 12:31:29,750:INFO:Soft dependency imported: xgboost: 2.1.4
2025-10-29 12:31:29,752:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-29 12:31:29,755:INFO:Preparing preprocessing pipeline...
2025-10-29 12:31:29,756:INFO:Set up simple imputation.
2025-10-29 12:31:29,756:INFO:Set up feature normalization.
2025-10-29 12:31:29,782:INFO:Finished creating preprocessing pipeline.
2025-10-29 12:31:29,788:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\hkh97\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Temperature', 'Pressure',
                                             'Vibration', 'GasFlow',
                                             'SteamOutput'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean',
                                                              verbose=...
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent',
                                                              verbose='deprecated'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2025-10-29 12:31:29,788:INFO:Creating final display dataframe.
2025-10-29 12:31:29,883:INFO:Setup _display_container:                     Description             Value
0                    Session id            441239
1                        Target           Failure
2                   Target type            Binary
3           Original data shape         (4368, 6)
4        Transformed data shape         (4368, 6)
5   Transformed train set shape         (3494, 6)
6    Transformed test set shape          (874, 6)
7              Numeric features                 5
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                    Normalize              True
13             Normalize method            zscore
14               Fold Generator   StratifiedKFold
15                  Fold Number                10
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              cf3e
2025-10-29 12:31:29,965:INFO:Soft dependency imported: xgboost: 2.1.4
2025-10-29 12:31:29,968:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-29 12:31:30,052:INFO:Soft dependency imported: xgboost: 2.1.4
2025-10-29 12:31:30,055:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-29 12:31:30,056:INFO:setup() successfully completed in 1.14s...............
2025-10-29 12:31:30,056:INFO:Initializing compare_models()
2025-10-29 12:31:30,056:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002342B2539D0>, include=None, fold=None, round=4, cross_validation=True, sort=, n_select=10, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000002342B2539D0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': '', 'n_select': 10, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2025-10-29 12:31:30,056:INFO:Checking exceptions
2025-10-29 12:31:37,934:INFO:PyCaret ClassificationExperiment
2025-10-29 12:31:37,934:INFO:Logging name: clf-default-name
2025-10-29 12:31:37,934:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-10-29 12:31:37,934:INFO:version 3.0.0
2025-10-29 12:31:37,934:INFO:Initializing setup()
2025-10-29 12:31:37,934:INFO:self.USI: 9da9
2025-10-29 12:31:37,934:INFO:self._variable_keys: {'pipeline', 'y_test', 'logging_param', 'html_param', 'exp_name_log', 'memory', 'seed', 'n_jobs_param', 'gpu_param', 'y_train', 'X', 'exp_id', 'fold_groups_param', 'X_test', 'fold_generator', 'gpu_n_jobs_param', 'log_plots_param', 'idx', 'X_train', 'target_param', 'y', 'fold_shuffle_param', '_available_plots', 'fix_imbalance', '_ml_usecase', 'is_multiclass', 'USI', 'data'}
2025-10-29 12:31:37,934:INFO:Checking environment
2025-10-29 12:31:37,934:INFO:python_version: 3.8.15
2025-10-29 12:31:37,934:INFO:python_build: ('default', 'Nov 24 2022 14:38:14')
2025-10-29 12:31:37,934:INFO:machine: AMD64
2025-10-29 12:31:37,935:INFO:platform: Windows-10-10.0.19045-SP0
2025-10-29 12:31:37,937:INFO:Memory: svmem(total=34299187200, available=18395996160, percent=46.4, used=15903191040, free=18395996160)
2025-10-29 12:31:37,937:INFO:Physical Core: 6
2025-10-29 12:31:37,937:INFO:Logical Core: 6
2025-10-29 12:31:37,937:INFO:Checking libraries
2025-10-29 12:31:37,937:INFO:System:
2025-10-29 12:31:37,937:INFO:    python: 3.8.15 (default, Nov 24 2022, 14:38:14) [MSC v.1916 64 bit (AMD64)]
2025-10-29 12:31:37,938:INFO:executable: c:\Users\hkh97\anaconda3\envs\han\python.exe
2025-10-29 12:31:37,938:INFO:   machine: Windows-10-10.0.19045-SP0
2025-10-29 12:31:37,938:INFO:PyCaret required dependencies:
2025-10-29 12:31:37,938:INFO:                 pip: 25.0.1
2025-10-29 12:31:37,938:INFO:          setuptools: 75.1.0
2025-10-29 12:31:37,938:INFO:             pycaret: 3.0.0
2025-10-29 12:31:37,938:INFO:             IPython: 8.12.3
2025-10-29 12:31:37,938:INFO:          ipywidgets: 8.1.7
2025-10-29 12:31:37,938:INFO:                tqdm: 4.67.1
2025-10-29 12:31:37,938:INFO:               numpy: 1.24.1
2025-10-29 12:31:37,938:INFO:              pandas: 1.5.3
2025-10-29 12:31:37,938:INFO:              jinja2: 3.1.4
2025-10-29 12:31:37,938:INFO:               scipy: 1.10.1
2025-10-29 12:31:37,938:INFO:              joblib: 1.2.0
2025-10-29 12:31:37,938:INFO:             sklearn: 1.2.2
2025-10-29 12:31:37,938:INFO:                pyod: 2.0.5
2025-10-29 12:31:37,938:INFO:            imblearn: 0.12.4
2025-10-29 12:31:37,939:INFO:   category_encoders: 2.6.4
2025-10-29 12:31:37,939:INFO:            lightgbm: 4.6.0
2025-10-29 12:31:37,939:INFO:               numba: 0.58.1
2025-10-29 12:31:37,939:INFO:            requests: 2.32.4
2025-10-29 12:31:37,939:INFO:          matplotlib: 3.6.0
2025-10-29 12:31:37,939:INFO:          scikitplot: 0.3.7
2025-10-29 12:31:37,939:INFO:         yellowbrick: 1.5
2025-10-29 12:31:37,939:INFO:              plotly: 6.3.0
2025-10-29 12:31:37,939:INFO:             kaleido: 1.1.0
2025-10-29 12:31:37,939:INFO:         statsmodels: 0.14.1
2025-10-29 12:31:37,939:INFO:              sktime: 0.21.1
2025-10-29 12:31:37,939:INFO:               tbats: 1.1.3
2025-10-29 12:31:37,939:INFO:            pmdarima: 2.0.4
2025-10-29 12:31:37,939:INFO:              psutil: 7.0.0
2025-10-29 12:31:37,939:INFO:PyCaret optional dependencies:
2025-10-29 12:31:37,939:INFO:                shap: Not installed
2025-10-29 12:31:37,939:INFO:           interpret: Not installed
2025-10-29 12:31:37,939:INFO:                umap: Not installed
2025-10-29 12:31:37,939:INFO:    pandas_profiling: Not installed
2025-10-29 12:31:37,940:INFO:  explainerdashboard: Not installed
2025-10-29 12:31:37,940:INFO:             autoviz: Not installed
2025-10-29 12:31:37,940:INFO:           fairlearn: Not installed
2025-10-29 12:31:37,940:INFO:             xgboost: 2.1.4
2025-10-29 12:31:37,940:INFO:            catboost: 1.2.8
2025-10-29 12:31:37,940:INFO:              kmodes: Not installed
2025-10-29 12:31:37,940:INFO:             mlxtend: Not installed
2025-10-29 12:31:37,940:INFO:       statsforecast: Not installed
2025-10-29 12:31:37,940:INFO:        tune_sklearn: Not installed
2025-10-29 12:31:37,940:INFO:                 ray: Not installed
2025-10-29 12:31:37,940:INFO:            hyperopt: Not installed
2025-10-29 12:31:37,940:INFO:              optuna: Not installed
2025-10-29 12:31:37,940:INFO:               skopt: Not installed
2025-10-29 12:31:37,940:INFO:              mlflow: Not installed
2025-10-29 12:31:37,940:INFO:              gradio: Not installed
2025-10-29 12:31:37,940:INFO:             fastapi: Not installed
2025-10-29 12:31:37,940:INFO:             uvicorn: Not installed
2025-10-29 12:31:37,940:INFO:              m2cgen: Not installed
2025-10-29 12:31:37,940:INFO:           evidently: Not installed
2025-10-29 12:31:37,940:INFO:               fugue: Not installed
2025-10-29 12:31:37,940:INFO:           streamlit: Not installed
2025-10-29 12:31:37,941:INFO:             prophet: Not installed
2025-10-29 12:31:37,941:INFO:None
2025-10-29 12:31:37,941:INFO:Set up data.
2025-10-29 12:31:37,945:INFO:Set up train/test split.
2025-10-29 12:31:37,949:INFO:Set up index.
2025-10-29 12:31:37,949:INFO:Set up folding strategy.
2025-10-29 12:31:37,949:INFO:Assigning column types.
2025-10-29 12:31:37,952:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-10-29 12:31:37,998:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-10-29 12:31:37,999:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-10-29 12:31:38,028:INFO:Soft dependency imported: xgboost: 2.1.4
2025-10-29 12:31:38,031:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-29 12:31:38,076:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-10-29 12:31:38,077:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-10-29 12:31:38,106:INFO:Soft dependency imported: xgboost: 2.1.4
2025-10-29 12:31:38,109:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-29 12:31:38,110:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-10-29 12:31:38,157:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-10-29 12:31:38,186:INFO:Soft dependency imported: xgboost: 2.1.4
2025-10-29 12:31:38,189:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-29 12:31:38,235:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-10-29 12:31:38,263:INFO:Soft dependency imported: xgboost: 2.1.4
2025-10-29 12:31:38,266:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-29 12:31:38,267:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-10-29 12:31:38,341:INFO:Soft dependency imported: xgboost: 2.1.4
2025-10-29 12:31:38,344:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-29 12:31:38,425:INFO:Soft dependency imported: xgboost: 2.1.4
2025-10-29 12:31:38,428:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-29 12:31:38,430:INFO:Preparing preprocessing pipeline...
2025-10-29 12:31:38,431:INFO:Set up simple imputation.
2025-10-29 12:31:38,431:INFO:Set up feature normalization.
2025-10-29 12:31:38,459:INFO:Finished creating preprocessing pipeline.
2025-10-29 12:31:38,464:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\hkh97\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Temperature', 'Pressure',
                                             'Vibration', 'GasFlow',
                                             'SteamOutput'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean',
                                                              verbose=...
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent',
                                                              verbose='deprecated'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2025-10-29 12:31:38,464:INFO:Creating final display dataframe.
2025-10-29 12:31:38,557:INFO:Setup _display_container:                     Description             Value
0                    Session id            441239
1                        Target           Failure
2                   Target type            Binary
3           Original data shape         (4368, 6)
4        Transformed data shape         (4368, 6)
5   Transformed train set shape         (3494, 6)
6    Transformed test set shape          (874, 6)
7              Numeric features                 5
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                    Normalize              True
13             Normalize method            zscore
14               Fold Generator   StratifiedKFold
15                  Fold Number                10
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              9da9
2025-10-29 12:31:38,636:INFO:Soft dependency imported: xgboost: 2.1.4
2025-10-29 12:31:38,639:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-29 12:31:38,717:INFO:Soft dependency imported: xgboost: 2.1.4
2025-10-29 12:31:38,720:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-29 12:31:38,721:INFO:setup() successfully completed in 0.94s...............
2025-10-29 12:31:38,721:INFO:Initializing compare_models()
2025-10-29 12:31:38,721:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002343087D550>, include=None, fold=None, round=4, cross_validation=True, sort=F1, n_select=10, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000002343087D550>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'F1', 'n_select': 10, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2025-10-29 12:31:38,721:INFO:Checking exceptions
2025-10-29 12:31:38,725:INFO:Preparing display monitor
2025-10-29 12:31:38,758:INFO:Initializing Logistic Regression
2025-10-29 12:31:38,759:INFO:Total runtime is 1.6637643178304036e-05 minutes
2025-10-29 12:31:38,764:INFO:SubProcess create_model() called ==================================
2025-10-29 12:31:38,765:INFO:Initializing create_model()
2025-10-29 12:31:38,765:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002343087D550>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023430C47970>, model_only=True, return_train_score=False, kwargs={})
2025-10-29 12:31:38,765:INFO:Checking exceptions
2025-10-29 12:31:38,765:INFO:Importing libraries
2025-10-29 12:31:38,766:INFO:Copying training dataset
2025-10-29 12:31:38,773:INFO:Defining folds
2025-10-29 12:31:38,774:INFO:Declaring metric variables
2025-10-29 12:31:38,781:INFO:Importing untrained model
2025-10-29 12:31:38,790:INFO:Logistic Regression Imported successfully
2025-10-29 12:31:38,800:INFO:Starting cross validation
2025-10-29 12:31:38,801:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 12:31:38,820:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 12:31:43,084:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:31:43,090:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:43,094:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:43,095:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:31:43,097:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:43,098:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:43,099:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:43,104:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:43,110:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:43,111:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:43,115:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:43,186:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:31:43,195:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:43,199:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:43,206:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:43,208:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:43,250:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:43,359:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:31:43,375:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:43,378:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:43,381:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:43,382:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:43,524:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:31:43,526:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:43,528:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:43,531:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:43,533:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:43,548:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:31:43,551:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:43,553:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:43,556:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:43,569:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:31:43,572:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:43,574:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:43,577:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:43,578:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:43,590:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:43,616:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:31:43,618:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:43,625:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:43,628:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:43,629:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:44,596:INFO:Calculating mean and std
2025-10-29 12:31:44,598:INFO:Creating metrics dataframe
2025-10-29 12:31:44,816:INFO:Uploading results into container
2025-10-29 12:31:44,816:INFO:Uploading model into container now
2025-10-29 12:31:44,817:INFO:_master_model_container: 1
2025-10-29 12:31:44,818:INFO:_display_container: 2
2025-10-29 12:31:44,819:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=441239, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-10-29 12:31:44,819:INFO:create_model() successfully completed......................................
2025-10-29 12:31:44,985:INFO:SubProcess create_model() end ==================================
2025-10-29 12:31:44,985:INFO:Creating metrics dataframe
2025-10-29 12:31:44,995:INFO:Initializing K Neighbors Classifier
2025-10-29 12:31:44,996:INFO:Total runtime is 0.1039554198582967 minutes
2025-10-29 12:31:45,000:INFO:SubProcess create_model() called ==================================
2025-10-29 12:31:45,000:INFO:Initializing create_model()
2025-10-29 12:31:45,000:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002343087D550>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023430C47970>, model_only=True, return_train_score=False, kwargs={})
2025-10-29 12:31:45,000:INFO:Checking exceptions
2025-10-29 12:31:45,000:INFO:Importing libraries
2025-10-29 12:31:45,000:INFO:Copying training dataset
2025-10-29 12:31:45,005:INFO:Defining folds
2025-10-29 12:31:45,005:INFO:Declaring metric variables
2025-10-29 12:31:45,009:INFO:Importing untrained model
2025-10-29 12:31:45,013:INFO:K Neighbors Classifier Imported successfully
2025-10-29 12:31:45,021:INFO:Starting cross validation
2025-10-29 12:31:45,023:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 12:31:45,025:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 12:31:45,173:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:31:45,175:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:45,178:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:45,181:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:45,187:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:45,207:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:45,230:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:31:45,230:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:31:45,233:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:31:45,234:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:45,236:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:45,237:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:45,239:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:45,239:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:45,241:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:45,241:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:45,242:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:45,243:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:45,261:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:45,263:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:45,265:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:45,266:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:45,569:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:31:45,571:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:45,573:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:45,576:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:45,577:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:45,587:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:31:45,589:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:45,591:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:45,594:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:45,595:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:45,611:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:31:45,615:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:45,618:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:45,621:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:45,622:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:31:45,622:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:45,624:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:45,626:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:45,629:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:45,630:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:46,746:INFO:Calculating mean and std
2025-10-29 12:31:46,748:INFO:Creating metrics dataframe
2025-10-29 12:31:46,976:INFO:Uploading results into container
2025-10-29 12:31:46,977:INFO:Uploading model into container now
2025-10-29 12:31:46,978:INFO:_master_model_container: 2
2025-10-29 12:31:46,978:INFO:_display_container: 2
2025-10-29 12:31:46,978:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-10-29 12:31:46,979:INFO:create_model() successfully completed......................................
2025-10-29 12:31:47,138:INFO:SubProcess create_model() end ==================================
2025-10-29 12:31:47,138:INFO:Creating metrics dataframe
2025-10-29 12:31:47,150:INFO:Initializing Naive Bayes
2025-10-29 12:31:47,150:INFO:Total runtime is 0.13985942204793295 minutes
2025-10-29 12:31:47,154:INFO:SubProcess create_model() called ==================================
2025-10-29 12:31:47,154:INFO:Initializing create_model()
2025-10-29 12:31:47,154:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002343087D550>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023430C47970>, model_only=True, return_train_score=False, kwargs={})
2025-10-29 12:31:47,154:INFO:Checking exceptions
2025-10-29 12:31:47,155:INFO:Importing libraries
2025-10-29 12:31:47,155:INFO:Copying training dataset
2025-10-29 12:31:47,160:INFO:Defining folds
2025-10-29 12:31:47,160:INFO:Declaring metric variables
2025-10-29 12:31:47,164:INFO:Importing untrained model
2025-10-29 12:31:47,169:INFO:Naive Bayes Imported successfully
2025-10-29 12:31:47,177:INFO:Starting cross validation
2025-10-29 12:31:47,178:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 12:31:47,180:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 12:31:47,283:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:31:47,285:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:47,288:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:47,291:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:47,291:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:47,293:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:31:47,296:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:47,299:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:47,302:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:47,303:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:47,306:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:47,307:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:47,310:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:31:47,317:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:47,319:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:47,323:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:47,324:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:31:47,324:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:47,326:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:47,330:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:47,332:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:47,333:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:47,663:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:31:47,665:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:47,667:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:47,670:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:47,671:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:47,674:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:31:47,676:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:47,679:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:47,681:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:47,681:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:31:47,681:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:47,683:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:31:47,684:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:47,685:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:47,686:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:47,687:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:47,688:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:47,689:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:47,690:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:47,691:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:48,828:INFO:Calculating mean and std
2025-10-29 12:31:48,829:INFO:Creating metrics dataframe
2025-10-29 12:31:49,044:INFO:Uploading results into container
2025-10-29 12:31:49,045:INFO:Uploading model into container now
2025-10-29 12:31:49,046:INFO:_master_model_container: 3
2025-10-29 12:31:49,046:INFO:_display_container: 2
2025-10-29 12:31:49,046:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-10-29 12:31:49,046:INFO:create_model() successfully completed......................................
2025-10-29 12:31:49,204:INFO:SubProcess create_model() end ==================================
2025-10-29 12:31:49,204:INFO:Creating metrics dataframe
2025-10-29 12:31:49,218:INFO:Initializing Decision Tree Classifier
2025-10-29 12:31:49,219:INFO:Total runtime is 0.17435055176417033 minutes
2025-10-29 12:31:49,223:INFO:SubProcess create_model() called ==================================
2025-10-29 12:31:49,223:INFO:Initializing create_model()
2025-10-29 12:31:49,224:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002343087D550>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023430C47970>, model_only=True, return_train_score=False, kwargs={})
2025-10-29 12:31:49,224:INFO:Checking exceptions
2025-10-29 12:31:49,224:INFO:Importing libraries
2025-10-29 12:31:49,225:INFO:Copying training dataset
2025-10-29 12:31:49,231:INFO:Defining folds
2025-10-29 12:31:49,231:INFO:Declaring metric variables
2025-10-29 12:31:49,237:INFO:Importing untrained model
2025-10-29 12:31:49,242:INFO:Decision Tree Classifier Imported successfully
2025-10-29 12:31:49,250:INFO:Starting cross validation
2025-10-29 12:31:49,252:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 12:31:49,255:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 12:31:49,377:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:31:49,378:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:31:49,381:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:49,381:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:49,384:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:49,387:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:49,388:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:49,427:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:31:49,431:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:49,433:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:49,437:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:49,438:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:49,476:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:49,485:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:31:49,487:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:49,491:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:49,493:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:49,494:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:49,762:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:31:49,764:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:49,768:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:49,770:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:31:49,771:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:49,772:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:49,772:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:49,775:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:49,778:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:49,778:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:31:49,779:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:49,780:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:49,784:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:49,786:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:49,789:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:49,798:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:31:49,800:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:49,804:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:49,807:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:49,810:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:50,939:INFO:Calculating mean and std
2025-10-29 12:31:50,941:INFO:Creating metrics dataframe
2025-10-29 12:31:51,185:INFO:Uploading results into container
2025-10-29 12:31:51,186:INFO:Uploading model into container now
2025-10-29 12:31:51,186:INFO:_master_model_container: 4
2025-10-29 12:31:51,186:INFO:_display_container: 2
2025-10-29 12:31:51,187:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=441239, splitter='best')
2025-10-29 12:31:51,187:INFO:create_model() successfully completed......................................
2025-10-29 12:31:51,354:INFO:SubProcess create_model() end ==================================
2025-10-29 12:31:51,354:INFO:Creating metrics dataframe
2025-10-29 12:31:51,366:INFO:Initializing SVM - Linear Kernel
2025-10-29 12:31:51,366:INFO:Total runtime is 0.21012158393859864 minutes
2025-10-29 12:31:51,370:INFO:SubProcess create_model() called ==================================
2025-10-29 12:31:51,370:INFO:Initializing create_model()
2025-10-29 12:31:51,370:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002343087D550>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023430C47970>, model_only=True, return_train_score=False, kwargs={})
2025-10-29 12:31:51,370:INFO:Checking exceptions
2025-10-29 12:31:51,370:INFO:Importing libraries
2025-10-29 12:31:51,371:INFO:Copying training dataset
2025-10-29 12:31:51,376:INFO:Defining folds
2025-10-29 12:31:51,376:INFO:Declaring metric variables
2025-10-29 12:31:51,382:INFO:Importing untrained model
2025-10-29 12:31:51,386:INFO:SVM - Linear Kernel Imported successfully
2025-10-29 12:31:51,395:INFO:Starting cross validation
2025-10-29 12:31:51,396:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 12:31:51,399:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 12:31:51,512:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 12:31:51,514:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:51,515:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 12:31:51,516:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 12:31:51,517:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:51,518:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:51,520:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:51,520:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:51,521:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:51,526:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:51,527:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:51,536:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 12:31:51,536:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 12:31:51,540:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:51,542:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:51,545:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:51,546:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

d and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 12:31:51,556:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:51,559:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:51,562:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:51,563:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:51,814:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 12:31:51,818:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:51,820:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:51,823:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:51,824:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:51,825:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 12:31:51,827:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:51,830:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:51,832:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:51,833:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:51,852:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 12:31:51,854:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:51,856:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:51,859:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:51,860:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:51,867:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 12:31:51,869:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:51,871:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:51,874:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:51,875:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:53,035:INFO:Calculating mean and std
2025-10-29 12:31:53,036:INFO:Creating metrics dataframe
2025-10-29 12:31:53,272:INFO:Uploading results into container
2025-10-29 12:31:53,273:INFO:Uploading model into container now
2025-10-29 12:31:53,274:INFO:_master_model_container: 5
2025-10-29 12:31:53,274:INFO:_display_container: 2
2025-10-29 12:31:53,275:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=441239, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2025-10-29 12:31:53,276:INFO:create_model() successfully completed......................................
2025-10-29 12:31:53,440:INFO:SubProcess create_model() end ==================================
2025-10-29 12:31:53,441:INFO:Creating metrics dataframe
2025-10-29 12:31:53,452:INFO:Initializing Ridge Classifier
2025-10-29 12:31:53,452:INFO:Total runtime is 0.24489528735478722 minutes
2025-10-29 12:31:53,457:INFO:SubProcess create_model() called ==================================
2025-10-29 12:31:53,458:INFO:Initializing create_model()
2025-10-29 12:31:53,458:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002343087D550>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023430C47970>, model_only=True, return_train_score=False, kwargs={})
2025-10-29 12:31:53,458:INFO:Checking exceptions
2025-10-29 12:31:53,458:INFO:Importing libraries
2025-10-29 12:31:53,458:INFO:Copying training dataset
2025-10-29 12:31:53,462:INFO:Defining folds
2025-10-29 12:31:53,463:INFO:Declaring metric variables
2025-10-29 12:31:53,467:INFO:Importing untrained model
2025-10-29 12:31:53,472:INFO:Ridge Classifier Imported successfully
2025-10-29 12:31:53,481:INFO:Starting cross validation
2025-10-29 12:31:53,483:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 12:31:53,485:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 12:31:53,567:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2025-10-29 12:31:53,570:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2025-10-29 12:31:53,570:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:53,572:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:53,572:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:53,575:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:53,575:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:53,579:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:53,580:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:53,580:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:53,592:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2025-10-29 12:31:53,597:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:53,600:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:53,605:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:53,606:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:53,611:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2025-10-29 12:31:53,615:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:53,624:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2025-10-29 12:31:53,630:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:53,636:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2025-10-29 12:31:53,638:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:53,640:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:53,643:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:53,645:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:53,904:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2025-10-29 12:31:53,908:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:53,910:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:53,914:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:53,915:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:53,921:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2025-10-29 12:31:53,924:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:53,927:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:53,929:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:53,931:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:53,948:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2025-10-29 12:31:53,951:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:53,953:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:53,956:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:53,957:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:53,966:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2025-10-29 12:31:53,968:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:53,973:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:53,976:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:53,978:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:55,097:INFO:Calculating mean and std
2025-10-29 12:31:55,098:INFO:Creating metrics dataframe
2025-10-29 12:31:55,322:INFO:Uploading results into container
2025-10-29 12:31:55,322:INFO:Uploading model into container now
2025-10-29 12:31:55,323:INFO:_master_model_container: 6
2025-10-29 12:31:55,324:INFO:_display_container: 2
2025-10-29 12:31:55,324:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=441239,
                solver='auto', tol=0.0001)
2025-10-29 12:31:55,324:INFO:create_model() successfully completed......................................
2025-10-29 12:31:55,481:INFO:SubProcess create_model() end ==================================
2025-10-29 12:31:55,481:INFO:Creating metrics dataframe
2025-10-29 12:31:55,493:INFO:Initializing Random Forest Classifier
2025-10-29 12:31:55,493:INFO:Total runtime is 0.27892100016276045 minutes
2025-10-29 12:31:55,498:INFO:SubProcess create_model() called ==================================
2025-10-29 12:31:55,498:INFO:Initializing create_model()
2025-10-29 12:31:55,499:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002343087D550>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023430C47970>, model_only=True, return_train_score=False, kwargs={})
2025-10-29 12:31:55,499:INFO:Checking exceptions
2025-10-29 12:31:55,499:INFO:Importing libraries
2025-10-29 12:31:55,499:INFO:Copying training dataset
2025-10-29 12:31:55,505:INFO:Defining folds
2025-10-29 12:31:55,505:INFO:Declaring metric variables
2025-10-29 12:31:55,509:INFO:Importing untrained model
2025-10-29 12:31:55,515:INFO:Random Forest Classifier Imported successfully
2025-10-29 12:31:55,523:INFO:Starting cross validation
2025-10-29 12:31:55,524:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 12:31:55,527:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 12:31:55,893:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:31:55,895:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:55,898:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:55,899:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:55,900:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:55,901:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:55,924:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:55,941:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:31:55,946:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:55,949:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:55,952:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:55,953:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:55,980:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:31:55,980:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:31:55,982:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:55,983:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:55,985:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:55,985:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:55,987:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:55,987:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:55,988:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:55,989:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:56,464:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:31:56,466:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:56,469:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:56,471:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:56,472:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:56,499:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:31:56,501:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:56,503:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:56,506:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:56,507:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:56,525:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:31:56,527:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:56,530:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:56,532:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:56,533:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:56,542:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:31:56,544:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:56,548:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:56,550:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:56,551:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:57,530:INFO:Calculating mean and std
2025-10-29 12:31:57,532:INFO:Creating metrics dataframe
2025-10-29 12:31:57,764:INFO:Uploading results into container
2025-10-29 12:31:57,765:INFO:Uploading model into container now
2025-10-29 12:31:57,765:INFO:_master_model_container: 7
2025-10-29 12:31:57,766:INFO:_display_container: 2
2025-10-29 12:31:57,766:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=441239, verbose=0, warm_start=False)
2025-10-29 12:31:57,766:INFO:create_model() successfully completed......................................
2025-10-29 12:31:57,952:INFO:SubProcess create_model() end ==================================
2025-10-29 12:31:57,952:INFO:Creating metrics dataframe
2025-10-29 12:31:57,970:INFO:Initializing Quadratic Discriminant Analysis
2025-10-29 12:31:57,970:INFO:Total runtime is 0.32019398212432865 minutes
2025-10-29 12:31:57,974:INFO:SubProcess create_model() called ==================================
2025-10-29 12:31:57,975:INFO:Initializing create_model()
2025-10-29 12:31:57,975:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002343087D550>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023430C47970>, model_only=True, return_train_score=False, kwargs={})
2025-10-29 12:31:57,975:INFO:Checking exceptions
2025-10-29 12:31:57,975:INFO:Importing libraries
2025-10-29 12:31:57,975:INFO:Copying training dataset
2025-10-29 12:31:57,984:INFO:Defining folds
2025-10-29 12:31:57,984:INFO:Declaring metric variables
2025-10-29 12:31:57,989:INFO:Importing untrained model
2025-10-29 12:31:57,993:INFO:Quadratic Discriminant Analysis Imported successfully
2025-10-29 12:31:58,002:INFO:Starting cross validation
2025-10-29 12:31:58,003:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 12:31:58,005:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 12:31:58,092:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-10-29 12:31:58,139:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-10-29 12:31:58,148:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:31:58,151:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:58,154:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:58,156:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:58,158:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:58,159:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-10-29 12:31:58,166:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-10-29 12:31:58,203:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:31:58,206:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:58,209:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:58,211:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:58,214:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:58,239:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:31:58,242:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:58,245:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:58,248:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:58,250:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:58,287:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:31:58,290:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:58,293:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:58,296:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:58,298:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:58,526:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-10-29 12:31:58,529:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-10-29 12:31:58,543:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-10-29 12:31:58,566:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-10-29 12:31:58,571:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:31:58,573:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:58,576:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:31:58,576:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:58,578:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:58,578:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:58,579:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:58,581:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:58,583:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:58,584:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:58,598:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:31:58,600:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:58,604:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:31:58,604:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:58,606:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:58,606:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:58,608:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:58,610:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:31:58,613:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:31:58,614:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:31:59,703:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_validation.py:378: FitFailedWarning: 
2 fits failed out of a total of 10.
The score on these train-test partitions for these parameters will be set to 0.
If these failures are not expected, you can try to debug them by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
2 fits failed with the following error:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\discriminant_analysis.py", line 917, in fit
    raise ValueError(
ValueError: y has only 1 sample in class 1, covariance is ill defined.

  warnings.warn(some_fits_failed_message, FitFailedWarning)

2025-10-29 12:31:59,703:INFO:Calculating mean and std
2025-10-29 12:31:59,705:INFO:Creating metrics dataframe
2025-10-29 12:31:59,940:INFO:Uploading results into container
2025-10-29 12:31:59,940:INFO:Uploading model into container now
2025-10-29 12:31:59,941:INFO:_master_model_container: 8
2025-10-29 12:31:59,941:INFO:_display_container: 2
2025-10-29 12:31:59,941:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2025-10-29 12:31:59,941:INFO:create_model() successfully completed......................................
2025-10-29 12:32:00,105:INFO:SubProcess create_model() end ==================================
2025-10-29 12:32:00,105:INFO:Creating metrics dataframe
2025-10-29 12:32:00,118:INFO:Initializing Ada Boost Classifier
2025-10-29 12:32:00,118:INFO:Total runtime is 0.3559982538223267 minutes
2025-10-29 12:32:00,122:INFO:SubProcess create_model() called ==================================
2025-10-29 12:32:00,122:INFO:Initializing create_model()
2025-10-29 12:32:00,123:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002343087D550>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023430C47970>, model_only=True, return_train_score=False, kwargs={})
2025-10-29 12:32:00,123:INFO:Checking exceptions
2025-10-29 12:32:00,123:INFO:Importing libraries
2025-10-29 12:32:00,123:INFO:Copying training dataset
2025-10-29 12:32:00,127:INFO:Defining folds
2025-10-29 12:32:00,128:INFO:Declaring metric variables
2025-10-29 12:32:00,131:INFO:Importing untrained model
2025-10-29 12:32:00,136:INFO:Ada Boost Classifier Imported successfully
2025-10-29 12:32:00,144:INFO:Starting cross validation
2025-10-29 12:32:00,145:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 12:32:00,147:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 12:32:00,261:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:00,262:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:00,264:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:00,265:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:00,267:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:00,270:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:00,271:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:00,291:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:00,295:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

lt))

2025-10-29 12:32:00,301:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:00,309:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:00,312:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:00,385:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:00,388:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:00,391:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:00,399:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:00,402:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:00,678:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:00,682:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:00,686:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:00,689:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:00,690:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:00,723:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:00,725:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:00,728:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:00,730:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:00,731:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:00,744:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:00,746:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:00,747:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:00,749:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:00,750:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:00,752:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:00,752:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:00,753:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:00,755:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:00,756:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:01,819:INFO:Calculating mean and std
2025-10-29 12:32:01,820:INFO:Creating metrics dataframe
2025-10-29 12:32:02,048:INFO:Uploading results into container
2025-10-29 12:32:02,049:INFO:Uploading model into container now
2025-10-29 12:32:02,049:INFO:_master_model_container: 9
2025-10-29 12:32:02,049:INFO:_display_container: 2
2025-10-29 12:32:02,049:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=441239)
2025-10-29 12:32:02,049:INFO:create_model() successfully completed......................................
2025-10-29 12:32:02,210:INFO:SubProcess create_model() end ==================================
2025-10-29 12:32:02,210:INFO:Creating metrics dataframe
2025-10-29 12:32:02,227:INFO:Initializing Gradient Boosting Classifier
2025-10-29 12:32:02,227:INFO:Total runtime is 0.3911542852719625 minutes
2025-10-29 12:32:02,231:INFO:SubProcess create_model() called ==================================
2025-10-29 12:32:02,231:INFO:Initializing create_model()
2025-10-29 12:32:02,231:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002343087D550>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023430C47970>, model_only=True, return_train_score=False, kwargs={})
2025-10-29 12:32:02,231:INFO:Checking exceptions
2025-10-29 12:32:02,231:INFO:Importing libraries
2025-10-29 12:32:02,231:INFO:Copying training dataset
2025-10-29 12:32:02,237:INFO:Defining folds
2025-10-29 12:32:02,237:INFO:Declaring metric variables
2025-10-29 12:32:02,241:INFO:Importing untrained model
2025-10-29 12:32:02,246:INFO:Gradient Boosting Classifier Imported successfully
2025-10-29 12:32:02,253:INFO:Starting cross validation
2025-10-29 12:32:02,255:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 12:32:02,257:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 12:32:02,510:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:02,513:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:02,517:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:02,521:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:02,522:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:02,529:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:02,532:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:02,553:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:02,556:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:02,559:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:02,561:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:02,563:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:02,517:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:02,664:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:02,667:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:02,669:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:02,673:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:02,674:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:02,972:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:02,975:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:02,978:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:02,981:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:02,982:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:03,086:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:03,089:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:03,091:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:03,093:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:03,095:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:03,099:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:03,102:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:03,104:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:03,106:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:03,107:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:03,128:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:03,131:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:03,134:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:03,136:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:03,137:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:04,110:INFO:Calculating mean and std
2025-10-29 12:32:04,111:INFO:Creating metrics dataframe
2025-10-29 12:32:04,339:INFO:Uploading results into container
2025-10-29 12:32:04,340:INFO:Uploading model into container now
2025-10-29 12:32:04,341:INFO:_master_model_container: 10
2025-10-29 12:32:04,341:INFO:_display_container: 2
2025-10-29 12:32:04,342:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=441239, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-10-29 12:32:04,342:INFO:create_model() successfully completed......................................
2025-10-29 12:32:04,515:INFO:SubProcess create_model() end ==================================
2025-10-29 12:32:04,515:INFO:Creating metrics dataframe
2025-10-29 12:32:04,528:INFO:Initializing Linear Discriminant Analysis
2025-10-29 12:32:04,528:INFO:Total runtime is 0.429501744111379 minutes
2025-10-29 12:32:04,531:INFO:SubProcess create_model() called ==================================
2025-10-29 12:32:04,531:INFO:Initializing create_model()
2025-10-29 12:32:04,531:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002343087D550>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023430C47970>, model_only=True, return_train_score=False, kwargs={})
2025-10-29 12:32:04,531:INFO:Checking exceptions
2025-10-29 12:32:04,532:INFO:Importing libraries
2025-10-29 12:32:04,532:INFO:Copying training dataset
2025-10-29 12:32:04,538:INFO:Defining folds
2025-10-29 12:32:04,538:INFO:Declaring metric variables
2025-10-29 12:32:04,543:INFO:Importing untrained model
2025-10-29 12:32:04,549:INFO:Linear Discriminant Analysis Imported successfully
2025-10-29 12:32:04,557:INFO:Starting cross validation
2025-10-29 12:32:04,559:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 12:32:04,562:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 12:32:04,697:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:04,700:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:04,702:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:04,704:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:04,705:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:04,715:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:04,718:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:04,721:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:04,724:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:04,726:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:04,749:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:04,752:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:04,754:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

ns that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:04,756:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:04,757:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:04,757:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:04,759:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:04,761:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:04,762:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:05,086:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:05,090:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:05,093:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:05,098:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:05,100:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:05,103:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:05,106:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:05,108:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:05,110:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:05,111:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:05,112:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:05,113:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:05,113:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:05,114:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:05,114:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:05,117:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:05,117:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:05,118:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:05,120:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:05,121:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:06,267:INFO:Calculating mean and std
2025-10-29 12:32:06,268:INFO:Creating metrics dataframe
2025-10-29 12:32:06,504:INFO:Uploading results into container
2025-10-29 12:32:06,504:INFO:Uploading model into container now
2025-10-29 12:32:06,506:INFO:_master_model_container: 11
2025-10-29 12:32:06,506:INFO:_display_container: 2
2025-10-29 12:32:06,507:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2025-10-29 12:32:06,507:INFO:create_model() successfully completed......................................
2025-10-29 12:32:06,665:INFO:SubProcess create_model() end ==================================
2025-10-29 12:32:06,666:INFO:Creating metrics dataframe
2025-10-29 12:32:06,680:INFO:Initializing Extra Trees Classifier
2025-10-29 12:32:06,680:INFO:Total runtime is 0.46535637776056926 minutes
2025-10-29 12:32:06,684:INFO:SubProcess create_model() called ==================================
2025-10-29 12:32:06,684:INFO:Initializing create_model()
2025-10-29 12:32:06,684:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002343087D550>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023430C47970>, model_only=True, return_train_score=False, kwargs={})
2025-10-29 12:32:06,684:INFO:Checking exceptions
2025-10-29 12:32:06,684:INFO:Importing libraries
2025-10-29 12:32:06,684:INFO:Copying training dataset
2025-10-29 12:32:06,689:INFO:Defining folds
2025-10-29 12:32:06,689:INFO:Declaring metric variables
2025-10-29 12:32:06,694:INFO:Importing untrained model
2025-10-29 12:32:06,698:INFO:Extra Trees Classifier Imported successfully
2025-10-29 12:32:06,706:INFO:Starting cross validation
2025-10-29 12:32:06,706:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 12:32:06,708:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 12:32:07,041:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:07,051:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:07,053:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:07,056:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:07,059:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:07,059:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:07,083:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:07,084:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:07,084:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:07,085:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:07,086:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:07,086:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:07,087:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:07,088:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:07,089:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:07,090:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:07,091:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:07,091:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:07,092:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:07,092:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:07,093:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:07,163:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:07,563:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:07,565:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:07,567:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:07,570:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:07,571:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:07,612:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:07,614:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:07,616:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:07,618:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:07,619:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:07,620:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:07,622:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:07,623:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:07,625:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:07,626:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:07,627:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:07,628:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:07,629:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:07,631:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:07,632:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:08,664:INFO:Calculating mean and std
2025-10-29 12:32:08,666:INFO:Creating metrics dataframe
2025-10-29 12:32:08,902:INFO:Uploading results into container
2025-10-29 12:32:08,902:INFO:Uploading model into container now
2025-10-29 12:32:08,903:INFO:_master_model_container: 12
2025-10-29 12:32:08,903:INFO:_display_container: 2
2025-10-29 12:32:08,904:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=441239, verbose=0, warm_start=False)
2025-10-29 12:32:08,904:INFO:create_model() successfully completed......................................
2025-10-29 12:32:09,070:INFO:SubProcess create_model() end ==================================
2025-10-29 12:32:09,070:INFO:Creating metrics dataframe
2025-10-29 12:32:09,084:INFO:Initializing Extreme Gradient Boosting
2025-10-29 12:32:09,084:INFO:Total runtime is 0.5054292877515157 minutes
2025-10-29 12:32:09,088:INFO:SubProcess create_model() called ==================================
2025-10-29 12:32:09,088:INFO:Initializing create_model()
2025-10-29 12:32:09,088:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002343087D550>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023430C47970>, model_only=True, return_train_score=False, kwargs={})
2025-10-29 12:32:09,088:INFO:Checking exceptions
2025-10-29 12:32:09,088:INFO:Importing libraries
2025-10-29 12:32:09,089:INFO:Copying training dataset
2025-10-29 12:32:09,094:INFO:Defining folds
2025-10-29 12:32:09,094:INFO:Declaring metric variables
2025-10-29 12:32:09,098:INFO:Importing untrained model
2025-10-29 12:32:09,103:INFO:Extreme Gradient Boosting Imported successfully
2025-10-29 12:32:09,111:INFO:Starting cross validation
2025-10-29 12:32:09,112:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 12:32:09,113:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 12:32:09,673:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:09,884:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:09,916:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:09,918:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:09,918:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:09,920:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:09,920:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:09,921:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:09,922:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:09,922:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:09,923:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:09,923:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:09,924:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:09,925:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:09,925:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:09,927:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:09,928:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:09,956:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:09,959:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:09,961:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:09,963:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:09,965:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:10,136:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:10,139:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:10,141:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:10,150:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:10,152:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:10,244:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:10,246:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:10,248:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:10,251:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:10,252:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:10,261:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:10,264:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:10,266:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:10,268:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:10,270:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:10,275:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:10,278:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:10,280:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:10,282:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:10,285:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:11,205:INFO:Calculating mean and std
2025-10-29 12:32:11,206:INFO:Creating metrics dataframe
2025-10-29 12:32:11,438:INFO:Uploading results into container
2025-10-29 12:32:11,439:INFO:Uploading model into container now
2025-10-29 12:32:11,439:INFO:_master_model_container: 13
2025-10-29 12:32:11,439:INFO:_display_container: 2
2025-10-29 12:32:11,440:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device=None, early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2025-10-29 12:32:11,440:INFO:create_model() successfully completed......................................
2025-10-29 12:32:11,596:INFO:SubProcess create_model() end ==================================
2025-10-29 12:32:11,596:INFO:Creating metrics dataframe
2025-10-29 12:32:11,611:INFO:Initializing Light Gradient Boosting Machine
2025-10-29 12:32:11,611:INFO:Total runtime is 0.547538161277771 minutes
2025-10-29 12:32:11,614:INFO:SubProcess create_model() called ==================================
2025-10-29 12:32:11,614:INFO:Initializing create_model()
2025-10-29 12:32:11,614:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002343087D550>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023430C47970>, model_only=True, return_train_score=False, kwargs={})
2025-10-29 12:32:11,614:INFO:Checking exceptions
2025-10-29 12:32:11,614:INFO:Importing libraries
2025-10-29 12:32:11,614:INFO:Copying training dataset
2025-10-29 12:32:11,620:INFO:Defining folds
2025-10-29 12:32:11,620:INFO:Declaring metric variables
2025-10-29 12:32:11,625:INFO:Importing untrained model
2025-10-29 12:32:11,629:INFO:Light Gradient Boosting Machine Imported successfully
2025-10-29 12:32:11,636:INFO:Starting cross validation
2025-10-29 12:32:11,638:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 12:32:11,639:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 12:32:11,774:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:11,774:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:11,775:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:11,776:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:11,776:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:11,777:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:11,778:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:11,779:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:11,779:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:11,780:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:11,780:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:11,781:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:11,781:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:11,781:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:11,782:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:11,819:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:11,857:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:11,859:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:11,862:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:11,864:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:11,865:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:12,141:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:12,141:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:12,143:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:12,143:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:12,146:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:12,146:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:12,148:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:12,148:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:12,149:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:12,149:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:12,154:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:12,157:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:12,160:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:12,163:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:12,164:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:12,174:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:12,176:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:13,275:INFO:Calculating mean and std
2025-10-29 12:32:13,277:INFO:Creating metrics dataframe
2025-10-29 12:32:13,614:INFO:Uploading results into container
2025-10-29 12:32:13,615:INFO:Uploading model into container now
2025-10-29 12:32:13,616:INFO:_master_model_container: 14
2025-10-29 12:32:13,616:INFO:_display_container: 2
2025-10-29 12:32:13,618:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=441239, reg_alpha=0.0, reg_lambda=0.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2025-10-29 12:32:13,618:INFO:create_model() successfully completed......................................
2025-10-29 12:32:13,799:INFO:SubProcess create_model() end ==================================
2025-10-29 12:32:13,800:INFO:Creating metrics dataframe
2025-10-29 12:32:13,817:INFO:Initializing CatBoost Classifier
2025-10-29 12:32:13,817:INFO:Total runtime is 0.584306530157725 minutes
2025-10-29 12:32:13,823:INFO:SubProcess create_model() called ==================================
2025-10-29 12:32:13,823:INFO:Initializing create_model()
2025-10-29 12:32:13,823:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002343087D550>, estimator=catboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023430C47970>, model_only=True, return_train_score=False, kwargs={})
2025-10-29 12:32:13,823:INFO:Checking exceptions
2025-10-29 12:32:13,823:INFO:Importing libraries
2025-10-29 12:32:13,824:INFO:Copying training dataset
2025-10-29 12:32:13,829:INFO:Defining folds
2025-10-29 12:32:13,829:INFO:Declaring metric variables
2025-10-29 12:32:13,834:INFO:Importing untrained model
2025-10-29 12:32:13,839:INFO:CatBoost Classifier Imported successfully
2025-10-29 12:32:13,849:INFO:Starting cross validation
2025-10-29 12:32:13,849:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 12:32:13,851:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 12:32:15,075:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:15,075:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:15,078:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:15,078:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:15,080:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:15,080:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:15,084:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:15,085:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:15,086:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:15,087:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:15,166:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:15,203:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:15,206:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:15,208:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:15,210:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:15,211:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:15,247:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:15,249:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:15,251:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:15,253:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:15,254:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:15,438:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:15,440:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:15,442:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:15,446:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:15,447:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:15,452:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:15,455:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:15,457:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:15,460:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:15,461:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:15,462:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:15,463:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:15,465:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:15,467:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:15,468:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:15,473:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:15,475:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:15,479:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:15,481:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:15,482:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:16,482:INFO:Calculating mean and std
2025-10-29 12:32:16,483:INFO:Creating metrics dataframe
2025-10-29 12:32:16,699:INFO:Uploading results into container
2025-10-29 12:32:16,699:INFO:Uploading model into container now
2025-10-29 12:32:16,700:INFO:_master_model_container: 15
2025-10-29 12:32:16,700:INFO:_display_container: 2
2025-10-29 12:32:16,701:INFO:<catboost.core.CatBoostClassifier object at 0x0000023430A7EDC0>
2025-10-29 12:32:16,701:INFO:create_model() successfully completed......................................
2025-10-29 12:32:16,856:INFO:SubProcess create_model() end ==================================
2025-10-29 12:32:16,856:INFO:Creating metrics dataframe
2025-10-29 12:32:16,869:INFO:Initializing Dummy Classifier
2025-10-29 12:32:16,869:INFO:Total runtime is 0.6351871569951376 minutes
2025-10-29 12:32:16,873:INFO:SubProcess create_model() called ==================================
2025-10-29 12:32:16,873:INFO:Initializing create_model()
2025-10-29 12:32:16,873:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002343087D550>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023430C47970>, model_only=True, return_train_score=False, kwargs={})
2025-10-29 12:32:16,873:INFO:Checking exceptions
2025-10-29 12:32:16,873:INFO:Importing libraries
2025-10-29 12:32:16,873:INFO:Copying training dataset
2025-10-29 12:32:16,879:INFO:Defining folds
2025-10-29 12:32:16,879:INFO:Declaring metric variables
2025-10-29 12:32:16,884:INFO:Importing untrained model
2025-10-29 12:32:16,888:INFO:Dummy Classifier Imported successfully
2025-10-29 12:32:16,896:INFO:Starting cross validation
2025-10-29 12:32:16,897:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 12:32:16,899:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 12:32:17,012:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:17,015:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:17,018:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:17,022:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:17,022:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:17,024:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:17,024:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:17,026:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:17,026:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:17,027:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:17,029:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:17,029:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:17,030:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:17,031:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:17,035:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:17,040:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:17,043:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:17,045:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:17,048:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:17,050:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:17,050:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:17,056:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:17,374:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:17,376:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:17,378:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:17,380:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:17,382:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:17,431:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:17,433:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

ally means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:17,435:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:17,435:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:17,435:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 572, in roc_auc_score
    return _average_binary_score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_ranking.py", line 339, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

  warnings.warn(

2025-10-29 12:32:17,437:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:17,437:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:17,438:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:17,438:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:17,439:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:17,440:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:17,440:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:17,442:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:17,443:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:18,518:INFO:Calculating mean and std
2025-10-29 12:32:18,520:INFO:Creating metrics dataframe
2025-10-29 12:32:18,740:INFO:Uploading results into container
2025-10-29 12:32:18,740:INFO:Uploading model into container now
2025-10-29 12:32:18,741:INFO:_master_model_container: 16
2025-10-29 12:32:18,741:INFO:_display_container: 2
2025-10-29 12:32:18,741:INFO:DummyClassifier(constant=None, random_state=441239, strategy='prior')
2025-10-29 12:32:18,741:INFO:create_model() successfully completed......................................
2025-10-29 12:32:18,892:INFO:SubProcess create_model() end ==================================
2025-10-29 12:32:18,893:INFO:Creating metrics dataframe
2025-10-29 12:32:18,920:INFO:Initializing create_model()
2025-10-29 12:32:18,920:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002343087D550>, estimator=SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=441239, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2025-10-29 12:32:18,920:INFO:Checking exceptions
2025-10-29 12:32:18,921:INFO:Importing libraries
2025-10-29 12:32:18,922:INFO:Copying training dataset
2025-10-29 12:32:18,928:INFO:Defining folds
2025-10-29 12:32:18,928:INFO:Declaring metric variables
2025-10-29 12:32:18,928:INFO:Importing untrained model
2025-10-29 12:32:18,928:INFO:Declaring custom model
2025-10-29 12:32:18,929:INFO:SVM - Linear Kernel Imported successfully
2025-10-29 12:32:18,930:INFO:Cross validation set to False
2025-10-29 12:32:18,930:INFO:Fitting Model
2025-10-29 12:32:19,112:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=441239, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2025-10-29 12:32:19,112:INFO:create_model() successfully completed......................................
2025-10-29 12:32:19,271:INFO:Initializing create_model()
2025-10-29 12:32:19,271:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002343087D550>, estimator=LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2025-10-29 12:32:19,272:INFO:Checking exceptions
2025-10-29 12:32:19,274:INFO:Importing libraries
2025-10-29 12:32:19,274:INFO:Copying training dataset
2025-10-29 12:32:19,277:INFO:Defining folds
2025-10-29 12:32:19,277:INFO:Declaring metric variables
2025-10-29 12:32:19,278:INFO:Importing untrained model
2025-10-29 12:32:19,278:INFO:Declaring custom model
2025-10-29 12:32:19,278:INFO:Linear Discriminant Analysis Imported successfully
2025-10-29 12:32:19,279:INFO:Cross validation set to False
2025-10-29 12:32:19,279:INFO:Fitting Model
2025-10-29 12:32:19,461:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2025-10-29 12:32:19,461:INFO:create_model() successfully completed......................................
2025-10-29 12:32:19,620:INFO:Initializing create_model()
2025-10-29 12:32:19,620:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002343087D550>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=441239, splitter='best'), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2025-10-29 12:32:19,620:INFO:Checking exceptions
2025-10-29 12:32:19,623:INFO:Importing libraries
2025-10-29 12:32:19,623:INFO:Copying training dataset
2025-10-29 12:32:19,626:INFO:Defining folds
2025-10-29 12:32:19,626:INFO:Declaring metric variables
2025-10-29 12:32:19,626:INFO:Importing untrained model
2025-10-29 12:32:19,626:INFO:Declaring custom model
2025-10-29 12:32:19,626:INFO:Decision Tree Classifier Imported successfully
2025-10-29 12:32:19,627:INFO:Cross validation set to False
2025-10-29 12:32:19,627:INFO:Fitting Model
2025-10-29 12:32:19,804:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=441239, splitter='best')
2025-10-29 12:32:19,804:INFO:create_model() successfully completed......................................
2025-10-29 12:32:19,964:INFO:Initializing create_model()
2025-10-29 12:32:19,964:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002343087D550>, estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=441239), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2025-10-29 12:32:19,964:INFO:Checking exceptions
2025-10-29 12:32:19,966:INFO:Importing libraries
2025-10-29 12:32:19,966:INFO:Copying training dataset
2025-10-29 12:32:19,970:INFO:Defining folds
2025-10-29 12:32:19,970:INFO:Declaring metric variables
2025-10-29 12:32:19,970:INFO:Importing untrained model
2025-10-29 12:32:19,970:INFO:Declaring custom model
2025-10-29 12:32:19,971:INFO:str Imported successfully
2025-10-29 12:32:19,971:INFO:Cross validation set to False
2025-10-29 12:32:19,971:INFO:Fitting Model
2025-10-29 12:32:20,149:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=441239)
2025-10-29 12:32:20,149:INFO:create_model() successfully completed......................................
2025-10-29 12:32:20,307:INFO:Initializing create_model()
2025-10-29 12:32:20,307:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002343087D550>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=441239, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2025-10-29 12:32:20,307:INFO:Checking exceptions
2025-10-29 12:32:20,309:INFO:Importing libraries
2025-10-29 12:32:20,309:INFO:Copying training dataset
2025-10-29 12:32:20,313:INFO:Defining folds
2025-10-29 12:32:20,313:INFO:Declaring metric variables
2025-10-29 12:32:20,313:INFO:Importing untrained model
2025-10-29 12:32:20,313:INFO:Declaring custom model
2025-10-29 12:32:20,314:INFO:Gradient Boosting Classifier Imported successfully
2025-10-29 12:32:20,314:INFO:Cross validation set to False
2025-10-29 12:32:20,314:INFO:Fitting Model
2025-10-29 12:32:20,492:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=441239, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-10-29 12:32:20,493:INFO:create_model() successfully completed......................................
2025-10-29 12:32:20,653:INFO:Initializing create_model()
2025-10-29 12:32:20,653:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002343087D550>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=441239, reg_alpha=0.0, reg_lambda=0.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2025-10-29 12:32:20,653:INFO:Checking exceptions
2025-10-29 12:32:20,656:INFO:Importing libraries
2025-10-29 12:32:20,656:INFO:Copying training dataset
2025-10-29 12:32:20,659:INFO:Defining folds
2025-10-29 12:32:20,659:INFO:Declaring metric variables
2025-10-29 12:32:20,659:INFO:Importing untrained model
2025-10-29 12:32:20,659:INFO:Declaring custom model
2025-10-29 12:32:20,661:INFO:Light Gradient Boosting Machine Imported successfully
2025-10-29 12:32:20,662:INFO:Cross validation set to False
2025-10-29 12:32:20,662:INFO:Fitting Model
2025-10-29 12:32:20,694:INFO:[LightGBM] [Info] Number of positive: 2, number of negative: 3492
2025-10-29 12:32:20,694:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000186 seconds.
2025-10-29 12:32:20,694:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-10-29 12:32:20,694:INFO:[LightGBM] [Info] Total Bins 1275
2025-10-29 12:32:20,695:INFO:[LightGBM] [Info] Number of data points in the train set: 3494, number of used features: 5
2025-10-29 12:32:20,695:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000572 -> initscore=-7.465083
2025-10-29 12:32:20,695:INFO:[LightGBM] [Info] Start training from score -7.465083
2025-10-29 12:32:20,695:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 12:32:20,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 12:32:20,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 12:32:20,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 12:32:20,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 12:32:20,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 12:32:20,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 12:32:20,697:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 12:32:20,697:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 12:32:20,697:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 12:32:20,697:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 12:32:20,697:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 12:32:20,698:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 12:32:20,698:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 12:32:20,698:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 12:32:20,698:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 12:32:20,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 12:32:20,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 12:32:20,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 12:32:20,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 12:32:20,700:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 12:32:20,700:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 12:32:20,701:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 12:32:20,702:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 12:32:20,702:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 12:32:20,703:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 12:32:20,703:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 12:32:20,704:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 12:32:20,704:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 12:32:20,705:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 12:32:20,706:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 12:32:20,706:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 12:32:20,732:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 12:32:20,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 12:32:20,735:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 12:32:20,735:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 12:32:20,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-29 12:32:20,893:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=441239, reg_alpha=0.0, reg_lambda=0.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2025-10-29 12:32:20,893:INFO:create_model() successfully completed......................................
2025-10-29 12:32:21,053:INFO:Initializing create_model()
2025-10-29 12:32:21,053:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002343087D550>, estimator=<catboost.core.CatBoostClassifier object at 0x0000023430A7EDC0>, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2025-10-29 12:32:21,053:INFO:Checking exceptions
2025-10-29 12:32:21,055:INFO:Importing libraries
2025-10-29 12:32:21,055:INFO:Copying training dataset
2025-10-29 12:32:21,059:INFO:Defining folds
2025-10-29 12:32:21,059:INFO:Declaring metric variables
2025-10-29 12:32:21,059:INFO:Importing untrained model
2025-10-29 12:32:21,059:INFO:Declaring custom model
2025-10-29 12:32:21,059:INFO:CatBoost Classifier Imported successfully
2025-10-29 12:32:21,060:INFO:Cross validation set to False
2025-10-29 12:32:21,060:INFO:Fitting Model
2025-10-29 12:32:21,233:INFO:<catboost.core.CatBoostClassifier object at 0x0000023430A8F190>
2025-10-29 12:32:21,233:INFO:create_model() successfully completed......................................
2025-10-29 12:32:21,391:INFO:Initializing create_model()
2025-10-29 12:32:21,391:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002343087D550>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=441239, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2025-10-29 12:32:21,391:INFO:Checking exceptions
2025-10-29 12:32:21,393:INFO:Importing libraries
2025-10-29 12:32:21,393:INFO:Copying training dataset
2025-10-29 12:32:21,397:INFO:Defining folds
2025-10-29 12:32:21,397:INFO:Declaring metric variables
2025-10-29 12:32:21,398:INFO:Importing untrained model
2025-10-29 12:32:21,398:INFO:Declaring custom model
2025-10-29 12:32:21,398:INFO:Logistic Regression Imported successfully
2025-10-29 12:32:21,399:INFO:Cross validation set to False
2025-10-29 12:32:21,399:INFO:Fitting Model
2025-10-29 12:32:21,583:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=441239, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-10-29 12:32:21,583:INFO:create_model() successfully completed......................................
2025-10-29 12:32:21,741:INFO:Initializing create_model()
2025-10-29 12:32:21,741:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002343087D550>, estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform'), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2025-10-29 12:32:21,742:INFO:Checking exceptions
2025-10-29 12:32:21,744:INFO:Importing libraries
2025-10-29 12:32:21,744:INFO:Copying training dataset
2025-10-29 12:32:21,748:INFO:Defining folds
2025-10-29 12:32:21,748:INFO:Declaring metric variables
2025-10-29 12:32:21,748:INFO:Importing untrained model
2025-10-29 12:32:21,748:INFO:Declaring custom model
2025-10-29 12:32:21,748:INFO:K Neighbors Classifier Imported successfully
2025-10-29 12:32:21,749:INFO:Cross validation set to False
2025-10-29 12:32:21,749:INFO:Fitting Model
2025-10-29 12:32:21,924:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-10-29 12:32:21,924:INFO:create_model() successfully completed......................................
2025-10-29 12:32:22,078:INFO:Initializing create_model()
2025-10-29 12:32:22,078:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002343087D550>, estimator=GaussianNB(priors=None, var_smoothing=1e-09), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2025-10-29 12:32:22,079:INFO:Checking exceptions
2025-10-29 12:32:22,080:INFO:Importing libraries
2025-10-29 12:32:22,080:INFO:Copying training dataset
2025-10-29 12:32:22,084:INFO:Defining folds
2025-10-29 12:32:22,084:INFO:Declaring metric variables
2025-10-29 12:32:22,084:INFO:Importing untrained model
2025-10-29 12:32:22,085:INFO:Declaring custom model
2025-10-29 12:32:22,085:INFO:Naive Bayes Imported successfully
2025-10-29 12:32:22,086:INFO:Cross validation set to False
2025-10-29 12:32:22,086:INFO:Fitting Model
2025-10-29 12:32:22,258:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-10-29 12:32:22,258:INFO:create_model() successfully completed......................................
2025-10-29 12:32:22,446:INFO:_master_model_container: 16
2025-10-29 12:32:22,446:INFO:_display_container: 2
2025-10-29 12:32:22,448:INFO:[SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=441239, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False), LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001), DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=441239, splitter='best'), AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=441239), GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=441239, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=441239, reg_alpha=0.0, reg_lambda=0.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), <catboost.core.CatBoostClassifier object at 0x0000023430A8F190>, LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=441239, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform'), GaussianNB(priors=None, var_smoothing=1e-09)]
2025-10-29 12:32:22,448:INFO:compare_models() successfully completed......................................
2025-10-29 12:32:22,449:INFO:Initializing tune_model()
2025-10-29 12:32:22,449:INFO:tune_model(estimator=SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=441239, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002343087D550>)
2025-10-29 12:32:22,449:INFO:Checking exceptions
2025-10-29 12:32:22,484:INFO:Copying training dataset
2025-10-29 12:32:22,489:INFO:Checking base model
2025-10-29 12:32:22,489:INFO:Base model : SVM - Linear Kernel
2025-10-29 12:32:22,492:INFO:Declaring metric variables
2025-10-29 12:32:22,497:INFO:Defining Hyperparameters
2025-10-29 12:32:22,659:INFO:Tuning with n_jobs=-1
2025-10-29 12:32:22,659:INFO:Initializing RandomizedSearchCV
2025-10-29 12:32:22,663:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 12:32:39,277:INFO:best_params: {'actual_estimator__penalty': 'elasticnet', 'actual_estimator__learning_rate': 'constant', 'actual_estimator__l1_ratio': 0.8000000001, 'actual_estimator__fit_intercept': True, 'actual_estimator__eta0': 0.001, 'actual_estimator__alpha': 0.05}
2025-10-29 12:32:39,278:INFO:Hyperparameter search completed
2025-10-29 12:32:39,278:INFO:SubProcess create_model() called ==================================
2025-10-29 12:32:39,279:INFO:Initializing create_model()
2025-10-29 12:32:39,279:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002343087D550>, estimator=SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=441239, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002342DFA4850>, model_only=True, return_train_score=False, kwargs={'penalty': 'elasticnet', 'learning_rate': 'constant', 'l1_ratio': 0.8000000001, 'fit_intercept': True, 'eta0': 0.001, 'alpha': 0.05})
2025-10-29 12:32:39,279:INFO:Checking exceptions
2025-10-29 12:32:39,279:INFO:Importing libraries
2025-10-29 12:32:39,279:INFO:Copying training dataset
2025-10-29 12:32:39,283:INFO:Defining folds
2025-10-29 12:32:39,284:INFO:Declaring metric variables
2025-10-29 12:32:39,287:INFO:Importing untrained model
2025-10-29 12:32:39,288:INFO:Declaring custom model
2025-10-29 12:32:39,291:INFO:SVM - Linear Kernel Imported successfully
2025-10-29 12:32:39,298:INFO:Starting cross validation
2025-10-29 12:32:39,299:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 12:32:39,301:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 12:32:39,376:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 12:32:39,379:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:39,381:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:39,382:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 12:32:39,383:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:39,385:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:39,386:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:39,386:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 12:32:39,388:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:39,390:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:39,390:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 12:32:39,390:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:39,391:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:39,395:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:39,395:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 12:32:39,397:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:39,399:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:39,402:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:39,403:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:39,436:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 12:32:39,438:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:39,440:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:39,443:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:39,444:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:39,672:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 12:32:39,674:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:39,676:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:39,678:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:39,679:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 12:32:39,679:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:39,681:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:39,683:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:39,685:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:39,686:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:39,688:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 12:32:39,690:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:39,692:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:39,694:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:39,695:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:39,706:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 12:32:39,708:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:39,710:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:39,712:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:39,713:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:40,905:INFO:Calculating mean and std
2025-10-29 12:32:40,906:INFO:Creating metrics dataframe
2025-10-29 12:32:40,912:INFO:Finalizing model
2025-10-29 12:32:41,172:INFO:Uploading results into container
2025-10-29 12:32:41,172:INFO:Uploading model into container now
2025-10-29 12:32:41,173:INFO:_master_model_container: 17
2025-10-29 12:32:41,173:INFO:_display_container: 3
2025-10-29 12:32:41,173:INFO:SGDClassifier(alpha=0.05, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.8000000001, learning_rate='constant', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1,
              penalty='elasticnet', power_t=0.5, random_state=441239,
              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,
              warm_start=False)
2025-10-29 12:32:41,173:INFO:create_model() successfully completed......................................
2025-10-29 12:32:41,333:INFO:SubProcess create_model() end ==================================
2025-10-29 12:32:41,333:INFO:choose_better activated
2025-10-29 12:32:41,338:INFO:SubProcess create_model() called ==================================
2025-10-29 12:32:41,338:INFO:Initializing create_model()
2025-10-29 12:32:41,338:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002343087D550>, estimator=SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=441239, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2025-10-29 12:32:41,338:INFO:Checking exceptions
2025-10-29 12:32:41,340:INFO:Importing libraries
2025-10-29 12:32:41,340:INFO:Copying training dataset
2025-10-29 12:32:41,345:INFO:Defining folds
2025-10-29 12:32:41,346:INFO:Declaring metric variables
2025-10-29 12:32:41,346:INFO:Importing untrained model
2025-10-29 12:32:41,346:INFO:Declaring custom model
2025-10-29 12:32:41,346:INFO:SVM - Linear Kernel Imported successfully
2025-10-29 12:32:41,347:INFO:Starting cross validation
2025-10-29 12:32:41,347:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-29 12:32:41,349:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\model_selection\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(

2025-10-29 12:32:41,429:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 12:32:41,431:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:41,433:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:41,435:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 12:32:41,436:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:41,437:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:41,437:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:41,439:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:41,442:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 12:32:41,443:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:41,444:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:41,445:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 12:32:41,453:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 12:32:41,457:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:41,459:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:41,461:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:41,463:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:41,464:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 12:32:41,466:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:41,469:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:41,471:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:41,472:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:41,733:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 12:32:41,733:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 12:32:41,734:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 12:32:41,735:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:41,736:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:41,737:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:41,738:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:41,738:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:41,740:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:41,740:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:41,741:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:41,741:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:41,742:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:41,742:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:41,743:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:41,750:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2025-10-29 12:32:41,752:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:41,755:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2025-10-29 12:32:41,757:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2025-10-29 12:32:41,758:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\sklearn\metrics\_classification.py:673: RuntimeWarning: invalid value encountered in scalar divide
  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)

2025-10-29 12:32:42,910:INFO:Calculating mean and std
2025-10-29 12:32:42,911:INFO:Creating metrics dataframe
2025-10-29 12:32:42,913:INFO:Finalizing model
2025-10-29 12:32:43,172:INFO:Uploading results into container
2025-10-29 12:32:43,173:INFO:Uploading model into container now
2025-10-29 12:32:43,173:INFO:_master_model_container: 18
2025-10-29 12:32:43,173:INFO:_display_container: 4
2025-10-29 12:32:43,173:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=441239, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2025-10-29 12:32:43,173:INFO:create_model() successfully completed......................................
2025-10-29 12:32:43,329:INFO:SubProcess create_model() end ==================================
2025-10-29 12:32:43,329:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=441239, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False) result for Accuracy is 1.0
2025-10-29 12:32:43,330:INFO:SGDClassifier(alpha=0.05, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.8000000001, learning_rate='constant', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1,
              penalty='elasticnet', power_t=0.5, random_state=441239,
              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,
              warm_start=False) result for Accuracy is 0.9994
2025-10-29 12:32:43,330:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=441239, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False) is best model
2025-10-29 12:32:43,330:INFO:choose_better completed
2025-10-29 12:32:43,330:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2025-10-29 12:32:43,340:INFO:_master_model_container: 18
2025-10-29 12:32:43,340:INFO:_display_container: 3
2025-10-29 12:32:43,340:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=441239, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2025-10-29 12:32:43,341:INFO:tune_model() successfully completed......................................
2025-10-29 12:32:43,667:INFO:Initializing evaluate_model()
2025-10-29 12:32:43,668:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002343087D550>, estimator=SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=441239, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None, use_train_data=False)
2025-10-29 12:32:43,677:INFO:Initializing plot_model()
2025-10-29 12:32:43,677:INFO:plot_model(plot=pipeline, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=441239, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002343087D550>, system=True)
2025-10-29 12:32:43,677:INFO:Checking exceptions
2025-10-29 12:32:43,679:INFO:Preloading libraries
2025-10-29 12:32:43,679:INFO:Copying training dataset
2025-10-29 12:32:43,679:INFO:Plot type: pipeline
2025-10-29 12:32:43,903:INFO:Visual Rendered Successfully
2025-10-29 12:32:44,071:INFO:plot_model() successfully completed......................................
2025-10-29 12:51:54,958:INFO:Initializing plot_model()
2025-10-29 12:51:54,958:INFO:plot_model(plot=auc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=441239, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002343087D550>, system=True)
2025-10-29 12:51:54,958:INFO:Checking exceptions
2025-10-29 12:51:56,515:INFO:Initializing plot_model()
2025-10-29 12:51:56,515:INFO:plot_model(plot=pipeline, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=441239, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002343087D550>, system=True)
2025-10-29 12:51:56,515:INFO:Checking exceptions
2025-10-29 12:51:56,517:INFO:Preloading libraries
2025-10-29 12:51:56,517:INFO:Copying training dataset
2025-10-29 12:51:56,517:INFO:Plot type: pipeline
2025-10-29 12:51:56,618:INFO:Visual Rendered Successfully
2025-10-29 12:51:57,042:INFO:plot_model() successfully completed......................................
2025-10-30 11:57:44,519:WARNING:C:\Users\hkh97\AppData\Local\Temp\ipykernel_6908\3151350187.py:46: UserWarning: Glyph 49884 (\N{HANGUL SYLLABLE SI}) missing from current font.
  plt.tight_layout()

2025-10-30 11:57:44,519:WARNING:C:\Users\hkh97\AppData\Local\Temp\ipykernel_6908\3151350187.py:46: UserWarning: Glyph 44036 (\N{HANGUL SYLLABLE GAN}) missing from current font.
  plt.tight_layout()

2025-10-30 11:57:44,962:WARNING:C:\Users\hkh97\AppData\Local\Temp\ipykernel_6908\3151350187.py:46: UserWarning: Glyph 48320 (\N{HANGUL SYLLABLE BYEON}) missing from current font.
  plt.tight_layout()

2025-10-30 11:57:44,962:WARNING:C:\Users\hkh97\AppData\Local\Temp\ipykernel_6908\3151350187.py:46: UserWarning: Glyph 54868 (\N{HANGUL SYLLABLE HWA}) missing from current font.
  plt.tight_layout()

2025-10-30 11:57:53,463:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\IPython\core\pylabtools.py:152: UserWarning: Glyph 48320 (\N{HANGUL SYLLABLE BYEON}) missing from current font.
  fig.canvas.print_figure(bytes_io, **kw)

2025-10-30 11:57:53,463:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\IPython\core\pylabtools.py:152: UserWarning: Glyph 54868 (\N{HANGUL SYLLABLE HWA}) missing from current font.
  fig.canvas.print_figure(bytes_io, **kw)

2025-10-30 11:58:19,433:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\IPython\core\pylabtools.py:152: UserWarning: Glyph 49884 (\N{HANGUL SYLLABLE SI}) missing from current font.
  fig.canvas.print_figure(bytes_io, **kw)

2025-10-30 11:58:19,433:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\IPython\core\pylabtools.py:152: UserWarning: Glyph 44036 (\N{HANGUL SYLLABLE GAN}) missing from current font.
  fig.canvas.print_figure(bytes_io, **kw)

2025-10-30 12:02:46,114:WARNING:C:\Users\hkh97\AppData\Local\Temp\ipykernel_6908\1427536003.py:45: UserWarning: Glyph 49884 (\N{HANGUL SYLLABLE SI}) missing from current font.
  plt.tight_layout()

2025-10-30 12:02:46,114:WARNING:C:\Users\hkh97\AppData\Local\Temp\ipykernel_6908\1427536003.py:45: UserWarning: Glyph 44036 (\N{HANGUL SYLLABLE GAN}) missing from current font.
  plt.tight_layout()

2025-10-30 12:02:46,133:WARNING:C:\Users\hkh97\AppData\Local\Temp\ipykernel_6908\1427536003.py:45: UserWarning: Glyph 48320 (\N{HANGUL SYLLABLE BYEON}) missing from current font.
  plt.tight_layout()

2025-10-30 12:02:46,133:WARNING:C:\Users\hkh97\AppData\Local\Temp\ipykernel_6908\1427536003.py:45: UserWarning: Glyph 54868 (\N{HANGUL SYLLABLE HWA}) missing from current font.
  plt.tight_layout()

2025-10-30 12:02:46,678:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\IPython\core\pylabtools.py:152: UserWarning: Glyph 49884 (\N{HANGUL SYLLABLE SI}) missing from current font.
  fig.canvas.print_figure(bytes_io, **kw)

2025-10-30 12:02:46,678:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\IPython\core\pylabtools.py:152: UserWarning: Glyph 44036 (\N{HANGUL SYLLABLE GAN}) missing from current font.
  fig.canvas.print_figure(bytes_io, **kw)

2025-10-30 12:02:46,686:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\IPython\core\pylabtools.py:152: UserWarning: Glyph 48320 (\N{HANGUL SYLLABLE BYEON}) missing from current font.
  fig.canvas.print_figure(bytes_io, **kw)

2025-10-30 12:02:46,686:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\IPython\core\pylabtools.py:152: UserWarning: Glyph 54868 (\N{HANGUL SYLLABLE HWA}) missing from current font.
  fig.canvas.print_figure(bytes_io, **kw)

2025-10-30 12:06:55,146:WARNING:C:\Users\hkh97\AppData\Local\Temp\ipykernel_6908\1128509029.py:48: UserWarning: Glyph 48320 (\N{HANGUL SYLLABLE BYEON}) missing from current font.
  plt.tight_layout()

2025-10-30 12:06:55,147:WARNING:C:\Users\hkh97\AppData\Local\Temp\ipykernel_6908\1128509029.py:48: UserWarning: Glyph 54868 (\N{HANGUL SYLLABLE HWA}) missing from current font.
  plt.tight_layout()

2025-10-30 12:06:55,708:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\IPython\core\pylabtools.py:152: UserWarning: Glyph 48320 (\N{HANGUL SYLLABLE BYEON}) missing from current font.
  fig.canvas.print_figure(bytes_io, **kw)

2025-10-30 12:06:55,708:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\IPython\core\pylabtools.py:152: UserWarning: Glyph 54868 (\N{HANGUL SYLLABLE HWA}) missing from current font.
  fig.canvas.print_figure(bytes_io, **kw)

2025-10-30 12:34:39,624:WARNING:C:\Users\hkh97\AppData\Local\Temp\ipykernel_6908\1718445785.py:47: UserWarning: Glyph 48320 (\N{HANGUL SYLLABLE BYEON}) missing from current font.
  plt.tight_layout()

2025-10-30 12:34:39,624:WARNING:C:\Users\hkh97\AppData\Local\Temp\ipykernel_6908\1718445785.py:47: UserWarning: Glyph 54868 (\N{HANGUL SYLLABLE HWA}) missing from current font.
  plt.tight_layout()

2025-10-30 12:34:39,978:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\IPython\core\pylabtools.py:152: UserWarning: Glyph 48320 (\N{HANGUL SYLLABLE BYEON}) missing from current font.
  fig.canvas.print_figure(bytes_io, **kw)

2025-10-30 12:34:39,978:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\IPython\core\pylabtools.py:152: UserWarning: Glyph 54868 (\N{HANGUL SYLLABLE HWA}) missing from current font.
  fig.canvas.print_figure(bytes_io, **kw)

2025-10-30 12:45:18,198:WARNING:C:\Users\hkh97\AppData\Local\Temp\ipykernel_6908\1128509029.py:48: UserWarning: Glyph 48320 (\N{HANGUL SYLLABLE BYEON}) missing from current font.
  plt.tight_layout()

2025-10-30 12:45:18,198:WARNING:C:\Users\hkh97\AppData\Local\Temp\ipykernel_6908\1128509029.py:48: UserWarning: Glyph 54868 (\N{HANGUL SYLLABLE HWA}) missing from current font.
  plt.tight_layout()

2025-10-30 12:45:18,738:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\IPython\core\pylabtools.py:152: UserWarning: Glyph 48320 (\N{HANGUL SYLLABLE BYEON}) missing from current font.
  fig.canvas.print_figure(bytes_io, **kw)

2025-10-30 12:45:18,739:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\IPython\core\pylabtools.py:152: UserWarning: Glyph 54868 (\N{HANGUL SYLLABLE HWA}) missing from current font.
  fig.canvas.print_figure(bytes_io, **kw)

2025-10-30 12:57:30,523:WARNING:C:\Users\hkh97\AppData\Local\Temp\ipykernel_6908\1050399074.py:33: UserWarning: Glyph 49884 (\N{HANGUL SYLLABLE SI}) missing from current font.
  plt.tight_layout()

2025-10-30 12:57:30,523:WARNING:C:\Users\hkh97\AppData\Local\Temp\ipykernel_6908\1050399074.py:33: UserWarning: Glyph 44036 (\N{HANGUL SYLLABLE GAN}) missing from current font.
  plt.tight_layout()

2025-10-30 12:57:30,523:WARNING:C:\Users\hkh97\AppData\Local\Temp\ipykernel_6908\1050399074.py:33: UserWarning: Glyph 12622 (\N{HANGUL LETTER HIEUH}) missing from current font.
  plt.tight_layout()

2025-10-30 12:57:30,523:WARNING:C:\Users\hkh97\AppData\Local\Temp\ipykernel_6908\1050399074.py:33: UserWarning: Glyph 47492 (\N{HANGUL SYLLABLE REUM}) missing from current font.
  plt.tight_layout()

2025-10-30 12:57:30,523:WARNING:C:\Users\hkh97\AppData\Local\Temp\ipykernel_6908\1050399074.py:33: UserWarning: Glyph 50640 (\N{HANGUL SYLLABLE E}) missing from current font.
  plt.tight_layout()

2025-10-30 12:57:30,524:WARNING:C:\Users\hkh97\AppData\Local\Temp\ipykernel_6908\1050399074.py:33: UserWarning: Glyph 46384 (\N{HANGUL SYLLABLE DDA}) missing from current font.
  plt.tight_layout()

2025-10-30 12:57:30,524:WARNING:C:\Users\hkh97\AppData\Local\Temp\ipykernel_6908\1050399074.py:33: UserWarning: Glyph 47480 (\N{HANGUL SYLLABLE REUN}) missing from current font.
  plt.tight_layout()

2025-10-30 12:57:30,524:WARNING:C:\Users\hkh97\AppData\Local\Temp\ipykernel_6908\1050399074.py:33: UserWarning: Glyph 48320 (\N{HANGUL SYLLABLE BYEON}) missing from current font.
  plt.tight_layout()

2025-10-30 12:57:30,524:WARNING:C:\Users\hkh97\AppData\Local\Temp\ipykernel_6908\1050399074.py:33: UserWarning: Glyph 54868 (\N{HANGUL SYLLABLE HWA}) missing from current font.
  plt.tight_layout()

2025-10-30 12:57:30,708:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\IPython\core\pylabtools.py:152: UserWarning: Glyph 49884 (\N{HANGUL SYLLABLE SI}) missing from current font.
  fig.canvas.print_figure(bytes_io, **kw)

2025-10-30 12:57:30,708:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\IPython\core\pylabtools.py:152: UserWarning: Glyph 44036 (\N{HANGUL SYLLABLE GAN}) missing from current font.
  fig.canvas.print_figure(bytes_io, **kw)

2025-10-30 12:57:30,708:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\IPython\core\pylabtools.py:152: UserWarning: Glyph 12622 (\N{HANGUL LETTER HIEUH}) missing from current font.
  fig.canvas.print_figure(bytes_io, **kw)

2025-10-30 12:57:30,708:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\IPython\core\pylabtools.py:152: UserWarning: Glyph 47492 (\N{HANGUL SYLLABLE REUM}) missing from current font.
  fig.canvas.print_figure(bytes_io, **kw)

2025-10-30 12:57:30,708:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\IPython\core\pylabtools.py:152: UserWarning: Glyph 50640 (\N{HANGUL SYLLABLE E}) missing from current font.
  fig.canvas.print_figure(bytes_io, **kw)

2025-10-30 12:57:30,709:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\IPython\core\pylabtools.py:152: UserWarning: Glyph 46384 (\N{HANGUL SYLLABLE DDA}) missing from current font.
  fig.canvas.print_figure(bytes_io, **kw)

2025-10-30 12:57:30,709:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\IPython\core\pylabtools.py:152: UserWarning: Glyph 47480 (\N{HANGUL SYLLABLE REUN}) missing from current font.
  fig.canvas.print_figure(bytes_io, **kw)

2025-10-30 12:57:30,709:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\IPython\core\pylabtools.py:152: UserWarning: Glyph 48320 (\N{HANGUL SYLLABLE BYEON}) missing from current font.
  fig.canvas.print_figure(bytes_io, **kw)

2025-10-30 12:57:30,709:WARNING:c:\Users\hkh97\anaconda3\envs\han\lib\site-packages\IPython\core\pylabtools.py:152: UserWarning: Glyph 54868 (\N{HANGUL SYLLABLE HWA}) missing from current font.
  fig.canvas.print_figure(bytes_io, **kw)

